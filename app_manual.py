# íŒŒì¼ ì´ë¦„: app_manual.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - ì™„ì „í•œ ë²„ì „)

# ===== í•„ìˆ˜ import ë¬¸ë“¤ì„ ë§¨ ìœ„ë¡œ ì´ë™ =====
import streamlit as st
import numpy as np  # SimpleVectorSearch í´ë˜ìŠ¤ë³´ë‹¤ ë¨¼ì € import í•„ìš”
from typing import List, Dict, Tuple, Optional, Set, TypedDict, Protocol, Iterator, Generator, OrderedDict, Any, Union
import json
import openai
import re
from collections import defaultdict, Counter, OrderedDict
import time
from dataclasses import dataclass, field
import os
import hashlib
from enum import Enum
import asyncio
import logging
from contextlib import contextmanager
import traceback
import gc
import concurrent.futures
from functools import lru_cache, wraps
import pickle
from pathlib import Path

# ì„ íƒì  import - ì—†ì–´ë„ ì•±ì´ ì‹¤í–‰ë˜ë„ë¡ ì²˜ë¦¬
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logging.warning("FAISS module not available - using alternative search method")

try:
    from cryptography.fernet import Fernet
    CRYPTOGRAPHY_AVAILABLE = True
except ImportError:
    CRYPTOGRAPHY_AVAILABLE = False
    logging.warning("cryptography module not available - encryption features disabled")

try:
    import ijson
    IJSON_AVAILABLE = True
except ImportError:
    IJSON_AVAILABLE = False
    logging.warning("ijson module not available - using standard JSON loading")

try:
    import nest_asyncio
    nest_asyncio.apply()
    NEST_ASYNCIO_AVAILABLE = True
except ImportError:
    NEST_ASYNCIO_AVAILABLE = False
    logging.warning("nest_asyncio not available - async features may be limited")

try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("sentence-transformers not available - will use OpenAI embeddings")

# ===== ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ (FAISS ëŒ€ì²´) =====
class SimpleVectorSearch:
    """FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰
    
    ì´ í´ë˜ìŠ¤ëŠ” NumPyë§Œì„ ì‚¬ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ì˜
    ë²¡í„° ê²€ìƒ‰ì„ êµ¬í˜„í•©ë‹ˆë‹¤. FAISSë³´ë‹¤ëŠ” ëŠë¦¬ì§€ë§Œ,
    ì–´ë–¤ í™˜ê²½ì—ì„œë„ ì‘ë™í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, embeddings: np.ndarray):
        """
        Args:
            embeddings: ë¬¸ì„œ ì„ë² ë”© ë°°ì—´ (n_docs, embedding_dim)
        """
        self.embeddings = embeddings
        # ì •ê·œí™”ëœ ì„ë² ë”© ì €ì¥ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”)
        self.normalized_embeddings = self._normalize_vectors(embeddings)
        logging.info(f"SimpleVectorSearch initialized with {len(embeddings)} documents")
    
    def _normalize_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """ë²¡í„° ì •ê·œí™” (L2 norm = 1)"""
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        norms = np.where(norms == 0, 1, norms)
        return vectors / norms
    
    def search(self, query_vector: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        
        Args:
            query_vector: ì¿¼ë¦¬ ë²¡í„° (1, embedding_dim)
            k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
        
        Returns:
            scores: ìœ ì‚¬ë„ ì ìˆ˜ ë°°ì—´
            indices: ë¬¸ì„œ ì¸ë±ìŠ¤ ë°°ì—´
        """
        # ì¿¼ë¦¬ ë²¡í„° ì •ê·œí™”
        query_norm = self._normalize_vectors(query_vector.reshape(1, -1))
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ ë²¡í„°ì˜ ë‚´ì )
        similarities = np.dot(self.normalized_embeddings, query_norm.T).squeeze()
        
        # ìƒìœ„ kê°œ ì„ íƒ
        top_k_indices = np.argpartition(similarities, -k)[-k:]
        top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]
        
        # FAISSì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        scores = similarities[top_k_indices].reshape(1, -1)
        indices = top_k_indices.reshape(1, -1)
        
        return scores, indices

# ===== ë¡œê¹… ì„¤ì • =====
def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    logger = logging.getLogger('ftc_chatbot')
    logger.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler('ftc_chatbot_debug.log')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# ===== ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ í´ë˜ìŠ¤ =====
class RAGPipelineError(Exception):
    """RAG íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

class IndexError(RAGPipelineError):
    """ì¸ë±ìŠ¤ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class EmbeddingError(RAGPipelineError):
    """ì„ë² ë”© ìƒì„± ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class GPTAnalysisError(RAGPipelineError):
    """GPT ë¶„ì„ ì‹¤íŒ¨ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class APIKeyError(RAGPipelineError):
    """API í‚¤ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

# ===== ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € =====
@contextmanager
def error_context(operation_name: str, fallback_value=None):
    """ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    
    ì´ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ëŠ” ì‘ì—… ì¤‘ ë°œìƒí•˜ëŠ” ì—ëŸ¬ë¥¼ ìš°ì•„í•˜ê²Œ ì²˜ë¦¬í•˜ê³ ,
    ì‚¬ìš©ìì—ê²Œ ì¹œìˆ™í•œ ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì£¼ë©´ì„œë„ ê°œë°œìë¥¼ ìœ„í•œ ìƒì„¸ ì •ë³´ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
    """
    try:
        logger.debug(f"Starting operation: {operation_name}")
        yield
    except Exception as e:
        logger.error(f"Error in {operation_name}: {str(e)}")
        logger.debug(f"Full traceback:\n{traceback.format_exc()}")
        
        if 'st' in globals():
            st.error(f"âš ï¸ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {operation_name}")
            with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                st.code(traceback.format_exc())
        
        if fallback_value is not None:
            return fallback_value
        raise

# ===== API ê´€ë¦¬ì =====
class APIManager:
    """OpenAI API í‚¤ì™€ í˜¸ì¶œì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._api_key = None
        self._last_call_time = {}
        self._embedding_cache = {}
        
        # ê° ëª¨ë¸ì˜ ì†ë„ ì œí•œ ì„¤ì •
        self._rate_limits = {
            'gpt-4o': {'calls_per_minute': 60, 'tokens_per_minute': 150000},
            'gpt-4o-mini': {'calls_per_minute': 500, 'tokens_per_minute': 200000},
            'o4-mini': {'calls_per_minute': 30, 'tokens_per_minute': 100000}
        }
    
    def load_api_key(self) -> str:
        """API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ
        
        ìš°ì„ ìˆœìœ„:
        1. Streamlit secrets (ê°œë°œ/í”„ë¡œë•ì…˜ í™˜ê²½)
        2. í™˜ê²½ ë³€ìˆ˜ (ë¡œì»¬ í…ŒìŠ¤íŠ¸)
        3. ì•”í˜¸í™”ëœ íŒŒì¼ (ê³ ê¸‰ ë³´ì•ˆ - ì„ íƒì )
        """
        # Streamlit secrets í™•ì¸ (ìµœìš°ì„ )
        try:
            if 'OPENAI_API_KEY' in st.secrets:
                self._api_key = st.secrets["OPENAI_API_KEY"]
                logger.info("API key loaded from Streamlit secrets")
                return self._api_key
        except Exception as e:
            logger.debug(f"Streamlit secrets not available: {e}")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        if os.environ.get('OPENAI_API_KEY'):
            self._api_key = os.environ.get('OPENAI_API_KEY')
            logger.info("API key loaded from environment variable")
            return self._api_key
        
        # ì•”í˜¸í™”ëœ íŒŒì¼ í™•ì¸ (ì„ íƒì  - cryptography ëª¨ë“ˆì´ ìˆì„ ë•Œë§Œ)
        if CRYPTOGRAPHY_AVAILABLE:
            try:
                from cryptography.fernet import Fernet
                encrypted_key_path = Path('.api_key.enc')
                if encrypted_key_path.exists():
                    cipher_key = os.environ.get('API_CIPHER_KEY')
                    if cipher_key:
                        cipher = Fernet(cipher_key.encode())
                        with open(encrypted_key_path, 'rb') as f:
                            encrypted_key = f.read()
                        self._api_key = cipher.decrypt(encrypted_key).decode()
                        logger.info("API key loaded from encrypted file")
                        return self._api_key
            except Exception as e:
                logger.error(f"Failed to decrypt API key: {e}")
        
        raise APIKeyError("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    def rate_limit(self, model: str = 'gpt-4o'):
        """API í˜¸ì¶œ ì†ë„ ì œí•œ ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                current_time = time.time()
                model_key = f"{model}_last_call"
                
                # ë§ˆì§€ë§‰ í˜¸ì¶œ ì‹œê°„ í™•ì¸
                if model_key in self._last_call_time:
                    elapsed = current_time - self._last_call_time[model_key]
                    min_interval = 60.0 / self._rate_limits[model]['calls_per_minute']
                    
                    if elapsed < min_interval:
                        sleep_time = min_interval - elapsed
                        logger.debug(f"Rate limiting: sleeping for {sleep_time:.2f}s")
                        time.sleep(sleep_time)
                
                # í˜¸ì¶œ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # ë§ˆì§€ë§‰ í˜¸ì¶œ ì‹œê°„ ì—…ë°ì´íŠ¸
                self._last_call_time[model_key] = time.time()
                
                return result
            return wrapper
        return decorator
    
    def get_embedding(self, text: str) -> np.ndarray:
        """OpenAI embeddings APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]
        
        try:
            response = openai.embeddings.create(
                input=text,
                model="text-embedding-ada-002"
            )
            embedding = np.array(response.data[0].embedding)
            
            # ìºì‹œì— ì €ì¥
            self._embedding_cache[cache_key] = embedding
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self._embedding_cache) > 1000:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self._embedding_cache))
                del self._embedding_cache[oldest_key]
            
            return embedding
        except Exception as e:
            logger.error(f"Failed to get OpenAI embedding: {e}")
            # ì‹¤íŒ¨ ì‹œ ëœë¤ ë²¡í„° ë°˜í™˜ (ì„ì‹œ)
            return np.random.randn(1536)  # OpenAI ì„ë² ë”© ì°¨ì›

# ===== ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë¡œë” =====
class ChunkLoader:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë¡œë”© ì‹œìŠ¤í…œ
    
    ì´ í´ë˜ìŠ¤ëŠ” ëŒ€ìš©ëŸ‰ JSON íŒŒì¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ijsonì´ ìˆìœ¼ë©´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì„, ì—†ìœ¼ë©´ ì¼ë°˜ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, filepath: str):
        self.filepath = filepath
        self._chunks = None
        self._chunk_cache = OrderedDict()
        self._cache_size = 1000
        self._use_streaming = False
        
        # ijson ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if IJSON_AVAILABLE:
            self._use_streaming = True
            logger.info("Using streaming JSON parser (ijson)")
        else:
            self._use_streaming = False
            logger.info("ijson not available, using standard JSON loading")
        
        self._initialize()
    
    def _initialize(self):
        """ì´ˆê¸°í™” - ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ ë°©ì‹ ì„ íƒ"""
        if self._use_streaming:
            try:
                self._build_streaming_index()
            except Exception as e:
                logger.warning(f"Streaming index failed: {e}, falling back to standard loading")
                self._use_streaming = False
                self._load_all_chunks()
        else:
            self._load_all_chunks()
    
    def _load_all_chunks(self):
        """ì „ì²´ ì²­í¬ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ (í´ë°± ë°©ì‹)"""
        logger.info(f"Loading all chunks from {self.filepath}")
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                self._chunks = json.load(f)
            logger.info(f"Loaded {len(self._chunks)} chunks into memory")
        except Exception as e:
            logger.error(f"Failed to load chunks: {e}")
            self._chunks = []
    
    def _build_streaming_index(self):
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì¶• (ijson í•„ìš”)"""
        import ijson
        logger.info(f"Building streaming index for {self.filepath}")
        self._index = {}
        
        with open(self.filepath, 'rb') as f:
            parser = ijson.items(f, 'item')
            for idx, item in enumerate(parser):
                # ì¸ë±ìŠ¤ë§Œ ì €ì¥í•˜ê³  ì‹¤ì œ ë°ì´í„°ëŠ” ë‚˜ì¤‘ì— ë¡œë“œ
                self._index[idx] = idx
        
        logger.info(f"Streaming index built: {len(self._index)} chunks found")
    
    def get_chunk(self, idx: int) -> Dict:
        """íŠ¹ì • ì¸ë±ìŠ¤ì˜ ì²­í¬ë¥¼ ê°€ì ¸ì˜´"""
        # ìºì‹œ í™•ì¸
        if idx in self._chunk_cache:
            self._chunk_cache.move_to_end(idx)
            return self._chunk_cache[idx]
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì´ ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ë°˜í™˜
        if not self._use_streaming:
            if self._chunks and 0 <= idx < len(self._chunks):
                chunk = self._chunks[idx]
                self._add_to_cache(idx, chunk)
                return chunk
            else:
                raise IndexError(f"Chunk index {idx} out of range")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŠ¹ì • ì²­í¬ ë¡œë“œ
        try:
            import ijson
            with open(self.filepath, 'rb') as f:
                parser = ijson.items(f, 'item')
                for i, item in enumerate(parser):
                    if i == idx:
                        self._add_to_cache(idx, item)
                        return item
            raise IndexError(f"Chunk index {idx} not found")
        except Exception as e:
            logger.error(f"Failed to load chunk {idx}: {e}")
            # í´ë°±: ì „ì²´ ë¡œë“œ ì‹œë„
            if not self._chunks:
                self._load_all_chunks()
            if self._chunks and 0 <= idx < len(self._chunks):
                return self._chunks[idx]
            raise
    
    def _add_to_cache(self, idx: int, chunk: Dict):
        """ìºì‹œì— ì²­í¬ ì¶”ê°€ (LRU ì •ì±…)"""
        if len(self._chunk_cache) >= self._cache_size:
            self._chunk_cache.popitem(last=False)
        self._chunk_cache[idx] = chunk
    
    def iter_chunks(self, indices: List[int] = None) -> Iterator[Dict]:
        """í•„ìš”í•œ ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜"""
        if not self._use_streaming and self._chunks:
            # ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ê²½ìš°
            if indices is None:
                indices = range(len(self._chunks))
            for idx in indices:
                if 0 <= idx < len(self._chunks):
                    yield self._chunks[idx]
        else:
            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹
            if indices is None:
                indices = range(self.get_total_chunks())
            for idx in indices:
                yield self.get_chunk(idx)
    
    def get_total_chunks(self) -> int:
        """ì „ì²´ ì²­í¬ ìˆ˜ ë°˜í™˜"""
        if not self._use_streaming and self._chunks:
            return len(self._chunks)
        elif hasattr(self, '_index'):
            return len(self._index)
        else:
            # ì¹´ìš´íŠ¸ë¥¼ ìœ„í•´ í•œ ë²ˆ ìŠ¤ìº”
            if not self._chunks:
                self._load_all_chunks()
            return len(self._chunks) if self._chunks else 0

# ===== íƒ€ì… ì •ì˜ =====
class ModelSelection(Enum):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤"""
    GPT4O = "gpt-4o"
    GPT4O_MINI = "gpt-4o-mini"
    O4_MINI = "o4-mini"  # ì¶”ë¡  íŠ¹í™” ëª¨ë¸

class ChunkDict(TypedDict):
    """ì²­í¬ ë°ì´í„°ì˜ íƒ€ì… ì •ì˜"""
    chunk_id: str
    content: str
    source: str
    page: int
    chunk_type: str
    metadata: str

class AnalysisResult(TypedDict):
    """GPT ë¶„ì„ ê²°ê³¼ì˜ íƒ€ì… ì •ì˜"""
    query_analysis: dict
    legal_concepts: list
    search_strategy: dict
    answer_requirements: dict
    model_used: str

# ===== ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')
    
    @property
    def is_latest(self) -> bool:
        """ìµœì‹  ìë£Œ ì—¬ë¶€ í™•ì¸"""
        return self.metadata.get('is_latest', False)

@dataclass
class SearchError:
    """ê²€ìƒ‰ ì¤‘ ë°œìƒí•œ ì—ëŸ¬ ì •ë³´"""
    error_type: str
    message: str
    timestamp: float
    context: Dict[str, Any]
    severity: str  # 'critical', 'warning', 'info'

@dataclass
class IntentAnalysis:
    """ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ê²°ê³¼"""
    core_intent: str  # í•µì‹¬ ì˜ë„
    query_type: str  # simple_lookup, complex_analysis, procedural
    target_documents: List[str]  # ê²€ìƒ‰í•´ì•¼ í•  ë¬¸ì„œë“¤
    key_entities: List[str]  # í•µì‹¬ ê°œì²´ë“¤ (íšŒì‚¬, ê±°ë˜ ìœ í˜• ë“±)
    search_keywords: List[str]  # ê²€ìƒ‰ í‚¤ì›Œë“œ
    requires_timeline: bool  # ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”í•œì§€
    requires_calculation: bool  # ê³„ì‚°ì´ í•„ìš”í•œì§€
    complexity_reason: str  # ë³µì¡ë„ íŒë‹¨ ì´ìœ 
    confidence: float  # ë¶„ì„ ì‹ ë¢°ë„

class QueryComplexity(Enum):
    """ì§ˆë¬¸ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"

# ===== ë³µì¡ë„ í‰ê°€ê¸° =====
class ComplexityAssessor:
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •"""
    
    def __init__(self):
        self.simple_indicators = [
            r'ì–¸ì œ', r'ë©°ì¹ ', r'ê¸°í•œ', r'ë‚ ì§œ', r'ê¸ˆì•¡', r'%', r'ì–¼ë§ˆ',
            r'ì •ì˜[ê°€ëŠ”]?', r'ë¬´ì—‡', r'ëœ»[ì´ì€]?', r'ì˜ë¯¸[ê°€ëŠ”]?'
        ]
        
        self.complex_indicators = [
            r'ë™ì‹œì—', r'ì—¬ëŸ¬', r'ë³µí•©', r'ì—°ê´€', r'ì˜í–¥',
            r'ë§Œ[ì•½ì¼].*ê²½ìš°', r'[AB].*ë™ì‹œ.*[CD]', r'ê±°ë˜.*ì—¬ëŸ¬',
            r'ì „ì²´ì ', r'ì¢…í•©ì ', r'ë¶„ì„', r'ê²€í† ', r'í‰ê°€',
            r'ë¦¬ìŠ¤í¬', r'ìœ„í—˜', r'ëŒ€ì‘', r'ì „ëµ'
        ]
        
        self.medium_indicators = [
            r'ì–´ë–»ê²Œ', r'ë°©ë²•', r'ì ˆì°¨', r'ê³¼ì •',
            r'ì£¼ì˜', r'ì˜ˆì™¸', r'íŠ¹ë³„', r'ê³ ë ¤'
        ]
    
    def assess(self, query: str) -> Tuple[QueryComplexity, float, Dict]:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ê³  ê´€ë ¨ ì •ë³´ ë°˜í™˜"""
        query_lower = query.lower()
        
        simple_score = sum(1 for pattern in self.simple_indicators 
                         if re.search(pattern, query_lower))
        complex_score = sum(2 for pattern in self.complex_indicators 
                          if re.search(pattern, query_lower))
        medium_score = sum(1.5 for pattern in self.medium_indicators 
                         if re.search(pattern, query_lower))
        
        # ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        if len(query) > 150:
            complex_score += 2
        elif len(query) > 100:
            complex_score += 1
        elif len(query) < 30:
            simple_score += 0.5
            
        # ë³µìˆ˜ ì§ˆë¬¸ ì—¬ë¶€
        if query.count('?') > 1 or re.search(r'ê·¸ë¦¬ê³ .*[?]', query_lower):
            complex_score += 1.5
            
        total_score = simple_score + medium_score + complex_score
        
        # ë³µì¡ë„ ê²°ì •
        if total_score == 0:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.5
        elif complex_score > simple_score * 2:
            complexity = QueryComplexity.COMPLEX
            confidence = min(complex_score / (total_score + 1), 0.9)
        elif simple_score > complex_score * 2:
            complexity = QueryComplexity.SIMPLE
            confidence = min(simple_score / (total_score + 1), 0.9)
        else:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.6
            
        analysis = {
            'simple_score': simple_score,
            'medium_score': medium_score,
            'complex_score': complex_score,
            'query_length': len(query),
            'confidence': confidence
        }
        
        return complexity, confidence, analysis

# ===== LRU ìºì‹œ êµ¬í˜„ =====
class LRUCache:
    """ì‹œê°„ ê¸°ë°˜ ë§Œë£Œë¥¼ ì§€ì›í•˜ëŠ” LRU ìºì‹œ êµ¬í˜„"""
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl
        
    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ë§Œë£Œëœ í•­ëª©ì€ ì œê±°"""
        if key not in self.cache:
            return None
            
        value, timestamp = self.cache[key]
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
            
        # LRU: ìµœê·¼ ì‚¬ìš© í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™
        self.cache.move_to_end(key)
        return value
        
    def put(self, key: str, value):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if key in self.cache:
            del self.cache[key]
            
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            self.cache.popitem(last=False)
            
        self.cache[key] = (value, time.time())
        
    def clear_expired(self):
        """ë§Œë£Œëœ ëª¨ë“  í•­ëª© ì œê±°"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]

# ===== ë¬¸ì„œ ë²„ì „ ê´€ë¦¬ =====
class DocumentVersionManager:
    """ë¬¸ì„œì˜ ë²„ì „ê³¼ ìµœì‹ ì„±ì„ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.regulation_changes = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜_ê¸ˆì•¡ê¸°ì¤€': [
                {'date': '2023-01-01', 'old_value': '50ì–µì›', 'new_value': '100ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 100ì–µì› ì´ìƒ'},
                {'date': '2020-01-01', 'old_value': '30ì–µì›', 'new_value': '50ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 50ì–µì› ì´ìƒ'}
            ],
            'ê³µì‹œ_ê¸°í•œ': [
                # í˜„í–‰ ê·œì •: ì˜ì—…ì¼ 7ì¼
                {'date': '2019-01-01', 'old_value': '7ì¼', 'new_value': 'ì˜ì—…ì¼ 7ì¼',
                 'description': 'ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œ (ì˜ì—…ì¼ ê¸°ì¤€ìœ¼ë¡œ ëª…í™•í™”)'}
            ]
        }
        
        self.critical_patterns = {
            'ê¸ˆì•¡': r'(\d+)ì–µ\s*ì›',
            'ë¹„ìœ¨': r'(\d+(?:\.\d+)?)\s*%',
            'ê¸°í•œ': r'(\d+)\s*ì¼',
            'ë‚ ì§œ': r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        }
    
    def extract_document_date(self, chunk: Dict) -> Optional[str]:
        """ë¬¸ì„œì—ì„œ ì‘ì„±/ê°œì • ë‚ ì§œ ì¶”ì¶œ"""
        content = chunk.get('content', '')
        metadata = json.loads(chunk.get('metadata', '{}'))
        
        # ë©”íƒ€ë°ì´í„°ì— ë‚ ì§œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if 'document_date' in metadata:
            return metadata['document_date']
        
        # ë‚´ìš©ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*ê°œì •',
            r'ì‹œí–‰ì¼\s*:\s*(\d{4})ë…„\s*(\d{1,2})ì›”',
            r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                return self._normalize_date(match.group(0))
        
        return None
    
    def _normalize_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # ìˆ«ìë§Œ ì¶”ì¶œ
        numbers = re.findall(r'\d+', date_str)
        if len(numbers) >= 2:
            year = numbers[0] if len(numbers[0]) == 4 else '20' + numbers[0]
            month = numbers[1].zfill(2)
            day = numbers[2].zfill(2) if len(numbers) > 2 else '01'
            return f"{year}-{month}-{day}"
        return None
    
    def check_for_outdated_info(self, content: str, document_date: str = None) -> List[Dict]:
        """êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        warnings = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ í™•ì¸
        amount_match = re.search(r'(\d+)ì–µ\s*ì›.*ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', content)
        if amount_match:
            amount = int(amount_match.group(1))
            if amount == 50:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '50ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
            elif amount == 30:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '30ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
        
        # ê³µì‹œ ê¸°í•œ í™•ì¸ - ì˜ì—…ì¼ ëª…ì‹œ ì—¬ë¶€ ì²´í¬
        deadline_match = re.search(r'ì˜ê²°.*?(\d+)\s*ì¼.*?ê³µì‹œ', content)
        if deadline_match:
            # "ì˜ì—…ì¼" ëª…ì‹œ ì—¬ë¶€ í™•ì¸
            context = content[max(0, deadline_match.start()-20):deadline_match.end()+20]
            if 'ì˜ì—…ì¼' not in context and 'ì˜ì—…ì¼' not in content[max(0, deadline_match.start()-50):deadline_match.end()+50]:
                # ì˜ì—…ì¼ì´ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° ê²½ê³ 
                warnings.append({
                    'type': 'unclear_deadline',
                    'found': f'{deadline_match.group(1)}ì¼',
                    'current': 'ì˜ì—…ì¼ 7ì¼',
                    'regulation': 'ê³µì‹œ ê¸°í•œ',
                    'changed_date': '2019-01-01',
                    'severity': 'warning',
                    'note': 'ì˜ì—…ì¼ ê¸°ì¤€ì„ì„ ëª…í™•íˆ í•´ì•¼ í•¨'
                })
        
        return warnings

# ===== ì¶©ëŒ í•´ê²° ì‹œìŠ¤í…œ =====
class ConflictResolver:
    """ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, version_manager: DocumentVersionManager):
        self.version_manager = version_manager
    
    def resolve_conflicts(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        
        # ê° ê²°ê³¼ì˜ ë‚ ì§œì™€ êµ¬ë²„ì „ ì •ë³´ í™•ì¸
        has_outdated = False
        for result in results:
            doc_date = result.document_date
            warnings = self.version_manager.check_for_outdated_info(result.content, doc_date)
            
            if warnings:
                result.metadata['warnings'] = warnings
                result.metadata['has_outdated_info'] = True
                has_outdated = True
            else:
                result.metadata['has_outdated_info'] = False
        
        # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ ë° ì¶©ëŒ í™•ì¸
        critical_info = self._extract_critical_info(results, query)
        if critical_info:
            conflicts = self._find_conflicts(critical_info)
            if conflicts:
                results = self._prioritize_latest_info(results, conflicts)
        
        # ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda r: (
            not r.metadata.get('has_outdated_info', False),  # ìµœì‹  ì •ë³´ ìš°ì„ 
            r.document_date or '1900-01-01',  # ìµœì‹  ë‚ ì§œ ìš°ì„ 
            r.score  # ê´€ë ¨ë„ ì ìˆ˜
        ), reverse=True)
        
        return results, has_outdated
    
    def _extract_critical_info(self, results: List[SearchResult], query: str) -> Dict:
        """ê²°ê³¼ì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ"""
        critical_info = defaultdict(list)
        
        for i, result in enumerate(results):
            # ê¸ˆì•¡ ì •ë³´
            amounts = re.findall(r'(\d+)ì–µ\s*ì›', result.content)
            for amount in amounts:
                critical_info['amounts'].append({
                    'value': amount + 'ì–µì›',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            # ë¹„ìœ¨ ì •ë³´
            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', result.content)
            for pct in percentages:
                critical_info['percentages'].append({
                    'value': pct + '%',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            # ê¸°í•œ ì •ë³´
            deadlines = re.findall(r'(\d+)\s*ì¼', result.content)
            for deadline in deadlines:
                critical_info['deadlines'].append({
                    'value': deadline + 'ì¼',
                    'result_index': i,
                    'context': result.content[:100]
                })
        
        return dict(critical_info)
    
    def _find_conflicts(self, critical_info: Dict) -> List[Dict]:
        """ì¤‘ìš” ì •ë³´ ê°„ ì¶©ëŒ ì°¾ê¸°"""
        conflicts = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ì¶©ëŒ í™•ì¸
        if 'amounts' in critical_info:
            amount_values = set()
            for item in critical_info['amounts']:
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in item['context']:
                    amount_values.add(item['value'])
            
            if len(amount_values) > 1 and ('50ì–µì›' in amount_values or '30ì–µì›' in amount_values):
                conflicts.append({
                    'type': 'amount_conflict',
                    'values': list(amount_values),
                    'correct_value': '100ì–µì›'
                })
        
        # ê³µì‹œ ê¸°í•œ ëª…í™•ì„± í™•ì¸ (ì¶©ëŒì´ ì•„ë‹Œ ëª…í™•ì„± ì²´í¬ë¡œ ë³€ê²½)
        if 'deadlines' in critical_info:
            unclear_deadlines = []
            for item in critical_info['deadlines']:
                if 'ê³µì‹œ' in item['context'] and 'ì˜ê²°' in item['context']:
                    # ì˜ì—…ì¼ ëª…ì‹œ ì—¬ë¶€ í™•ì¸
                    if 'ì˜ì—…ì¼' not in item['context']:
                        unclear_deadlines.append(item['value'])
            
            if unclear_deadlines:
                conflicts.append({
                    'type': 'deadline_clarity',
                    'values': list(unclear_deadlines),
                    'correct_value': 'ì˜ì—…ì¼ 7ì¼',
                    'issue': 'ì˜ì—…ì¼ ê¸°ì¤€ì„ì„ ëª…ì‹œí•´ì•¼ í•¨'
                })
        
        return conflicts
    
    def _prioritize_latest_info(self, results: List[SearchResult], conflicts: List[Dict]) -> List[SearchResult]:
        """ì¶©ëŒì´ ìˆì„ ë•Œ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        for conflict in conflicts:
            if conflict['type'] == 'amount_conflict':
                # êµ¬ë²„ì „ ê¸ˆì•¡ì´ í¬í•¨ëœ ê²°ê³¼ì˜ ì ìˆ˜ ê°ì†Œ
                for i, result in enumerate(results):
                    if any(old_val in result.content for old_val in ['50ì–µì›', '30ì–µì›']):
                        results[i].score *= 0.5  # ì ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
                        results[i].metadata['score_reduced'] = True
                        results[i].metadata['reduction_reason'] = 'outdated_amount'
            
            elif conflict['type'] == 'deadline_clarity':
                # ì˜ì—…ì¼ì´ ëª…ì‹œë˜ì§€ ì•Šì€ ê²°ê³¼ì˜ ì ìˆ˜ ì•½ê°„ ê°ì†Œ
                for i, result in enumerate(results):
                    deadline_match = re.search(r'ì˜ê²°.*?(\d+)\s*ì¼.*?ê³µì‹œ', result.content)
                    if deadline_match:
                        context = result.content[max(0, deadline_match.start()-50):deadline_match.end()+50]
                        if 'ì˜ì—…ì¼' not in context:
                            results[i].score *= 0.85  # ì ìˆ˜ë¥¼ 15% ê°ì†Œ
                            results[i].metadata['score_reduced'] = True
                            results[i].metadata['reduction_reason'] = 'unclear_deadline_specification'
                            results[i].metadata['clarification_needed'] = 'ì˜ì—…ì¼ ê¸°ì¤€ì„ì„ ëª…ì‹œ í•„ìš”'
        
        return results

# ===== 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ êµ¬í˜„ =====

class Step1_IntentAnalyzer:
    """Step 1: ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ íŒŒì•…
    
    ì´ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬:
    - ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…
    - ê²€ìƒ‰í•´ì•¼ í•  ë¬¸ì„œë¥¼ ê²°ì •
    - ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
    """
    
    def __init__(self, api_manager: APIManager):
        self.api_manager = api_manager
        self.complexity_assessor = ComplexityAssessor()
        self.analysis_cache = LRUCache(max_size=100, ttl=3600)
        
    async def analyze_intent(self, query: str) -> IntentAnalysis:
        """ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„"""
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(query.encode()).hexdigest()
        cached = self.analysis_cache.get(cache_key)
        if cached:
            logger.info("Using cached intent analysis")
            return cached
        
        # ë³µì¡ë„ í‰ê°€
        complexity, confidence, complexity_analysis = self.complexity_assessor.assess(query)
        
        # ëª¨ë¸ ì„ íƒ (ì˜ë„ ë¶„ì„ì€ ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©)
        model = "gpt-4o-mini"
        
        prompt = f"""
ë‹¹ì‹ ì€ ê³µì •ê±°ë˜ìœ„ì›íšŒ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ì–´ë–¤ ë§¤ë‰´ì–¼ì„ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ë‰´ì–¼:
1. ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼: ê³„ì—´ì‚¬ ê°„ ìê¸ˆê±°ë˜, ìì‚°ê±°ë˜, ìƒí’ˆìš©ì—­ê±°ë˜ ê´€ë ¨ ê·œì •
2. í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼: ê¸°ì—…ì§‘ë‹¨ í˜„í™©ê³µì‹œ, ê³µì‹œ ì˜ë¬´ì‚¬í•­
3. ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼: ë¹„ìƒì¥íšŒì‚¬ì˜ ì£¼ì‹ ì–‘ë„, í•©ë³‘, ë¶„í•  ë“±

ì‚¬ìš©ì ì§ˆë¬¸: {query}
ì§ˆë¬¸ ë³µì¡ë„: {complexity.value} (ì‹ ë¢°ë„: {confidence:.1%})

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”:

{{
    "core_intent": "í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„",
    "query_type": "simple_lookup/complex_analysis/procedural ì¤‘ ì„ íƒ",
    "target_documents": ["ê²€ìƒ‰í•  ë§¤ë‰´ì–¼ ì´ë¦„ë“¤"],
    "key_entities": ["ì§ˆë¬¸ì— í¬í•¨ëœ í•µì‹¬ ê°œì²´ë“¤ (ì˜ˆ: ê³„ì—´ì‚¬, ìê¸ˆ, ì´ì‚¬íšŒ ë“±)"],
    "search_keywords": ["ë§¤ë‰´ì–¼ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œ 5-10ê°œ"],
    "requires_timeline": true/false,
    "requires_calculation": true/false,
    "complexity_reason": "ë³µì¡ë„ íŒë‹¨ì˜ êµ¬ì²´ì  ì´ìœ ",
    "confidence": 0.0-1.0
}}

ë¶„ì„ ì‹œ ê³ ë ¤ì‚¬í•­:
- query_type íŒë‹¨ ê¸°ì¤€:
  - simple_lookup: ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ (ê¸°í•œ, ê¸ˆì•¡ ë“±)
  - complex_analysis: ì—¬ëŸ¬ ì¡°ê±´ì´ ê²°í•©ëœ ë³µì¡í•œ ìƒí™©
  - procedural: ì ˆì°¨ë‚˜ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì§ˆë¬¸
- ì§ˆë¬¸ì— ì—¬ëŸ¬ ë§¤ë‰´ì–¼ì´ ê´€ë ¨ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì‹ ì¤‘íˆ íŒë‹¨
- search_keywordsëŠ” ì‹¤ì œ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ìš©ì–´ë¡œ ì„ ì •
- ë²•ë¥  ìš©ì–´ì™€ ì¼ìƒ ìš©ì–´ë¥¼ ëª¨ë‘ í¬í•¨ (ì˜ˆ: 'ëŒ€ì—¬'ì™€ 'ìê¸ˆê±°ë˜' ëª¨ë‘ í¬í•¨)
"""
        
        try:
            response = await openai.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            
            analysis_data = json.loads(response.choices[0].message.content)
            
            intent = IntentAnalysis(
                core_intent=analysis_data.get("core_intent", ""),
                query_type=analysis_data.get("query_type", "simple_lookup"),
                target_documents=analysis_data.get("target_documents", []),
                key_entities=analysis_data.get("key_entities", []),
                search_keywords=analysis_data.get("search_keywords", []),
                requires_timeline=analysis_data.get("requires_timeline", False),
                requires_calculation=analysis_data.get("requires_calculation", False),
                complexity_reason=analysis_data.get("complexity_reason", ""),
                confidence=analysis_data.get("confidence", 0.8)
            )
            
            # ìºì‹œ ì €ì¥
            self.analysis_cache.put(cache_key, intent)
            
            return intent
            
        except Exception as e:
            logger.error(f"Intent analysis failed: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return IntentAnalysis(
                core_intent=query,
                query_type="simple_lookup",
                target_documents=["ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼", "í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼", "ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼"],
                key_entities=[],
                search_keywords=query.split()[:5],
                requires_timeline=False,
                requires_calculation=False,
                complexity_reason="ìë™ ë¶„ì„ ì‹¤íŒ¨",
                confidence=0.5
            )

class Step2_DocumentSearcher:
    """Step 2: ì˜ë„ì— ë§ëŠ” ë§¤ë‰´ì–¼ ê²€ìƒ‰
    
    ì´ ë‹¨ê³„ì—ì„œëŠ” Step 1ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
    - ê´€ë ¨ ë¬¸ì„œë¥¼ ë²¡í„° ê²€ìƒ‰
    - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬ ë° í•„í„°ë§
    - ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ ì„ ë³„
    """
    
    def __init__(self, index, chunk_loader: ChunkLoader, embedding_model, api_manager: APIManager):
        self.index = index
        self.chunk_loader = chunk_loader
        self.embedding_model = embedding_model
        self.api_manager = api_manager
        self.use_faiss = index is not None
        self._embedding_cache = {}
        
        # SimpleVectorSearch ì´ˆê¸°í™” (FAISSê°€ ì—†ì„ ë•Œ)
        if not self.use_faiss:
            self._initialize_simple_search()
        
        # ë§¤ë‰´ì–¼ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.manual_indices = self._build_manual_indices()
        
        # ë¬¸ì„œ ë²„ì „ ê´€ë¦¬ ë° ì¶©ëŒ í•´ê²°
        self.version_manager = DocumentVersionManager()
        self.conflict_resolver = ConflictResolver(self.version_manager)
        
    def _initialize_simple_search(self):
        """SimpleVectorSearch ì´ˆê¸°í™”"""
        embeddings_file = "chunk_embeddings.npy"
        if os.path.exists(embeddings_file):
            try:
                embeddings = np.load(embeddings_file)
                self.simple_search = SimpleVectorSearch(embeddings)
                logger.info(f"Loaded {len(embeddings)} pre-computed embeddings")
            except Exception as e:
                logger.warning(f"Failed to load embeddings: {e}")
                self._create_simple_search()
        else:
            self._create_simple_search()
    
    def _create_simple_search(self):
        """ê°„ë‹¨í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒì„±"""
        logger.info("Creating embeddings for simple search...")
        
        total_chunks = self.chunk_loader.get_total_chunks()
        max_chunks = min(total_chunks, 1000)  # ë°ëª¨ë¥¼ ìœ„í•´ ì œí•œ
        
        embeddings = []
        
        if self.embedding_model is not None:
            # sentence-transformers ì‚¬ìš© ê°€ëŠ¥
            for i in range(max_chunks):
                try:
                    chunk = self.chunk_loader.get_chunk(i)
                    text = chunk['content'][:500]
                    embedding = self.embedding_model.encode([text])
                    embeddings.append(embedding[0])
                except Exception as e:
                    logger.warning(f"Failed to create embedding for chunk {i}: {e}")
                    embeddings.append(np.random.randn(384))
        else:
            # OpenAI embeddings ì‚¬ìš©
            logger.info("Using OpenAI embeddings...")
            for i in range(max_chunks):
                try:
                    chunk = self.chunk_loader.get_chunk(i)
                    text = chunk['content'][:500]
                    embedding = self.api_manager.get_embedding(text)
                    embeddings.append(embedding)
                except Exception as e:
                    logger.warning(f"Failed to create OpenAI embedding for chunk {i}: {e}")
                    embeddings.append(np.random.randn(1536))
        
        embeddings_array = np.array(embeddings, dtype=np.float32)
        
        # ì„ë² ë”© ì €ì¥
        try:
            np.save("chunk_embeddings.npy", embeddings_array)
            logger.info("Saved embeddings for future use")
        except Exception as e:
            logger.warning(f"Failed to save embeddings: {e}")
        
        self.simple_search = SimpleVectorSearch(embeddings_array)
    
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx in range(self.chunk_loader.get_total_chunks()):
            try:
                chunk = self.chunk_loader.get_chunk(idx)
                source = chunk.get('source', '').lower()
                
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                    indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼'].append(idx)
                elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                    indices['í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼'].append(idx)
                elif 'ë¹„ìƒì¥' in source:
                    indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼'].append(idx)
                else:
                    indices['ê¸°íƒ€'].append(idx)
                    
                # ë¬¸ì„œ ë‚ ì§œ ì¶”ì¶œ ë° ì €ì¥
                doc_date = self.version_manager.extract_document_date(chunk)
                if doc_date:
                    metadata = json.loads(chunk.get('metadata', '{}'))
                    metadata['document_date'] = doc_date
                    chunk['metadata'] = json.dumps(metadata)
                    
            except Exception as e:
                logger.warning(f"Failed to process chunk {idx}: {e}")
                continue
        
        return dict(indices)
    
    async def search_documents(self, intent: IntentAnalysis, top_k: int = 10) -> Tuple[List[SearchResult], bool]:
        """ì˜ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰"""
        
        # ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ê²°ì •
        search_indices = []
        for doc_name in intent.target_documents:
            if doc_name in self.manual_indices:
                search_indices.extend(self.manual_indices[doc_name])
        
        if not search_indices:
            # ëª¨ë“  ë¬¸ì„œì—ì„œ ê²€ìƒ‰
            search_indices = list(range(self.chunk_loader.get_total_chunks()))
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (í‚¤ì›Œë“œ ê²°í•©)
        search_query = f"{intent.core_intent} {' '.join(intent.search_keywords)}"
        
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        if self.use_faiss:
            results = await self._vector_search(search_query, search_indices, top_k * 2)
        else:
            results = self._keyword_search(search_query, search_indices, top_k * 2)
        
        # ì˜ë„ì— ë§ê²Œ ê²°ê³¼ ì¬ì •ë ¬
        results = self._rerank_by_intent(results, intent)
        
        # ì¶©ëŒ í•´ê²° ë° ìµœì‹  ì •ë³´ ìš°ì„ ì‹œ
        results, has_outdated = self.conflict_resolver.resolve_conflicts(results, search_query)
        
        return results[:top_k], has_outdated
    
    async def _vector_search(self, query: str, indices: List[int], k: int) -> List[SearchResult]:
        """ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_vector = self._get_query_embedding(query)
        query_vector = query_vector.reshape(1, -1).astype(np.float32)
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        if self.use_faiss:
            scores, search_indices = self.index.search(query_vector, k)
        else:
            scores, search_indices = self.simple_search.search(query_vector, k)
        
        # ê²°ê³¼ ë³€í™˜
        results = []
        for idx, score in zip(search_indices[0], scores[0]):
            if idx in indices and 0 <= idx < self.chunk_loader.get_total_chunks():
                chunk = self.chunk_loader.get_chunk(idx)
                result = SearchResult(
                    chunk_id=str(idx),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk.get('source', 'Unknown'),
                    page=chunk.get('page', 0),
                    chunk_type=chunk.get('chunk_type', 'text'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                )
                results.append(result)
        
        return results
    
    def _get_query_embedding(self, query: str) -> np.ndarray:
        """ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ìºì‹œ í™œìš©)"""
        cache_key = hashlib.md5(query.encode()).hexdigest()
        
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]
        
        if self.embedding_model is not None:
            embedding = self.embedding_model.encode([query])[0]
        else:
            embedding = self.api_manager.get_embedding(query)
        
        self._embedding_cache[cache_key] = embedding
        
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self._embedding_cache) > 1000:
            oldest_key = next(iter(self._embedding_cache))
            del self._embedding_cache[oldest_key]
        
        return embedding
    
    def _keyword_search(self, query: str, indices: List[int], k: int) -> List[SearchResult]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (í´ë°±)"""
        query_words = set(query.lower().split())
        scored_chunks = []
        
        for idx in indices[:1000]:  # ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ
            try:
                chunk = self.chunk_loader.get_chunk(idx)
                content_lower = chunk['content'].lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                score = sum(1 for word in query_words if word in content_lower)
                
                if score > 0:
                    scored_chunks.append((idx, score, chunk))
            except Exception as e:
                continue
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_chunks.sort(key=lambda x: x[1], reverse=True)
        
        # SearchResult ìƒì„±
        results = []
        for idx, score, chunk in scored_chunks[:k]:
            result = SearchResult(
                chunk_id=str(idx),
                content=chunk['content'],
                score=float(score),
                source=chunk.get('source', 'Unknown'),
                page=chunk.get('page', 0),
                chunk_type=chunk.get('chunk_type', 'text'),
                metadata=json.loads(chunk.get('metadata', '{}'))
            )
            results.append(result)
        
        return results
    
    def _rerank_by_intent(self, results: List[SearchResult], intent: IntentAnalysis) -> List[SearchResult]:
        """ì˜ë„ì— ë§ê²Œ ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬"""
        
        for result in results:
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            keyword_matches = sum(1 for kw in intent.search_keywords 
                                if kw.lower() in result.content.lower())
            result.score += keyword_matches * 0.1
            
            # ê°œì²´ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            entity_matches = sum(1 for entity in intent.key_entities 
                               if entity.lower() in result.content.lower())
            result.score += entity_matches * 0.15
            
            # íŠ¹ì • ì¡°ê±´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
            if intent.requires_calculation and re.search(r'\d+ì–µ|\d+%', result.content):
                result.score *= 1.2
            
            if intent.requires_timeline and re.search(r'\d+ì¼|ê¸°í•œ|ë‚ ì§œ', result.content):
                result.score *= 1.2
            
            # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
            if intent.query_type == "procedural" and re.search(r'ì ˆì°¨|ë‹¨ê³„|ê³¼ì •', result.content):
                result.score *= 1.15
        
        # ì¬ì •ë ¬
        results.sort(key=lambda x: x.score, reverse=True)
        return results

class Step3_AnswerGenerator:
    """Step 3: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    
    ì´ ë‹¨ê³„ì—ì„œëŠ”:
    - ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ ì˜ë„ë¥¼ ê²°í•©
    - ì ì ˆí•œ ëª¨ë¸ ì„ íƒ (o4-mini, gpt-4o ë“±)
    - ì²´ê³„ì ì´ê³  ì •í™•í•œ ë‹µë³€ ìƒì„±
    """
    
    def __init__(self, api_manager: APIManager):
        self.api_manager = api_manager
        
    async def generate_answer(self, 
                            query: str, 
                            intent: IntentAnalysis,
                            search_results: List[SearchResult],
                            has_outdated_info: bool = False) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        # ëª¨ë¸ ì„ íƒ
        model = self._select_model(intent)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(search_results)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if model == "o4-mini":
            prompt = self._build_reasoning_prompt(query, intent, context, has_outdated_info)
        else:
            prompt = self._build_standard_prompt(query, intent, context, has_outdated_info)
        
        # ë‹µë³€ ìƒì„±
        try:
            response = await openai.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=2000
            )
            
            answer = response.choices[0].message.content
            
            # ë‹µë³€ í›„ì²˜ë¦¬
            answer = self._postprocess_answer(answer, intent, has_outdated_info)
            
            return answer
            
        except Exception as e:
            logger.error(f"Answer generation failed: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _select_model(self, intent: IntentAnalysis) -> str:
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ ì„ íƒ"""
        
        if intent.query_type == "complex_analysis":
            # ë³µì¡í•œ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ì¶”ë¡  ëª¨ë¸ ì‚¬ìš©
            return "o4-mini"
        elif intent.query_type == "simple_lookup":
            # ë‹¨ìˆœ ì¡°íšŒëŠ” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            return "gpt-4o-mini"
        else:
            # ì ˆì°¨ì  ì§ˆë¬¸ì€ ë²”ìš© ëª¨ë¸ ì‚¬ìš©
            return "gpt-4o"
    
    def _build_context(self, search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        for i, result in enumerate(search_results[:5]):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            # êµ¬ë²„ì „ ì •ë³´ ê²½ê³  í¬í•¨
            warnings = result.metadata.get('warnings', [])
            warning_text = ""
            if warnings:
                warning_text = "\nâš ï¸ ì£¼ì˜: ì´ ë¬¸ì„œì—ëŠ” ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            
            context_parts.append(f"""
[ì°¸ê³ ìë£Œ {i+1}]
ì¶œì²˜: {result.source} (í˜ì´ì§€ {result.page}){warning_text}
ë‚´ìš©: {result.content}
""")
        
        return "\n---\n".join(context_parts)
    
    def _build_reasoning_prompt(self, query: str, intent: IntentAnalysis, context: str, has_outdated_info: bool) -> str:
        """o4-miniìš© ì¶”ë¡  ì¤‘ì‹¬ í”„ë¡¬í”„íŠ¸"""
        
        outdated_warning = ""
        if has_outdated_info:
            outdated_warning = """
[ì¤‘ìš” ì‚¬í•­]
ì¼ë¶€ ì°¸ê³ ìë£Œì— ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€: í˜„ì¬ 100ì–µì› (2023ë…„ 1ì›” 1ì¼ë¶€í„°)
- ê³µì‹œ ê¸°í•œ: ì˜ì—…ì¼ ê¸°ì¤€ 7ì¼
ìƒì¶©í•˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œí•˜ì„¸ìš”.
"""
        
        return f"""
ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì§ˆë¬¸ ì˜ë„ ë¶„ì„]
- í•µì‹¬ ì˜ë„: {intent.core_intent}
- ê´€ë ¨ ê°œì²´: {', '.join(intent.key_entities)}
- ì‹œê°„ìˆœì„œ ì¤‘ìš”: {'ì˜ˆ' if intent.requires_timeline else 'ì•„ë‹ˆì˜¤'}
- ê³„ì‚° í•„ìš”: {'ì˜ˆ' if intent.requires_calculation else 'ì•„ë‹ˆì˜¤'}

[ì°¸ê³ ìë£Œ]
{context}
{outdated_warning}

[ë‹µë³€ ì‘ì„± ì§€ì¹¨]
1. ë¨¼ì € ì§ˆë¬¸ì˜ ê° ìš”ì†Œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
2. ê´€ë ¨ ê·œì •ê³¼ ì¡°ê±´ì„ ë‹¨ê³„ë³„ë¡œ í™•ì¸í•˜ì„¸ìš”
3. í•„ìš”í•œ ê²½ìš° ê³„ì‚° ê³¼ì •ì„ ëª…ì‹œí•˜ì„¸ìš”
4. ìµœì‹  ê·œì •ì„ ê¸°ì¤€ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
5. ì‹¤ë¬´ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì§€ì¹¨ì„ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€ í˜•ì‹:
## í•µì‹¬ ë‹µë³€
(1-2ë¬¸ì¥ìœ¼ë¡œ ì§ì ‘ì ì¸ ë‹µë³€)

## ìƒì„¸ ë¶„ì„
### 1. ì ìš© ê·œì •
- ê´€ë ¨ ì¡°í•­ê³¼ ê¸°ì¤€

### 2. êµ¬ì²´ì  ê²€í† 
- ë‹¨ê³„ë³„ ë¶„ì„ ë‚´ìš©
- í•„ìš”ì‹œ ê³„ì‚° ê³¼ì •

### 3. ì£¼ì˜ì‚¬í•­
- ì˜ˆì™¸ì‚¬í•­ì´ë‚˜ íŠ¹ë³„íˆ ìœ ì˜í•  ì 

## ê²°ë¡ 
(ìµœì¢… ì •ë¦¬ ë° ì‹¤ë¬´ ì§€ì¹¨)
"""
    
    def _build_standard_prompt(self, query: str, intent: IntentAnalysis, context: str, has_outdated_info: bool) -> str:
        """í‘œì¤€ ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸"""
        
        outdated_warning = ""
        if has_outdated_info:
            outdated_warning = """
âš ï¸ ì°¸ê³ : ì¼ë¶€ ìë£Œì— ê°œì • ì „ ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ìµœì‹  ê·œì • - ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜: 100ì–µì›, ê³µì‹œê¸°í•œ: ì˜ì—…ì¼ 7ì¼
"""
        
        return f"""
ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì°¸ê³ ìë£Œ]
{context}
{outdated_warning}

[ë‹µë³€ ì§€ì¹¨]
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì œì‹œí•˜ê³  ìƒì„¸ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”
- ê·¼ê±° ì¡°í•­ì„ ëª…í™•íˆ ì¸ìš©í•˜ì„¸ìš”
- ì‹¤ë¬´ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
- ê´€ë ¨ ì£¼ì˜ì‚¬í•­ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”
- ìµœì‹  ê·œì •ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”

ë‹µë³€:
"""
    
    def _postprocess_answer(self, answer: str, intent: IntentAnalysis, has_outdated_info: bool) -> str:
        """ë‹µë³€ í›„ì²˜ë¦¬ - ìµœì‹  ì •ë³´ í™•ì¸ ë“±"""
        
        # ìµœì‹  ê·œì • ì •ë³´ ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°ë§Œ)
        if has_outdated_info and "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜" in ' '.join(intent.target_documents):
            if "50ì–µ" in answer or "30ì–µ" in answer:
                answer += "\n\nâš ï¸ **ì¤‘ìš”**: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸°ì¤€ ê¸ˆì•¡ì€ 2023ë…„ 1ì›” 1ì¼ë¶€í„° 100ì–µì›ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        if has_outdated_info and "ê³µì‹œ" in answer and "7ì¼" in answer:
            if "ì˜ì—…ì¼" not in answer:
                answer += "\n\nğŸ“Œ **ì°¸ê³ **: ê³µì‹œ ê¸°í•œì€ ì˜ì—…ì¼ ê¸°ì¤€ 7ì¼ì…ë‹ˆë‹¤."
        
        return answer

# ===== í†µí•© RAG íŒŒì´í”„ë¼ì¸ =====
class IntegratedRAGPipeline:
    """3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•©í•œ RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, embedding_model, index, chunk_loader: ChunkLoader, api_manager: APIManager):
        self.step1_analyzer = Step1_IntentAnalyzer(api_manager)
        self.step2_searcher = Step2_DocumentSearcher(index, chunk_loader, embedding_model, api_manager)
        self.step3_generator = Step3_AnswerGenerator(api_manager)
        
    async def process_query(self, query: str) -> Tuple[str, Dict]:
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        stats = {
            "process_times": {},
            "intent_analysis": None,
            "search_results_count": 0,
            "has_outdated_info": False
        }
        
        try:
            # Step 1: ì˜ë„ ë¶„ì„
            start_time = time.time()
            intent = await self.step1_analyzer.analyze_intent(query)
            stats["process_times"]["intent_analysis"] = time.time() - start_time
            stats["intent_analysis"] = intent
            
            # Step 2: ë¬¸ì„œ ê²€ìƒ‰
            start_time = time.time()
            search_results, has_outdated = await self.step2_searcher.search_documents(intent)
            stats["process_times"]["document_search"] = time.time() - start_time
            stats["search_results_count"] = len(search_results)
            stats["has_outdated_info"] = has_outdated
            stats["search_results"] = search_results  # ìƒì„¸ ì •ë³´ í‘œì‹œë¥¼ ìœ„í•´ ì €ì¥
            
            # Step 3: ë‹µë³€ ìƒì„±
            start_time = time.time()
            answer = await self.step3_generator.generate_answer(query, intent, search_results, has_outdated)
            stats["process_times"]["answer_generation"] = time.time() - start_time
            
            stats["total_time"] = sum(stats["process_times"].values())
            
            return answer, stats
            
        except Exception as e:
            logger.error(f"Pipeline error: {e}")
            return "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", stats

# ===== ë¹„ë™ê¸° ì‹¤í–‰ í—¬í¼ =====
def run_async_in_streamlit(coro):
    """Streamlit í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰"""
    try:
        # ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸ ë° ì‹¤í–‰
        try:
            loop = asyncio.get_running_loop()
            if NEST_ASYNCIO_AVAILABLE:
                import nest_asyncio
                nest_asyncio.apply()
            return asyncio.run(coro)
        except RuntimeError:
            return asyncio.run(coro)
    except Exception as e:
        logger.error(f"Async execution failed: {str(e)}")
        raise

# ===== Streamlit UI =====
st.set_page_config(
    page_title="ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    /* ë©”ì¸ í—¤ë” */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„ í‘œì‹œ */
    .process-step {
        display: inline-block;
        padding: 0.5rem 1rem;
        margin: 0.25rem;
        border-radius: 20px;
        font-size: 0.9rem;
        font-weight: 500;
    }
    
    .step-active {
        background-color: #2a5298;
        color: white;
    }
    
    .step-completed {
        background-color: #28a745;
        color: white;
    }
    
    .step-pending {
        background-color: #e0e0e0;
        color: #666;
    }
    
    /* ì˜ë„ ë¶„ì„ ê²°ê³¼ */
    .intent-box {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    
    /* ê²½ê³  ë©”ì‹œì§€ */
    .outdated-warning {
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        border-radius: 4px;
        padding: 0.75rem 1.25rem;
        margin: 0.5rem 0;
        color: #856404;
    }
    
    /* ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
</style>
""", unsafe_allow_html=True)

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë”©
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    try:
        # API ê´€ë¦¬ì ì´ˆê¸°í™”
        api_manager = APIManager()
        api_manager.load_api_key()
        openai.api_key = api_manager._api_key
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            return None
        
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            index = None
            if os.path.exists("manuals_vector_db.index") and FAISS_AVAILABLE:
                try:
                    import faiss
                    index = faiss.read_index("manuals_vector_db.index")
                    logger.info("FAISS index loaded successfully")
                except Exception as e:
                    logger.warning(f"Failed to load FAISS index: {e}")
            
            # ì²­í¬ ë¡œë” ì´ˆê¸°í™”
            chunk_loader = ChunkLoader("all_manual_chunks.json")
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œë„
            embedding_model = None
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                try:
                    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                    logger.info("Sentence transformer model loaded")
                except Exception as e:
                    logger.warning(f"Failed to load sentence transformer: {e}")
            
            return embedding_model, index, chunk_loader, api_manager
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
            st.code(traceback.format_exc())
        return None

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜, í˜„í™©ê³µì‹œ, ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ê´€ë ¨ ì „ë¬¸ ìƒë‹´</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë¡œë“œ
    models = load_models_and_data()
    if not models or len(models) != 4:
        st.stop()
    
    embedding_model, index, chunk_loader, api_manager = models
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = IntegratedRAGPipeline(embedding_model, index, chunk_loader, api_manager)
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "user":
                st.write(message["content"])
            else:
                st.write(message["answer"])
                
                # êµ¬ë²„ì „ ì •ë³´ ê²½ê³ 
                if message.get("stats", {}).get("has_outdated_info"):
                    st.markdown("""
                    <div class="outdated-warning">
                    âš ï¸ ì¼ë¶€ ì°¸ê³  ìë£Œì— ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    </div>
                    """, unsafe_allow_html=True)
                
                # í”„ë¡œì„¸ìŠ¤ ì •ë³´ í‘œì‹œ
                if "stats" in message:
                    stats = message["stats"]
                    if stats.get("intent_analysis"):
                        with st.expander("ğŸ” ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"):
                            intent = stats["intent_analysis"]
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write(f"**í•µì‹¬ ì˜ë„**: {intent.core_intent}")
                                st.write(f"**ì§ˆë¬¸ ìœ í˜•**: {intent.query_type}")
                                st.write(f"**ê²€ìƒ‰ ëŒ€ìƒ**: {', '.join(intent.target_documents)}")
                            with col2:
                                st.write(f"**í•µì‹¬ í‚¤ì›Œë“œ**: {', '.join(intent.search_keywords[:5])}")
                                st.write(f"**ê´€ë ¨ ê°œì²´**: {', '.join(intent.key_entities)}")
                                st.write(f"**ì‹ ë¢°ë„**: {intent.confidence:.1%}")
                    
                    # ì²˜ë¦¬ ì‹œê°„ í‘œì‹œ
                    if "process_times" in stats:
                        times = stats["process_times"]
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ì˜ë„ ë¶„ì„", f"{times.get('intent_analysis', 0):.1f}ì´ˆ")
                        with col2:
                            st.metric("ë¬¸ì„œ ê²€ìƒ‰", f"{times.get('document_search', 0):.1f}ì´ˆ")
                        with col3:
                            st.metric("ë‹µë³€ ìƒì„±", f"{times.get('answer_generation', 0):.1f}ì´ˆ")
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´
                    if "search_results" in stats and stats["search_results"]:
                        with st.expander("ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ"):
                            for i, result in enumerate(stats["search_results"][:3]):
                                st.caption(f"**{i+1}. {result.source}** (í˜ì´ì§€ {result.page}, ê´€ë ¨ë„: {result.score:.2f})")
                                if result.metadata.get('has_outdated_info'):
                                    st.warning("âš ï¸ ì´ ë¬¸ì„œì—ëŠ” ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ìƒˆ ì§ˆë¬¸ ì…ë ¥
    if prompt := st.chat_input("ê³µì •ê±°ë˜ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.write(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            # í”„ë¡œì„¸ìŠ¤ ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_container = st.container()
            with progress_container:
                col1, col2, col3 = st.columns(3)
                with col1:
                    step1_placeholder = st.empty()
                    step1_placeholder.markdown('<span class="process-step step-active">1ï¸âƒ£ ì§ˆë¬¸ ë¶„ì„ ì¤‘...</span>', unsafe_allow_html=True)
                with col2:
                    step2_placeholder = st.empty()
                    step2_placeholder.markdown('<span class="process-step step-pending">2ï¸âƒ£ ë¬¸ì„œ ê²€ìƒ‰</span>', unsafe_allow_html=True)
                with col3:
                    step3_placeholder = st.empty()
                    step3_placeholder.markdown('<span class="process-step step-pending">3ï¸âƒ£ ë‹µë³€ ìƒì„±</span>', unsafe_allow_html=True)
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            answer, stats = run_async_in_streamlit(pipeline.process_query(prompt))
            
            # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ í‘œì‹œ
            step1_placeholder.markdown('<span class="process-step step-completed">1ï¸âƒ£ ì§ˆë¬¸ ë¶„ì„ âœ“</span>', unsafe_allow_html=True)
            step2_placeholder.markdown('<span class="process-step step-completed">2ï¸âƒ£ ë¬¸ì„œ ê²€ìƒ‰ âœ“</span>', unsafe_allow_html=True)
            step3_placeholder.markdown('<span class="process-step step-completed">3ï¸âƒ£ ë‹µë³€ ìƒì„± âœ“</span>', unsafe_allow_html=True)
            
            # ë‹µë³€ í‘œì‹œ
            st.write(answer)
            
            # êµ¬ë²„ì „ ì •ë³´ ê²½ê³ 
            if stats.get("has_outdated_info"):
                st.markdown("""
                <div class="outdated-warning">
                âš ï¸ ì¼ë¶€ ì°¸ê³  ìë£Œì— ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                </div>
                """, unsafe_allow_html=True)
            
            # ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append({
                "role": "assistant",
                "answer": answer,
                "stats": stats
            })
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“š ì‚¬ìš© ê°€ì´ë“œ")
        
        st.subheader("ì§ˆë¬¸ ì˜ˆì‹œ")
        
        example_questions = [
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œì€?",
            "ê³„ì—´ì‚¬ ê°„ ìê¸ˆ ëŒ€ì—¬ ì‹œ ì´ì‚¬íšŒ ì˜ê²°ì´ í•„ìš”í•œ ê¸ˆì•¡ì€?",
            "ë¹„ìƒì¥íšŒì‚¬ê°€ ì£¼ì‹ì„ ì–‘ë„í•  ë•Œ í•„ìš”í•œ ì ˆì°¨ëŠ”?",
            "ê¸°ì—…ì§‘ë‹¨ í˜„í™©ê³µì‹œëŠ” ì–¸ì œ í•´ì•¼ í•˜ë‚˜ìš”?",
            "ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜í•  ë•Œ ì£¼ì˜ì‚¬í•­ì€?"
        ]
        
        for q in example_questions:
            if st.button(q, key=q):
                st.session_state.new_question = q
                st.rerun()
        
        st.divider()
        
        st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´")
        st.caption("ì´ ì‹œìŠ¤í…œì€ 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‘ë™í•©ë‹ˆë‹¤:")
        st.caption("1ï¸âƒ£ **ì§ˆë¬¸ ì˜ë„ ë¶„ì„**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½")
        st.caption("2ï¸âƒ£ **ë¬¸ì„œ ê²€ìƒ‰**: ê´€ë ¨ ë§¤ë‰´ì–¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ê²€ìƒ‰")
        st.caption("3ï¸âƒ£ **ë‹µë³€ ìƒì„±**: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ìƒì„±")
        
        st.divider()
        
        st.caption("ğŸ’¡ ê° ë‹¨ê³„ì—ì„œ ìµœì í™”ëœ AI ëª¨ë¸ì´ ì‚¬ìš©ë©ë‹ˆë‹¤:")
        st.caption("â€¢ **ì˜ë„ ë¶„ì„**: GPT-4o-mini (ë¹ ë¥¸ ë¶„ì„)")
        st.caption("â€¢ **ë‹µë³€ ìƒì„±**: o4-mini, GPT-4o (ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼)")
        
        st.divider()
        
        st.caption("âš ï¸ **ìµœì‹  ê·œì • ì•ˆë‚´**")
        st.caption("â€¢ ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸°ì¤€: 100ì–µì›")
        st.caption("â€¢ ê³µì‹œ ê¸°í•œ: ì˜ì—…ì¼ 7ì¼")
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
