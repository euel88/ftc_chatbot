# íŒŒì¼ ì´ë¦„: app_manual.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - ì™„ì „ í†µí•© ìµœì í™” ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set, TypedDict, Protocol, Iterator, Generator, OrderedDict, Any, Union
import re
from collections import defaultdict, Counter, OrderedDict
import time
from dataclasses import dataclass, field
import os
import hashlib
from enum import Enum
import asyncio
import logging
from contextlib import contextmanager
import traceback
import gc
import concurrent.futures
from functools import lru_cache
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta

# ===== ë¡œê¹… ì„¤ì • =====
def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •
    
    ë¡œê¹…ì€ í”„ë¡œê·¸ë¨ì˜ ì‘ë™ ìƒí™©ì„ ê¸°ë¡í•˜ëŠ” ì¼ê¸°ì¥ê³¼ ê°™ìŠµë‹ˆë‹¤.
    ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë•Œ ì›ì¸ì„ ì°¾ê±°ë‚˜, ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.
    """
    logger = logging.getLogger('ftc_chatbot')
    logger.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ - ì˜êµ¬ì ì¸ ë¡œê·¸ ê¸°ë¡
    file_handler = logging.FileHandler('ftc_chatbot_debug.log')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ - ê°œë°œ ì¤‘ ì‹¤ì‹œê°„ í™•ì¸
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# ===== ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ í´ë˜ìŠ¤ =====
class RAGPipelineError(Exception):
    """RAG íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

class IndexError(RAGPipelineError):
    """ì¸ë±ìŠ¤ ê´€ë ¨ ì˜¤ë¥˜ - FAISS ì¸ë±ìŠ¤ ë¬¸ì œ ì‹œ ë°œìƒ"""
    pass

class EmbeddingError(RAGPipelineError):
    """ì„ë² ë”© ìƒì„± ê´€ë ¨ ì˜¤ë¥˜ - ë²¡í„° ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë°œìƒ"""
    pass

class GPTAnalysisError(RAGPipelineError):
    """GPT ë¶„ì„ ì‹¤íŒ¨ ê´€ë ¨ ì˜¤ë¥˜ - API í˜¸ì¶œ ë¬¸ì œ ì‹œ ë°œìƒ"""
    pass

class ModelSelectionError(RAGPipelineError):
    """ëª¨ë¸ ì„ íƒ ê´€ë ¨ ì˜¤ë¥˜ - ì ì ˆí•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒ"""
    pass

# ===== ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € =====
@contextmanager
def error_context(operation_name: str, fallback_value=None):
    """ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    
    ì´ëŠ” ë§ˆì¹˜ ì‘ì—…ì¥ì˜ ì•ˆì „ë§ê³¼ ê°™ì•„ì„œ, ì‘ì—… ì¤‘ ë¬¸ì œê°€ ë°œìƒí•´ë„
    ì „ì²´ ì‹œìŠ¤í…œì´ ë©ˆì¶”ì§€ ì•Šê³  ì ì ˆíˆ ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """
    try:
        logger.debug(f"Starting operation: {operation_name}")
        yield
    except Exception as e:
        logger.error(f"Error in {operation_name}: {str(e)}")
        logger.debug(f"Full traceback:\n{traceback.format_exc()}")
        
        # Streamlit UIì— ì—ëŸ¬ í‘œì‹œ
        if 'st' in globals():
            st.error(f"âš ï¸ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {operation_name}")
            with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                st.code(traceback.format_exc())
        
        # í´ë°± ê°’ì´ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ì˜ˆì™¸ ì¬ë°œìƒ
        if fallback_value is not None:
            return fallback_value
        raise

# ===== í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ =====
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS ìŠ¤íƒ€ì¼ - UIì˜ ì‹œê°ì  ë””ìì¸ì„ ì •ì˜
st.markdown("""
<style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display: none;}
    
    /* ë©”ì¸ í—¤ë” ìŠ¤íƒ€ì¼ */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 900px;
        margin: 0 auto;
    }
    
    /* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ ê°œì„  */
    [data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* ë³µì¡ë„ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .complexity-indicator {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 16px;
        font-size: 0.85rem;
        font-weight: 500;
        margin-left: 8px;
    }
    
    .complexity-simple {
        background-color: #d4edda;
        color: #155724;
    }
    
    .complexity-medium {
        background-color: #fff3cd;
        color: #856404;
    }
    
    .complexity-complex {
        background-color: #f8d7da;
        color: #721c24;
    }
    
    /* ë¹„ìš© íš¨ìœ¨ì„± í‘œì‹œ */
    .cost-efficiency {
        display: inline-block;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.8rem;
        margin-left: 4px;
    }
    
    .cost-saved {
        background-color: #d4edda;
        color: #155724;
    }
    
    .cost-normal {
        background-color: #e2e3e5;
        color: #383d41;
    }
    
    .cost-high {
        background-color: #f8d7da;
        color: #721c24;
    }
    
    /* ê²½ê³  ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .version-warning {
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        color: #856404;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# ===== íƒ€ì… ì •ì˜ =====
class ChunkDict(TypedDict):
    """ì²­í¬ ë°ì´í„°ì˜ íƒ€ì… ì •ì˜
    
    ì²­í¬ëŠ” ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆˆ ê²ƒì…ë‹ˆë‹¤.
    ê° ì²­í¬ëŠ” ê²€ìƒ‰ê³¼ ì°¸ì¡°ê°€ ê°€ëŠ¥í•œ ë…ë¦½ì ì¸ ì •ë³´ ë‹¨ìœ„ì…ë‹ˆë‹¤.
    """
    chunk_id: str
    content: str
    source: str
    page: int
    chunk_type: str
    metadata: str

class AnalysisResult(TypedDict):
    """GPT ë¶„ì„ ê²°ê³¼ì˜ íƒ€ì… ì •ì˜"""
    query_analysis: dict
    legal_concepts: list
    search_strategy: dict
    answer_requirements: dict

# ===== ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜ ë„ì„œê´€ì—ì„œ ì°¾ì€ ì±…ì˜ ì •ë³´ ì¹´ë“œì™€ ê°™ìŠµë‹ˆë‹¤.
    ì–´ë–¤ ë‚´ìš©ì´ ì–´ë””ì— ìˆëŠ”ì§€, ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    """
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')
    
    @property
    def is_latest(self) -> bool:
        """ìµœì‹  ìë£Œ ì—¬ë¶€ í™•ì¸"""
        return self.metadata.get('is_latest', False)

class QueryComplexity(Enum):
    """ì§ˆë¬¸ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"

# ===== LRU ìºì‹œ êµ¬í˜„ =====
class LRUCache:
    """ì‹œê°„ ê¸°ë°˜ ë§Œë£Œë¥¼ ì§€ì›í•˜ëŠ” LRU ìºì‹œ êµ¬í˜„
    
    LRU(Least Recently Used) ìºì‹œëŠ” ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©í•˜ì§€ ì•Šì€ í•­ëª©ì„
    ìë™ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë˜‘ë˜‘í•œ ë³´ê´€í•¨ì…ë‹ˆë‹¤. ìì£¼ ì‚¬ìš©í•˜ëŠ” ì •ë³´ëŠ”
    ë¹ ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë©”ëª¨ë¦¬ì— ë³´ê´€í•©ë‹ˆë‹¤.
    """
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl  # Time To Live (ì´ˆ ë‹¨ìœ„)
        
    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ë§Œë£Œëœ í•­ëª©ì€ ì œê±°"""
        if key not in self.cache:
            return None
            
        value, timestamp = self.cache[key]
        # ë§Œë£Œ ì‹œê°„ í™•ì¸
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
            
        # ìµœê·¼ ì‚¬ìš©ëœ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™ (LRU êµ¬í˜„)
        self.cache.move_to_end(key)
        return value
        
    def put(self, key: str, value):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if key in self.cache:
            del self.cache[key]
            
        # ìºì‹œê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(self.cache) >= self.max_size:
            self.cache.popitem(last=False)
            
        self.cache[key] = (value, time.time())
        
    def clear_expired(self):
        """ë§Œë£Œëœ ëª¨ë“  í•­ëª© ì œê±° - ì£¼ê¸°ì ì¸ ì •ë¦¬ë¥¼ ìœ„í•´"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]

# ===== ë¹„ë™ê¸° ì‹¤í–‰ í—¬í¼ =====
def run_async_in_streamlit(coro):
    """Streamlit í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
    
    Streamlitì€ ê¸°ë³¸ì ìœ¼ë¡œ ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ë¯€ë¡œ,
    ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ê°€ ê·¸ ë‹¤ë¦¬ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    try:
        loop = asyncio.get_running_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, coro)
            return future.result()
    except RuntimeError:
        return asyncio.run(coro)

# ===== ë¬¸ì„œ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ =====
class DocumentVersionManager:
    """ë¬¸ì„œì˜ ë²„ì „ê³¼ ìµœì‹ ì„±ì„ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ë²•ë¥ ì€ ì‹œê°„ì— ë”°ë¼ ë³€ê²½ë˜ë¯€ë¡œ, ì–´ë–¤ ì •ë³´ê°€ ìµœì‹ ì¸ì§€
    íŒŒì•…í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜
    ë„ì„œê´€ì˜ ê°œì •íŒ ê´€ë¦¬ ì‹œìŠ¤í…œê³¼ ê°™ì€ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ì£¼ìš” ê·œì • ë³€ê²½ ì´ë ¥
        self.regulation_changes = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜_ê¸ˆì•¡ê¸°ì¤€': [
                {
                    'date': '2023-01-01', 
                    'old_value': '50ì–µì›', 
                    'new_value': '100ì–µì›',
                    'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 100ì–µì› ì´ìƒ'
                },
                {
                    'date': '2020-01-01', 
                    'old_value': '30ì–µì›', 
                    'new_value': '50ì–µì›',
                    'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 50ì–µì› ì´ìƒ'
                }
            ],
            'ê³µì‹œ_ê¸°í•œ': [
                {
                    'date': '2022-07-01', 
                    'old_value': '7ì¼', 
                    'new_value': '5ì¼',
                    'description': 'ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œ ë‹¨ì¶•'
                }
            ]
        }
        
        # ì¤‘ìš” ì •ë³´ íŒ¨í„´ ì •ì˜
        self.critical_patterns = {
            'ê¸ˆì•¡': r'(\d+)ì–µ\s*ì›',
            'ë¹„ìœ¨': r'(\d+(?:\.\d+)?)\s*%',
            'ê¸°í•œ': r'(\d+)\s*ì¼',
            'ë‚ ì§œ': r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        }
    
    def extract_document_date(self, chunk: Dict) -> Optional[str]:
        """ë¬¸ì„œì—ì„œ ì‘ì„±/ê°œì • ë‚ ì§œ ì¶”ì¶œ
        
        ë¬¸ì„œì˜ ë‚ ì§œëŠ” ê·¸ ë¬¸ì„œì˜ ìœ íš¨ì„±ì„ íŒë‹¨í•˜ëŠ” ì¤‘ìš”í•œ ê¸°ì¤€ì…ë‹ˆë‹¤.
        ì—¬ëŸ¬ íŒ¨í„´ì„ ì‚¬ìš©í•´ ë‚ ì§œë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        """
        content = chunk.get('content', '')
        metadata = json.loads(chunk.get('metadata', '{}'))
        
        # ë©”íƒ€ë°ì´í„°ì— ë‚ ì§œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if 'document_date' in metadata:
            return metadata['document_date']
        
        # ë‚´ìš©ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*ê°œì •',
            r'ì‹œí–‰ì¼\s*:\s*(\d{4})ë…„\s*(\d{1,2})ì›”',
            r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                return self._normalize_date(match.group(0))
        
        return None
    
    def _normalize_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹(YYYY-MM-DD)ìœ¼ë¡œ ë³€í™˜"""
        # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ìë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½
        date_str = re.sub(r'[^\d]', '-', date_str)
        parts = date_str.split('-')
        if len(parts) >= 2:
            year = parts[0] if len(parts[0]) == 4 else '20' + parts[0]
            month = parts[1].zfill(2)
            day = parts[2].zfill(2) if len(parts) > 2 else '01'
            return f"{year}-{month}-{day}"
        return None
    
    def check_for_outdated_info(self, content: str, document_date: str = None) -> List[Dict]:
        """êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        ì´ëŠ” ë§ˆì¹˜ ì‹í’ˆì˜ ìœ í†µê¸°í•œì„ í™•ì¸í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
        ì˜¤ë˜ëœ ì •ë³´ëŠ” ì‚¬ìš©ìì—ê²Œ í•´ê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
        ì‹ ì¤‘í•˜ê²Œ í™•ì¸í•˜ê³  ê²½ê³ í•©ë‹ˆë‹¤.
        """
        warnings = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ í™•ì¸
        amount_match = re.search(r'(\d+)ì–µ\s*ì›.*ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', content)
        if amount_match:
            amount = int(amount_match.group(1))
            if amount == 50:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '50ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
            elif amount == 30:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '30ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
        
        # ê³µì‹œ ê¸°í•œ í™•ì¸
        deadline_match = re.search(r'ì´ì‚¬íšŒ.*ì˜ê²°.*(\d+)ì¼', content)
        if deadline_match:
            days = int(deadline_match.group(1))
            if days == 7:
                warnings.append({
                    'type': 'outdated_deadline',
                    'found': '7ì¼',
                    'current': '5ì¼',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œ',
                    'changed_date': '2022-07-01',
                    'severity': 'high'
                })
        
        return warnings

# ===== ì¶©ëŒ í•´ê²° ì‹œìŠ¤í…œ =====
class ConflictResolver:
    """ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì œê³µí•  ë•Œ,
    ì–´ë–¤ ê²ƒì´ ë§ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.
    ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜ ì¬íŒê´€ê³¼ ê°™ì€ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, version_manager: DocumentVersionManager):
        self.version_manager = version_manager
    
    def resolve_conflicts(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ
        
        ì´ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤:
        1. ê° ê²°ê³¼ì˜ ë‚ ì§œì™€ ë‚´ìš©ì„ í™•ì¸
        2. êµ¬ë²„ì „ ì •ë³´ê°€ ìˆëŠ”ì§€ ê²€ì‚¬
        3. ì¶©ëŒí•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì•„ë‚´ê¸°
        4. ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì¬ì •ë ¬
        """
        
        # ê° ê²°ê³¼ì— ëŒ€í•´ êµ¬ë²„ì „ ì •ë³´ í™•ì¸
        for result in results:
            doc_date = result.document_date
            warnings = self.version_manager.check_for_outdated_info(result.content, doc_date)
            
            if warnings:
                result.metadata['warnings'] = warnings
                result.metadata['has_outdated_info'] = True
            else:
                result.metadata['has_outdated_info'] = False
        
        # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ ë° ì¶©ëŒ í™•ì¸
        critical_info = self._extract_critical_info(results, query)
        if critical_info:
            conflicts = self._find_conflicts(critical_info)
            if conflicts:
                results = self._prioritize_latest_info(results, conflicts)
        
        # ìµœì¢… ì •ë ¬: ìµœì‹  ì •ë³´ì™€ ë†’ì€ ì ìˆ˜ë¥¼ ìš°ì„ ì‹œ
        results.sort(key=lambda r: (
            not r.metadata.get('has_outdated_info', False),  # ìµœì‹  ì •ë³´ ìš°ì„ 
            r.document_date or '1900-01-01',  # ìµœì‹  ë‚ ì§œ ìš°ì„ 
            r.score  # ê´€ë ¨ì„± ì ìˆ˜
        ), reverse=True)
        
        return results
    
    def _extract_critical_info(self, results: List[SearchResult], query: str) -> Dict:
        """ê²°ê³¼ì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
        
        ê¸ˆì•¡, ë¹„ìœ¨, ê¸°í•œ ë“± ë²•ì ìœ¼ë¡œ ì¤‘ìš”í•œ ìˆ˜ì¹˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        critical_info = defaultdict(list)
        
        for i, result in enumerate(results):
            # ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ
            amounts = re.findall(r'(\d+)ì–µ\s*ì›', result.content)
            for amount in amounts:
                critical_info['amounts'].append({
                    'value': amount + 'ì–µì›',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            # ë¹„ìœ¨ ì •ë³´ ì¶”ì¶œ
            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', result.content)
            for pct in percentages:
                critical_info['percentages'].append({
                    'value': pct + '%',
                    'result_index': i,
                    'context': result.content[:100]
                })
        
        return dict(critical_info)
    
    def _find_conflicts(self, critical_info: Dict) -> List[Dict]:
        """ì¤‘ìš” ì •ë³´ ê°„ ì¶©ëŒ ì°¾ê¸°"""
        conflicts = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ ì¶©ëŒ í™•ì¸
        if 'amounts' in critical_info:
            amount_values = set()
            for item in critical_info['amounts']:
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in item['context']:
                    amount_values.add(item['value'])
            
            # ì—¬ëŸ¬ ë‹¤ë¥¸ ê¸ˆì•¡ì´ ì–¸ê¸‰ë˜ê³ , ê·¸ ì¤‘ êµ¬ë²„ì „ì´ ìˆëŠ” ê²½ìš°
            if len(amount_values) > 1 and ('50ì–µì›' in amount_values or '30ì–µì›' in amount_values):
                conflicts.append({
                    'type': 'amount_conflict',
                    'values': list(amount_values),
                    'correct_value': '100ì–µì›'
                })
        
        return conflicts
    
    def _prioritize_latest_info(self, results: List[SearchResult], conflicts: List[Dict]) -> List[SearchResult]:
        """ì¶©ëŒì´ ìˆì„ ë•Œ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ
        
        êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ëœ ê²°ê³¼ì˜ ì ìˆ˜ë¥¼ ë‚®ì¶°ì„œ
        ìì—°ìŠ¤ëŸ½ê²Œ í•˜ìœ„ë¡œ ë°€ë ¤ë‚˜ê²Œ í•©ë‹ˆë‹¤.
        """
        for conflict in conflicts:
            if conflict['type'] == 'amount_conflict':
                for i, result in enumerate(results):
                    # êµ¬ë²„ì „ ê¸ˆì•¡ì´ í¬í•¨ëœ ê²½ìš° ì ìˆ˜ ê°ì†Œ
                    if any(old_val in result.content for old_val in ['50ì–µì›', '30ì–µì›']):
                        results[i].score *= 0.5  # ì ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ
                        results[i].metadata['score_reduced'] = True
                        results[i].metadata['reduction_reason'] = 'outdated_amount'
        
        return results

# ===== ë¹„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ =====
class BudgetManager:
    """API ì‚¬ìš© ë¹„ìš©ì„ ì¶”ì í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜ ê°€ê³„ë¶€ë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    ì–¼ë§ˆë‚˜ ì‚¬ìš©í–ˆê³ , ì–¼ë§ˆë‚˜ ë‚¨ì•˜ëŠ”ì§€ë¥¼ ì¶”ì í•˜ì—¬
    ì˜ˆì‚° ë‚´ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ìš´ì˜í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """
    def __init__(self, daily_budget: float = 50.0):
        self.daily_budget = daily_budget
        self.reset_time = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        # ëª¨ë¸ë³„ ë¹„ìš© ì •ë³´ (1M í† í°ë‹¹ ë‹¬ëŸ¬)
        self.model_costs = {
            'gpt-4o-mini': {
                'input': 0.15,
                'cached': 0.075,
                'output': 0.60
            },
            'o4-mini': {
                'input': 1.10,
                'cached': 0.275,
                'output': 4.40
            },
            'gpt-4o': {
                'input': 2.50,
                'cached': 1.25,
                'output': 10.00
            }
        }
    
    def calculate_cost(self, model: str, input_tokens: int, 
                      output_tokens: int, cached: bool = False) -> float:
        """í† í° ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ìš© ê³„ì‚°
        
        API ë¹„ìš©ì€ ì…ë ¥ê³¼ ì¶œë ¥ í† í° ìˆ˜ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤.
        ìºì‹œëœ ì…ë ¥ì€ í• ì¸ëœ ê°€ê²©ì´ ì ìš©ë©ë‹ˆë‹¤.
        """
        costs = self.model_costs[model]
        input_cost = costs['cached' if cached else 'input'] * (input_tokens / 1_000_000)
        output_cost = costs['output'] * (output_tokens / 1_000_000)
        return input_cost + output_cost
    
    def get_current_status(self) -> Dict:
        """í˜„ì¬ ì˜ˆì‚° ìƒí™© ë°˜í™˜"""
        # ì„¸ì…˜ ìƒíƒœì—ì„œ ì˜¤ëŠ˜ì˜ ì‚¬ìš©ëŸ‰ ê°€ì ¸ì˜¤ê¸°
        if 'daily_cost' not in st.session_state:
            st.session_state.daily_cost = 0.0
            
        # ë‚ ì§œê°€ ë°”ë€Œì—ˆìœ¼ë©´ ë¦¬ì…‹
        current_date = datetime.now().date()
        if 'last_reset_date' not in st.session_state:
            st.session_state.last_reset_date = current_date
        elif st.session_state.last_reset_date != current_date:
            st.session_state.daily_cost = 0.0
            st.session_state.last_reset_date = current_date
            logger.info("Daily budget reset for new day")
            
        used = st.session_state.daily_cost
        remaining = self.daily_budget - used
        
        return {
            'used': used,
            'remaining': remaining,
            'remaining_ratio': remaining / self.daily_budget if self.daily_budget > 0 else 0,
            'is_budget_critical': remaining < self.daily_budget * 0.2,
            'budget_exhausted': remaining <= 0
        }
    
    def add_usage(self, cost: float):
        """ì‚¬ìš© ë¹„ìš© ì¶”ê°€"""
        st.session_state.daily_cost = st.session_state.get('daily_cost', 0.0) + cost
        logger.info(f"Added ${cost:.4f} to daily cost. Total: ${st.session_state.daily_cost:.4f}")

# ===== ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ =====
class OptimizedModelSelector:
    """ì„¸ ê°€ì§€ ëª¨ë¸ë§Œì„ ì‚¬ìš©í•˜ëŠ” ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ
    
    ì´ ì‹œìŠ¤í…œì€ ë§ˆì¹˜ í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €ê°€ íŒ€ì›ì„ ë°°ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    ê° ì‘ì—…ì˜ íŠ¹ì„±ì„ íŒŒì•…í•˜ê³ , ê°€ì¥ ì í•©í•œ íŒ€ì›(ëª¨ë¸)ì„ ì„ íƒí•©ë‹ˆë‹¤.
    ì¤‘ìš”í•œ ì ì€ o4-miniê°€ gpt-4oë³´ë‹¤ ì €ë ´í•˜ë©´ì„œë„ ì¶”ë¡  ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.model_profiles = {
            'gpt-4o-mini': {
                'cost_per_1k': 0.00015,
                'strengths': ['speed', 'simple_queries', 'fact_checking'],
                'max_tokens': 2000,
                'decision_threshold': 0.3,
                'performance_score': 0.6,
                'description': 'ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸ì— ìµœì í™”ëœ ê²½ì œì  ëª¨ë¸'
            },
            'o4-mini': {
                'cost_per_1k': 0.0011,
                'strengths': ['reasoning', 'analysis', 'multi_step', 'complex_logic'],
                'max_tokens': 4000,
                'decision_threshold': 0.85,
                'performance_score': 0.85,  # gpt-4oë³´ë‹¤ ë†’ì€ ì¶”ë¡  ëŠ¥ë ¥
                'description': 'ë›°ì–´ë‚œ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°€ì§„ ì£¼ë ¥ ëª¨ë¸'
            },
            'gpt-4o': {
                'cost_per_1k': 0.0025,
                'strengths': ['long_context', 'creative', 'special_formats'],
                'max_tokens': 8000,
                'decision_threshold': 0.95,
                'performance_score': 0.75,  # o4-minië³´ë‹¤ ë‚®ì€ ì¶”ë¡  ëŠ¥ë ¥
                'description': 'íŠ¹ìˆ˜í•œ ê²½ìš°ë¥¼ ìœ„í•œ ê³ ê¸‰ ëª¨ë¸'
            }
        }
        
        self.budget_manager = BudgetManager()
        
    def select_model(self, query: str, initial_analysis: Dict, 
                    complexity_assessment: Dict) -> Tuple[str, Dict]:
        """ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒ
        
        ì„ íƒ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        1. ì§ˆë¬¸ì˜ ë³µì¡ë„ì™€ íŠ¹ì„± íŒŒì•…
        2. í˜„ì¬ ì˜ˆì‚° ìƒí™© í™•ì¸
        3. ê° ëª¨ë¸ì˜ ê°•ì ê³¼ ë¹„ìš©ì„ ê³ ë ¤
        4. ìµœì ì˜ ëª¨ë¸ ì„ íƒ
        """
        
        # ë³µì¡ë„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        complexity_score = complexity_assessment.get('score', 0.5)
        complexity_level = complexity_assessment.get('level', QueryComplexity.MEDIUM)
        
        # í˜„ì¬ ì˜ˆì‚° ìƒí™© í™•ì¸
        budget_status = self.budget_manager.get_current_status()
        
        # ê¸°ë³¸ ëª¨ë¸ ì„ íƒ ë¡œì§
        selected_model = self._select_base_model(
            query, complexity_score, initial_analysis
        )
        
        # ì˜ˆì‚° ì œì•½ì— ë”°ë¥¸ ì¡°ì •
        if budget_status['is_budget_critical']:
            selected_model = self._adjust_for_budget(
                selected_model, budget_status
            )
        
        # ì„ íƒ ì´ìœ  ìƒì„±
        reason = self._generate_selection_reason(
            selected_model, complexity_score, budget_status
        )
        
        selection_info = {
            'model': selected_model,
            'reason': reason,
            'complexity_score': complexity_score,
            'complexity_level': complexity_level.value,
            'budget_remaining': budget_status['remaining_ratio'],
            'estimated_cost': self._estimate_query_cost(selected_model),
            'performance_score': self.model_profiles[selected_model]['performance_score']
        }
        
        return selected_model, selection_info
    
    def _select_base_model(self, query: str, complexity_score: float, 
                          analysis: Dict) -> str:
        """ê¸°ë³¸ ëª¨ë¸ ì„ íƒ ë¡œì§
        
        ë³µì¡ë„ì™€ ì§ˆë¬¸ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
        o4-minië¥¼ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
        """
        
        # ë‹¨ìˆœí•œ ì§ˆë¬¸: gpt-4o-mini
        if complexity_score < 0.3:
            # ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸, ì •ì˜, ë‚ ì§œ/ê¸ˆì•¡ í™•ì¸ ë“±
            return 'gpt-4o-mini'
        
        # í‘œì¤€~ë³µì¡í•œ ì§ˆë¬¸: o4-mini (ì£¼ë ¥ ëª¨ë¸)
        elif complexity_score < 0.85:
            # ì¶”ë¡ , ë¶„ì„, ë‹¨ê³„ë³„ ì„¤ëª…ì´ í•„ìš”í•œ ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸
            return 'o4-mini'
        
        # ë§¤ìš° ë³µì¡í•œ ì§ˆë¬¸
        else:
            # íŠ¹ë³„í•œ ê²½ìš° í™•ì¸
            if self._requires_special_handling(query):
                return 'gpt-4o'
            else:
                # ëŒ€ë¶€ë¶„ì˜ ë³µì¡í•œ ì§ˆë¬¸ë„ o4-miniê°€ ë” íš¨ê³¼ì 
                return 'o4-mini'
    
    def _requires_special_handling(self, query: str) -> bool:
        """gpt-4oê°€ ê¼­ í•„ìš”í•œ íŠ¹ìˆ˜í•œ ê²½ìš°ì¸ì§€ í™•ì¸
        
        o4-miniê°€ ì¶”ë¡ ì€ ë” ì˜í•˜ì§€ë§Œ, gpt-4oê°€ í•„ìš”í•œ ê²½ìš°:
        - ë§¤ìš° ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ (1000ì ì´ìƒ)
        - ì°½ì˜ì  ì½˜í…ì¸  ìƒì„±
        - íŠ¹ìˆ˜í•œ í˜•ì‹ì˜ ì¶œë ¥
        """
        # ê¸´ ë¬¸ë§¥
        if len(query) > 1000:
            return True
        
        # ì°½ì˜ì  ì‘ì—…
        creative_keywords = ['ì‹œë‚˜ë¦¬ì˜¤', 'ìŠ¤í† ë¦¬', 'ì°½ì˜ì ', 'ì•„ì´ë””ì–´', 'ì œì•ˆì„œ']
        if any(keyword in query for keyword in creative_keywords):
            return True
        
        # íŠ¹ìˆ˜ í˜•ì‹
        format_keywords = ['í‘œ', 'ë‹¤ì´ì–´ê·¸ë¨', 'ì°¨íŠ¸', 'ê·¸ë˜í”„']
        if any(keyword in query for keyword in format_keywords):
            return True
        
        return False
    
    def _adjust_for_budget(self, selected_model: str, 
                          budget_status: Dict) -> str:
        """ì˜ˆì‚° ì œì•½ì— ë”°ë¥¸ ëª¨ë¸ ì¡°ì •"""
        if budget_status['budget_exhausted']:
            return 'gpt-4o-mini'  # ì˜ˆì‚° ì†Œì§„ ì‹œ ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ë§Œ ì‚¬ìš©
        
        if selected_model == 'gpt-4o' and budget_status['remaining_ratio'] < 0.3:
            return 'o4-mini'  # gpt-4o ëŒ€ì‹  o4-mini ì‚¬ìš©
        elif selected_model == 'o4-mini' and budget_status['remaining_ratio'] < 0.1:
            return 'gpt-4o-mini'  # ê·¹ë„ë¡œ ì˜ˆì‚°ì´ ë¶€ì¡±í•  ë•Œ
        
        return selected_model
    
    def _generate_selection_reason(self, model: str, complexity_score: float,
                                  budget_status: Dict) -> str:
        """ëª¨ë¸ ì„ íƒ ì´ìœ ë¥¼ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…"""
        base_reasons = {
            'gpt-4o-mini': f"ê°„ë‹¨í•œ ì§ˆë¬¸ (ë³µì¡ë„: {complexity_score:.2f})",
            'o4-mini': f"ì¶”ë¡ ê³¼ ë¶„ì„ì´ í•„ìš”í•œ ì§ˆë¬¸ (ë³µì¡ë„: {complexity_score:.2f})",
            'gpt-4o': f"íŠ¹ìˆ˜í•œ ì²˜ë¦¬ê°€ í•„ìš”í•œ ì§ˆë¬¸ (ë³µì¡ë„: {complexity_score:.2f})"
        }
        
        reason = base_reasons.get(model, "ì•Œ ìˆ˜ ì—†ìŒ")
        
        # ì˜ˆì‚° ì¡°ì •ì´ ìˆì—ˆë‹¤ë©´ ì¶”ê°€
        if budget_status['is_budget_critical']:
            reason += " (ì˜ˆì‚° ê³ ë ¤)"
        
        return reason
    
    def _estimate_query_cost(self, model: str) -> float:
        """ì§ˆë¬¸ ì²˜ë¦¬ ì˜ˆìƒ ë¹„ìš©"""
        avg_tokens = 3000  # í‰ê·  í† í° ìˆ˜
        return self.model_profiles[model]['cost_per_1k'] * (avg_tokens / 1000) * 2

# ===== ìºì‹± ì „ëµ ì‹œìŠ¤í…œ =====
class SmartCacheStrategy:
    """ëª¨ë¸ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ìºì‹± ì „ëµ
    
    ìºì‹±ì€ ìì£¼ ì°¾ëŠ” ì±…ì„ ì±…ìƒ ìœ„ì— ì˜¬ë ¤ë†“ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    í•œ ë²ˆ ì°¾ì€ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´
    ì‹œê°„ê³¼ ë¹„ìš©ì„ í¬ê²Œ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.cache_configs = {
            'gpt-4o-mini': {
                'ttl': 7200,  # 2ì‹œê°„ - ì €ë ´í•˜ë¯€ë¡œ ì˜¤ë˜ ë³´ê´€
                'similarity_threshold': 0.85,
                'cache_benefit_ratio': 0.5  # 50% ë¹„ìš© ì ˆê°
            },
            'o4-mini': {
                'ttl': 3600,  # 1ì‹œê°„
                'similarity_threshold': 0.9,
                'cache_benefit_ratio': 0.75  # 75% ë¹„ìš© ì ˆê°
            },
            'gpt-4o': {
                'ttl': 2400,  # 40ë¶„
                'similarity_threshold': 0.92,
                'cache_benefit_ratio': 0.5
            }
        }
        
        # ì§ˆë¬¸-ë‹µë³€ ìºì‹œ
        self.query_cache = LRUCache(max_size=200, ttl=7200)
        # ì„ë² ë”© ìºì‹œ
        self.embedding_cache = LRUCache(max_size=500, ttl=10800)
        # ë¶„ì„ ê²°ê³¼ ìºì‹œ
        self.analysis_cache = LRUCache(max_size=100, ttl=3600)
    
    def get_cached_result(self, query: str) -> Optional[Dict]:
        """ìºì‹œëœ ê²°ê³¼ ë°˜í™˜"""
        cache_key = self._generate_cache_key(query)
        return self.query_cache.get(cache_key)
    
    def store_result(self, query: str, result: Dict, model: str):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_key = self._generate_cache_key(query)
        result['cached_at'] = time.time()
        result['model'] = model
        self.query_cache.put(cache_key, result)
    
    def _generate_cache_key(self, query: str) -> str:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ê³ ìœ í•œ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(query.encode()).hexdigest()
    
    def should_use_cache(self, similarity: float, model: str, 
                        cache_age: float) -> bool:
        """ìºì‹œ ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        config = self.cache_configs[model]
        
        # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë†’ê³ 
        if similarity < config['similarity_threshold']:
            return False
        
        # ìºì‹œê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´
        if cache_age > config['ttl']:
            return False
        
        return True

# ===== GPT-4o ì§ˆë¬¸ ë¶„ì„ê¸° =====
class GPT4oQueryAnalyzer:
    """GPT-4o-minië¥¼ í™œìš©í•œ í†µí•© ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
    
    ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜ ê²½í—˜ ë§ì€ ì‚¬ì„œê°€ ë„ì„œê´€ì—ì„œ
    ì±…ì„ ì°¾ëŠ” ê²ƒì„ ë„ì™€ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€,
    ì–´ë””ì„œ ì°¾ì•„ì•¼ í•˜ëŠ”ì§€ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, cache_strategy: SmartCacheStrategy):
        self.cache_strategy = cache_strategy
    
    def analyze_and_strategize(self, query: str, available_chunks_info: Dict) -> Dict:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½"""
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(f"analysis_{query}".encode()).hexdigest()
        cached = self.cache_strategy.analysis_cache.get(cache_key)
        if cached:
            logger.info(f"Analysis cache hit for query: {query[:50]}...")
            return cached
        
        prompt = f"""
        ë‹¹ì‹ ì€ ê³µì •ê±°ë˜ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ì •ë³´:
        - ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼: {available_chunks_info.get('ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 0)}ê°œ ì²­í¬
        - í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼: {available_chunks_info.get('í˜„í™©ê³µì‹œ', 0)}ê°œ ì²­í¬
        - ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼: {available_chunks_info.get('ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­', 0)}ê°œ ì²­í¬
        
        ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "query_analysis": {{
                "core_intent": "ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ (í•œ ë¬¸ì¥)",
                "actual_complexity": "simple/medium/complex",
                "complexity_reason": "ë³µì¡ë„ íŒë‹¨ ì´ìœ "
            }},
            "legal_concepts": [
                {{
                    "concept": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜/í˜„í™©ê³µì‹œ/ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­",
                    "relevance": "primary/secondary",
                    "specific_aspects": ["ê¸ˆì•¡ê¸°ì¤€", "ì ˆì°¨", "ê³µì‹œì˜ë¬´"]
                }}
            ],
            "search_strategy": {{
                "approach": "direct_lookup/focused_search/comprehensive_analysis",
                "primary_manual": "ì£¼ë¡œ ê²€ìƒ‰í•  ë§¤ë‰´ì–¼",
                "search_keywords": ["í•µì‹¬ ê²€ìƒ‰ì–´1", "í•µì‹¬ ê²€ìƒ‰ì–´2"],
                "expected_chunks_needed": 10,
                "rationale": "ì´ ì „ëµì„ ì„ íƒí•œ ì´ìœ "
            }},
            "answer_requirements": {{
                "needs_specific_numbers": true,
                "needs_process_steps": false,
                "needs_timeline": false,
                "needs_exceptions": false,
                "needs_multiple_perspectives": false
            }}
        }}
        """
        
        try:
            response = openai.chat.completions.create(
                model="gpt-4o-mini",  # ë¶„ì„ì—ëŠ” ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            analysis = json.loads(response.choices[0].message.content)
            
            # ìºì‹œ ì €ì¥
            self.cache_strategy.analysis_cache.put(cache_key, analysis)
            
            return analysis
            
        except Exception as e:
            logger.error(f"GPT-4o analysis error: {str(e)}")
            return self._get_fallback_strategy(query)
    
    def _get_fallback_strategy(self, query: str) -> Dict:
        """GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì „ëµ"""
        return {
            "query_analysis": {
                "core_intent": "ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨ - ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰",
                "actual_complexity": "medium",
                "complexity_reason": "ìë™ ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¤‘ê°„ ë³µì¡ë„ ê°€ì •"
            },
            "legal_concepts": [],
            "search_strategy": {
                "approach": "focused_search",
                "primary_manual": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜",
                "search_keywords": query.split()[:5],
                "expected_chunks_needed": 30,
                "rationale": "ê¸°ë³¸ ê²€ìƒ‰ ì „ëµ"
            },
            "answer_requirements": {
                "needs_specific_numbers": True,
                "needs_process_steps": True,
                "needs_timeline": True,
                "needs_exceptions": False,
                "needs_multiple_perspectives": False
            }
        }

# ===== ì§ˆë¬¸ ë¶„ë¥˜ê¸° =====
class QuestionClassifier:
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë§¤ë‰´ì–¼ì„ ìš°ì„  ê²€ìƒ‰í• ì§€ ê²°ì •
    
    ì´ëŠ” ë„ì„œê´€ì—ì„œ ì–´ëŠ ì„¹ì…˜ìœ¼ë¡œ ê°€ì•¼ í• ì§€
    ì•ˆë‚´í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì˜ í‚¤ì›Œë“œì™€ íŒ¨í„´ì„
    ë¶„ì„í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë§¤ë‰´ì–¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.categories = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': {
                'keywords': ['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 'ë‚´ë¶€ê±°ë˜', 'ì´ì‚¬íšŒ ì˜ê²°', 'ì´ì‚¬íšŒ', 'ì˜ê²°', 
                           'ê³„ì—´ì‚¬', 'ê³„ì—´íšŒì‚¬', 'íŠ¹ìˆ˜ê´€ê³„ì¸', 'ìê¸ˆ', 'ëŒ€ì—¬', 'ì°¨ì…', 'ë³´ì¦',
                           'ìê¸ˆê±°ë˜', 'ìœ ê°€ì¦ê¶Œ', 'ìì‚°ê±°ë˜', '50ì–µ', 'ê±°ë˜ê¸ˆì•¡', '100ì–µ'],
                'patterns': [r'ì´ì‚¬íšŒ.*ì˜ê²°', r'ê³„ì—´.*ê±°ë˜', r'ë‚´ë¶€.*ê±°ë˜'],
                'manual_pattern': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜.*ë§¤ë‰´ì–¼',
                'priority': 1
            },
            'í˜„í™©ê³µì‹œ': {
                'keywords': ['í˜„í™©ê³µì‹œ', 'ê¸°ì—…ì§‘ë‹¨', 'ì†Œì†íšŒì‚¬', 'ë™ì¼ì¸', 'ì¹œì¡±', 
                           'ì§€ë¶„ìœ¨', 'ì„ì›', 'ìˆœí™˜ì¶œì', 'ìƒí˜¸ì¶œì', 'ì§€ë°°êµ¬ì¡°',
                           'ê³„ì—´í¸ì…', 'ê³„ì—´ì œì™¸', 'ì£¼ì£¼í˜„í™©', 'ì„ì›í˜„í™©'],
                'patterns': [r'ê¸°ì—…ì§‘ë‹¨.*í˜„í™©', r'ì†Œì†.*íšŒì‚¬', r'ì§€ë¶„.*ë³€ë™'],
                'manual_pattern': 'ê¸°ì—…ì§‘ë‹¨í˜„í™©ê³µì‹œ.*ë§¤ë‰´ì–¼',
                'priority': 2
            },
            'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': {
                'keywords': ['ë¹„ìƒì¥', 'ì¤‘ìš”ì‚¬í•­', 'ì£¼ì‹', 'ì–‘ë„', 'ì–‘ìˆ˜', 'í•©ë³‘', 
                           'ë¶„í• ', 'ì˜ì—…ì–‘ë„', 'ì„ì›ë³€ê²½', 'ì¦ì', 'ê°ì',
                           'ì •ê´€ë³€ê²½', 'í•´ì‚°', 'ì²­ì‚°'],
                'patterns': [r'ë¹„ìƒì¥.*ê³µì‹œ', r'ì£¼ì‹.*ì–‘ë„', r'ì¤‘ìš”.*ì‚¬í•­'],
                'manual_pattern': 'ë¹„ìƒì¥ì‚¬.*ì¤‘ìš”ì‚¬í•­.*ë§¤ë‰´ì–¼',
                'priority': 3
            }
        }
    
    def classify(self, question: str) -> Tuple[str, float]:
        """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì‹ ë¢°ë„ë¥¼ ë°˜í™˜
        
        ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ ,
        ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        ì‹ ë¢°ë„ëŠ” ì–¼ë§ˆë‚˜ í™•ì‹¤í•œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        """
        question_lower = question.lower()
        scores = {}
        
        for category, info in self.categories.items():
            score = 0
            matched_keywords = []
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ì¤‘ìš”ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬)
            for i, keyword in enumerate(info['keywords']):
                if keyword in question_lower:
                    weight = 1.0 if i < 5 else 0.7  # ì•ìª½ í‚¤ì›Œë“œê°€ ë” ì¤‘ìš”
                    score += weight
                    matched_keywords.append(keyword)
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in info.get('patterns', []):
                if re.search(pattern, question_lower):
                    score += 1.5
            
            scores[category] = score
        
        if scores:
            best_category = max(scores, key=scores.get)
            max_possible_score = len(self.categories[best_category]['keywords']) + \
                               len(self.categories[best_category].get('patterns', [])) * 1.5
            confidence = min(scores[best_category] / max_possible_score, 1.0)
            
            # ë„ˆë¬´ ë‚®ì€ ì‹ ë¢°ë„ëŠ” ë¶„ë¥˜í•˜ì§€ ì•ŠìŒ
            if confidence < 0.15:
                return None, 0.0
                
            return best_category, confidence
        
        return None, 0.0

# ===== ë³µì¡ë„ í‰ê°€ê¸° =====
class ComplexityAssessor:
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •
    
    ì´ëŠ” ìš”ë¦¬ì˜ ë‚œì´ë„ë¥¼ í‰ê°€í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    ì¬ë£Œì˜ ìˆ˜, ì¡°ë¦¬ ë‹¨ê³„, í•„ìš”í•œ ê¸°ìˆ  ë“±ì„
    ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ë‚œì´ë„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ë³µì¡ë„ ì§€í‘œë“¤
        self.simple_indicators = [
            r'ì–¸ì œ', r'ë©°ì¹ ', r'ê¸°í•œ', r'ë‚ ì§œ', r'ê¸ˆì•¡', r'%', r'ì–¼ë§ˆ',
            r'ì •ì˜[ê°€ëŠ”]?', r'ë¬´ì—‡', r'ëœ»[ì´ì€]?', r'ì˜ë¯¸[ê°€ëŠ”]?'
        ]
        
        self.complex_indicators = [
            r'ë™ì‹œì—', r'ì—¬ëŸ¬', r'ë³µí•©', r'ì—°ê´€', r'ì˜í–¥',
            r'ë§Œ[ì•½ì¼].*ê²½ìš°', r'[AB].*ë™ì‹œ.*[CD]', r'ê±°ë˜.*ì—¬ëŸ¬',
            r'ì „ì²´ì ', r'ì¢…í•©ì ', r'ë¶„ì„', r'ê²€í† ', r'í‰ê°€',
            r'ë¦¬ìŠ¤í¬', r'ìœ„í—˜', r'ëŒ€ì‘', r'ì „ëµ'
        ]
        
        self.medium_indicators = [
            r'ì–´ë–»ê²Œ', r'ë°©ë²•', r'ì ˆì°¨', r'ê³¼ì •',
            r'ì£¼ì˜', r'ì˜ˆì™¸', r'íŠ¹ë³„', r'ê³ ë ¤'
        ]
    
    def assess(self, query: str) -> Tuple[QueryComplexity, float, Dict]:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ê³  ê´€ë ¨ ì •ë³´ ë°˜í™˜"""
        query_lower = query.lower()
        
        # ê° ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°
        simple_score = sum(1 for pattern in self.simple_indicators 
                         if re.search(pattern, query_lower))
        complex_score = sum(2 for pattern in self.complex_indicators 
                          if re.search(pattern, query_lower))
        medium_score = sum(1.5 for pattern in self.medium_indicators 
                         if re.search(pattern, query_lower))
        
        # ê¸¸ì´ì— ë”°ë¥¸ ì¶”ê°€ ì ìˆ˜
        if len(query) > 150:
            complex_score += 2
        elif len(query) > 100:
            complex_score += 1
        elif len(query) < 30:
            simple_score += 0.5
            
        # íŠ¹ìˆ˜ íŒ¨í„´ í™•ì¸
        if re.search(r'[AB]íšŒì‚¬.*[CD]íšŒì‚¬', query_lower):
            complex_score += 2
        if '?' in query and query.count('?') > 1:
            complex_score += 1
            
        total_score = simple_score + medium_score + complex_score
        
        # ë³µì¡ë„ ë ˆë²¨ ê²°ì •
        if total_score == 0:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.5
        elif complex_score > simple_score * 3:
            complexity = QueryComplexity.COMPLEX
            confidence = min(complex_score / (total_score + 1), 0.9)
        elif simple_score > complex_score * 2:
            complexity = QueryComplexity.SIMPLE
            confidence = min(simple_score / (total_score + 1), 0.9)
        else:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.6
            
        # ì •ê·œí™”ëœ ì ìˆ˜ (0-1)
        normalized_score = min(total_score / 10, 1.0)
        
        analysis = {
            'simple_score': simple_score,
            'medium_score': medium_score,
            'complex_score': complex_score,
            'total_score': total_score,
            'normalized_score': normalized_score,
            'query_length': len(query),
            'estimated_cost_multiplier': self._estimate_cost_multiplier(complexity)
        }
        
        return complexity, confidence, analysis
    
    def _estimate_cost_multiplier(self, complexity: QueryComplexity) -> float:
        """ë³µì¡ë„ì— ë”°ë¥¸ ì˜ˆìƒ ë¹„ìš© ë°°ìˆ˜"""
        multipliers = {
            QueryComplexity.SIMPLE: 1.0,
            QueryComplexity.MEDIUM: 3.0,
            QueryComplexity.COMPLEX: 10.0
        }
        return multipliers[complexity]

# ===== í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ =====
class OptimizedHybridRAGPipeline:
    """ë¹„ìš© ìµœì í™”ê°€ ì ìš©ëœ ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸
    
    ì´ í´ë˜ìŠ¤ëŠ” ì „ì²´ ì‹œìŠ¤í…œì˜ í•µì‹¬ì…ë‹ˆë‹¤.
    ë§ˆì¹˜ ì˜¤ì¼€ìŠ¤íŠ¸ë¼ì˜ ì§€íœ˜ìì²˜ëŸ¼, ëª¨ë“  êµ¬ì„± ìš”ì†Œë“¤ì„
    ì¡°í™”ë¡­ê²Œ ìš´ì˜í•˜ì—¬ ìµœê³ ì˜ ì„±ëŠ¥ì„ ì´ëŒì–´ëƒ…ë‹ˆë‹¤.
    """
    
    def __init__(self, embedding_model, reranker_model, index, chunks):
        # ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ ê²€ì¦
        if not chunks:
            raise ValueError("No chunks provided to HybridRAGPipeline")
        
        if index.ntotal == 0:
            raise ValueError("FAISS index is empty")
        
        # ì„ë² ë”© ì°¨ì› ê²€ì¦
        test_embedding = embedding_model.encode(["test"])
        if len(test_embedding[0]) != index.d:
            raise ValueError(f"Embedding dimension {len(test_embedding[0])} doesn't match index dimension {index.d}")
        
        # ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunks = chunks
        
        # ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œë“¤
        self.cache_strategy = SmartCacheStrategy()
        self.version_manager = DocumentVersionManager()
        self.conflict_resolver = ConflictResolver(self.version_manager)
        self.budget_manager = BudgetManager()
        
        self.classifier = QuestionClassifier()
        self.complexity_assessor = ComplexityAssessor()
        self.gpt4o_analyzer = GPT4oQueryAnalyzer(self.cache_strategy)
        self.model_selector = OptimizedModelSelector()
        
        # ë§¤ë‰´ì–¼ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.manual_indices = self._build_manual_indices()
        
        # ì²­í¬ ì •ë³´
        self.chunks_info = {
            category: len(indices) 
            for category, indices in self.manual_indices.items()
        }
        
        # ëª¨ë“  ì²­í¬ì˜ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        self._extract_chunk_dates()
        
        logger.info(f"HybridRAGPipeline initialized with {len(chunks)} chunks")
        logger.info(f"Manual distribution: {self.chunks_info}")
    
    def _extract_chunk_dates(self):
        """ëª¨ë“  ì²­í¬ì˜ ë‚ ì§œ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ
        
        ì‹œì‘í•  ë•Œ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
        """
        for chunk in self.chunks:
            doc_date = self.version_manager.extract_document_date(chunk)
            if doc_date:
                metadata = json.loads(chunk.get('metadata', '{}'))
                metadata['document_date'] = doc_date
                chunk['metadata'] = json.dumps(metadata)
    
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx, chunk in enumerate(self.chunks):
            source = chunk.get('source', '').lower()
            
            if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜'].append(idx)
            elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                indices['í˜„í™©ê³µì‹œ'].append(idx)
            elif 'ë¹„ìƒì¥' in source:
                indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­'].append(idx)
            else:
                indices['ê¸°íƒ€'].append(idx)
        
        return dict(indices)
    
    def _get_query_embedding(self, query: str) -> np.ndarray:
        """ì¿¼ë¦¬ ì„ë² ë”©ì„ ìºì‹œì™€ í•¨ê»˜ ê°€ì ¸ì˜¤ê¸°"""
        # ìºì‹œ í™•ì¸
        cached_embedding = self.cache_strategy.embedding_cache.get(query)
        if cached_embedding is not None:
            return cached_embedding
            
        # ìƒˆë¡œ ìƒì„±
        embedding = self.embedding_model.encode([query])
        embedding = np.array(embedding, dtype=np.float32)
        
        # ìºì‹œ ì €ì¥
        self.cache_strategy.embedding_cache.put(query, embedding)
        
        return embedding
    
    async def process_query(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ” ì „ì²´ ì²˜ë¦¬ ê³¼ì •ì„ ì¡°ìœ¨í•©ë‹ˆë‹¤:
        1. ìºì‹œ í™•ì¸
        2. ì§ˆë¬¸ ë¶„ì„ ë° ë³µì¡ë„ í‰ê°€
        3. ëª¨ë¸ ì„ íƒ
        4. ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
        5. ì¶©ëŒ í•´ê²° ë° ìµœì‹  ì •ë³´ ìš°ì„ ì‹œ
        """
        start_time = time.time()
        
        # 1. ìºì‹œ í™•ì¸
        cached_result = self.cache_strategy.get_cached_result(query)
        if cached_result:
            logger.info(f"Cache hit for query: {query[:50]}...")
            return cached_result['results'], cached_result['stats']
        
        # 2. ì§ˆë¬¸ ë¶„ë¥˜ ë° ë³µì¡ë„ í‰ê°€
        category, cat_confidence = self.classifier.classify(query)
        complexity, comp_confidence, complexity_analysis = self.complexity_assessor.assess(query)
        
        # 3. GPT ë¶„ì„
        analysis_start = time.time()
        try:
            gpt_analysis = self.gpt4o_analyzer.analyze_and_strategize(
                query, self.chunks_info
            )
            analysis_time = time.time() - analysis_start
        except Exception as e:
            logger.error(f"GPT-4o analysis failed: {str(e)}, falling back to rule-based")
            gpt_analysis = self._get_fallback_analysis(query, category)
            analysis_time = time.time() - analysis_start
        
        # 4. ëª¨ë¸ ì„ íƒ
        complexity_assessment = {
            'level': complexity,
            'confidence': comp_confidence,
            'score': complexity_analysis['normalized_score'],
            **complexity_analysis
        }
        
        selected_model, selection_info = self.model_selector.select_model(
            query, gpt_analysis, complexity_assessment
        )
        
        # 5. ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
        search_approach = gpt_analysis['search_strategy']['approach']
        
        if search_approach == 'direct_lookup':
            results, search_stats = await self._gpt_guided_direct_search(
                query, gpt_analysis, top_k
            )
        elif search_approach == 'focused_search':
            results, search_stats = await self._gpt_guided_focused_search(
                query, gpt_analysis, top_k
            )
        else:  # comprehensive_analysis
            results, search_stats = await self._gpt_guided_comprehensive_search(
                query, gpt_analysis, top_k
            )
        
        # 6. ì¶©ëŒ í•´ê²° ë° ìµœì‹  ì •ë³´ ìš°ì„ ì‹œ
        results = self.conflict_resolver.resolve_conflicts(results, query)
        
        # 7. êµ¬ë²„ì „ ê²½ê³  ìˆ˜ì§‘
        outdated_warnings = []
        for result in results:
            if result.metadata.get('has_outdated_info'):
                outdated_warnings.extend(result.metadata.get('warnings', []))
        
        # 8. í†µê³„ ì •ë³´ êµ¬ì„±
        stats = {
            'total_time': time.time() - start_time,
            'analysis_time': analysis_time,
            'gpt_analysis': gpt_analysis,
            'category': category,
            'category_confidence': cat_confidence,
            'complexity': complexity.value,
            'complexity_confidence': comp_confidence,
            'complexity_analysis': complexity_analysis,
            'selected_model': selected_model,
            'selection_info': selection_info,
            'search_approach': search_approach,
            'outdated_warnings': outdated_warnings,
            'has_version_conflicts': len(outdated_warnings) > 0,
            **search_stats
        }
        
        # 9. ìºì‹œ ì €ì¥ (ê°„ë‹¨í•œ ì§ˆë¬¸ë§Œ)
        if complexity == QueryComplexity.SIMPLE and not outdated_warnings:
            self.cache_strategy.store_result(query, {
                'results': results,
                'stats': stats
            }, selected_model)
        
        return results, stats
    
    async def _gpt_guided_direct_search(self, query: str, gpt_analysis: Dict, 
                                       top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§ì ‘ ê²€ìƒ‰"""
        start_time = time.time()
        
        primary_manual = gpt_analysis['search_strategy']['primary_manual']
        search_keywords = gpt_analysis['search_strategy']['search_keywords']
        
        # ëŒ€ìƒ ì¸ë±ìŠ¤ ì„ íƒ
        target_indices = self.manual_indices.get(primary_manual, [])[:100]
        
        if not target_indices:
            logger.warning(f"No indices for manual '{primary_manual}', using all chunks")
            target_indices = list(range(min(len(self.chunks), 50)))
        
        # ê²€ìƒ‰ì–´ í™•ì¥
        enhanced_query = f"{query} {' '.join(search_keywords)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        # FAISS ê²€ìƒ‰
        k_search = min(len(target_indices), max(1, top_k * 3))
        
        try:
            scores, indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error: {str(e)}")
            return [], {'search_time': time.time() - start_time, 'error': str(e)}
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        target_set = set(target_indices)
        
        for idx, score in zip(indices[0], scores[0]):
            if idx in target_set and idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
                if len(results) >= top_k:
                    break
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': len(target_indices),
            'search_method': 'direct_vector'
        }
        
        return results, stats
    
    async def _gpt_guided_focused_search(self, query: str, gpt_analysis: Dict, 
                                        top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§‘ì¤‘ ê²€ìƒ‰"""
        start_time = time.time()
        
        primary_manual = gpt_analysis['search_strategy'].get('primary_manual', '')
        search_keywords = gpt_analysis['search_strategy'].get('search_keywords', [])
        expected_chunks = gpt_analysis['search_strategy'].get('expected_chunks_needed', 10)
        
        # ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •
        search_limit = min(expected_chunks * 2, 200)
        target_indices = self.manual_indices.get(primary_manual, [])[:search_limit]
        
        if not target_indices:
            target_indices = list(range(min(len(self.chunks), 100)))
        
        # ë‹µë³€ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ í•„í„°ë§
        requirements = gpt_analysis.get('answer_requirements', {})
        if requirements.get('needs_specific_numbers'):
            filtered_indices = [
                idx for idx in target_indices 
                if idx < len(self.chunks) and
                re.search(r'\d+ì–µ|\d+%', self.chunks[idx].get('content', ''))
            ]
            if filtered_indices:
                target_indices = filtered_indices
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        enhanced_query = f"{query} {' '.join(search_keywords)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        k_search = min(len(target_indices), max(1, top_k * 5))
        
        try:
            scores, indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error: {str(e)}")
            return [], {'search_time': time.time() - start_time, 'error': str(e)}
        
        # ê²°ê³¼ êµ¬ì„± ë° ê´€ë ¨ì„± ë¶€ìŠ¤íŒ…
        results = []
        target_set = set(target_indices)
        
        for idx, score in zip(indices[0], scores[0]):
            if idx < 0 or idx >= len(self.chunks):
                continue
                
            if idx not in target_set:
                continue
                
            chunk = self.chunks[idx]
            
            # GPT ë¶„ì„ê³¼ì˜ ê´€ë ¨ì„± ê³„ì‚°
            relevance_boost = self._calculate_gpt_relevance(
                chunk['content'], gpt_analysis
            )
            
            result = SearchResult(
                chunk_id=chunk.get('chunk_id', str(idx)),
                content=chunk.get('content', ''),
                score=float(score) * (1 + relevance_boost),
                source=chunk.get('source', 'Unknown'),
                page=chunk.get('page', 0),
                chunk_type=chunk.get('chunk_type', 'unknown'),
                metadata=json.loads(chunk.get('metadata', '{}'))
            )
            
            results.append(result)
            
            if len(results) >= top_k * 2:
                break
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ ì„ íƒ
        results.sort(key=lambda x: x.score, reverse=True)
        results = results[:top_k]
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': len(target_indices),
            'search_method': 'focused_vector',
            'results_count': len(results)
        }
        
        return results, stats
    
    async def _gpt_guided_comprehensive_search(self, query: str, gpt_analysis: Dict, 
                                              top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¢…í•© ê²€ìƒ‰"""
        start_time = time.time()
        
        all_results = []
        
        # ëª¨ë“  ê´€ë ¨ ë²•ì  ê°œë…ì— ëŒ€í•´ ê²€ìƒ‰
        for concept in gpt_analysis['legal_concepts']:
            if concept['relevance'] in ['primary', 'secondary']:
                manual = concept['concept']
                if manual in self.manual_indices:
                    partial_results = await self._search_in_manual(
                        query, manual, concept['specific_aspects'], top_k // 2
                    )
                    all_results.extend(partial_results)
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ ê²°ê³¼ ì„ íƒ
        seen_chunks = set()
        unique_results = []
        for result in sorted(all_results, key=lambda x: x.score, reverse=True):
            if result.chunk_id not in seen_chunks:
                seen_chunks.add(result.chunk_id)
                unique_results.append(result)
                if len(unique_results) >= top_k:
                    break
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': sum(len(self.manual_indices.get(c['concept'], [])) 
                                 for c in gpt_analysis['legal_concepts'] 
                                 if c['relevance'] in ['primary', 'secondary']),
            'search_method': 'comprehensive_multi_manual'
        }
        
        return unique_results, stats
    
    def _calculate_gpt_relevance(self, content: str, gpt_analysis: Dict) -> float:
        """GPT ë¶„ì„ ê²°ê³¼ì™€ ì²­í¬ ë‚´ìš©ì˜ ê´€ë ¨ì„± ê³„ì‚°"""
        relevance_boost = 0.0
        content_lower = content.lower()
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in gpt_analysis['search_strategy']['search_keywords']:
            if keyword.lower() in content_lower:
                relevance_boost += 0.1
        
        # ë‹µë³€ ìš”êµ¬ì‚¬í•­ê³¼ì˜ ë§¤ì¹­
        requirements = gpt_analysis['answer_requirements']
        if requirements.get('needs_specific_numbers') and re.search(r'\d+ì–µ|\d+%', content):
            relevance_boost += 0.2
        if requirements.get('needs_timeline') and re.search(r'\d+ì¼|ê¸°í•œ', content):
            relevance_boost += 0.2
        if requirements.get('needs_process_steps') and re.search(r'ì ˆì°¨|ë‹¨ê³„|ìˆœì„œ', content):
            relevance_boost += 0.15
        
        return min(relevance_boost, 0.5)
    
    async def _search_in_manual(self, query: str, manual: str, aspects: List[str], 
                               limit: int) -> List[SearchResult]:
        """íŠ¹ì • ë§¤ë‰´ì–¼ ë‚´ì—ì„œ ê²€ìƒ‰"""
        indices = self.manual_indices.get(manual, [])[:100]
        
        if not indices:
            return []
        
        enhanced_query = f"{query} {' '.join(aspects)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        k_search = min(len(indices), max(1, limit * 3))
        
        try:
            scores, search_indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error in manual search: {str(e)}")
            return []
        
        results = []
        indices_set = set(indices)
        
        for idx, score in zip(search_indices[0], scores[0]):
            if idx in indices_set and idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
                if len(results) >= limit:
                    break
        
        return results
    
    def _get_fallback_analysis(self, query: str, category: str = None) -> Dict:
        """GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ í´ë°± ë¶„ì„"""
        primary_manual = category or "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜"
        
        return {
            'query_analysis': {
                'core_intent': query,
                'actual_complexity': 'medium',
                'complexity_reason': 'GPT ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©'
            },
            'legal_concepts': [{
                'concept': primary_manual,
                'relevance': 'primary',
                'specific_aspects': []
            }],
            'search_strategy': {
                'approach': 'focused_search',
                'primary_manual': primary_manual,
                'search_keywords': query.split()[:5],
                'expected_chunks_needed': 20,
                'rationale': 'ê¸°ë³¸ ê²€ìƒ‰ ì „ëµ'
            },
            'answer_requirements': {
                'needs_specific_numbers': True,
                'needs_process_steps': True,
                'needs_timeline': False,
                'needs_exceptions': False,
                'needs_multiple_perspectives': False
            }
        }

# ===== ë‹µë³€ ìƒì„± í•¨ìˆ˜ =====
def determine_temperature(query: str, complexity: str, model: str) -> float:
    """ì§ˆë¬¸ ìœ í˜•, ë³µì¡ë„, ëª¨ë¸ì— ë”°ë¼ ìµœì ì˜ temperature ê²°ì •
    
    TemperatureëŠ” AIì˜ ì°½ì˜ì„± ìˆ˜ì¤€ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
    ë‚®ì„ìˆ˜ë¡ ì¼ê´€ë˜ê³  ì •í™•í•œ ë‹µë³€ì„,
    ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•˜ê³  ì°½ì˜ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    query_lower = query.lower()
    
    # ëª¨ë¸ë³„ ê¸°ë³¸ temperature
    base_temps = {
        'gpt-4o-mini': {
            'simple': 0.1,
            'medium': 0.2,
            'complex': 0.3
        },
        'o4-mini': {
            'simple': 0.1,
            'medium': 0.3,
            'complex': 0.5
        },
        'gpt-4o': {
            'simple': 0.2,
            'medium': 0.4,
            'complex': 0.6
        }
    }
    
    temp = base_temps.get(model, {}).get(complexity, 0.3)
    
    # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì¡°ì •
    if any(keyword in query_lower for keyword in ['ì–¸ì œ', 'ë©°ì¹ ', 'ê¸°í•œ', 'ë‚ ì§œ', 'ê¸ˆì•¡', '%']):
        temp = min(temp, 0.1)  # ì‚¬ì‹¤ í™•ì¸ì€ ì •í™•ì„±ì´ ì¤‘ìš”
    elif any(keyword in query_lower for keyword in ['ì „ëµ', 'ëŒ€ì‘', 'ë¦¬ìŠ¤í¬', 'ì£¼ì˜', 'ê¶Œì¥']):
        temp = max(temp, 0.5)  # ì „ëµì  ì¡°ì–¸ì€ ë‹¤ì–‘í•œ ê´€ì  í•„ìš”
    
    return temp

async def generate_answer(query: str, results: List[SearchResult], stats: Dict) -> Tuple[str, Dict]:
    """ì„ íƒëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±
    
    ì´ í•¨ìˆ˜ëŠ” ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ
    ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë§ˆì¹˜ ì „ë¬¸ê°€ê°€ ìë£Œë¥¼ ê²€í† í•œ í›„
    ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    """
    
    # ì„ íƒëœ ëª¨ë¸ í™•ì¸
    model = stats.get('selected_model', 'gpt-4o-mini')
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    
    # êµ¬ë²„ì „ ê²½ê³ ê°€ ìˆìœ¼ë©´ ë¨¼ì € í‘œì‹œ
    if stats.get('has_version_conflicts'):
        context_parts.append("âš ï¸ ì£¼ì˜: ì¼ë¶€ ì°¸ê³  ìë£Œì— êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
    
    for i, result in enumerate(results[:5]):
        # êµ¬ë²„ì „ ì •ë³´ í‘œì‹œ
        warning_marker = ""
        if result.metadata.get('has_outdated_info'):
            warnings = result.metadata.get('warnings', [])
            if warnings:
                warning_marker = " âš ï¸ [êµ¬ë²„ì „ ì •ë³´ í¬í•¨]"
        
        context_parts.append(f"""
[ì°¸ê³  {i+1}] {result.source} (í˜ì´ì§€ {result.page}){warning_marker}
{result.content}
""")
    
    context = "\n---\n".join(context_parts)
    
    # ë³µì¡ë„ ì •ë³´ í™œìš©
    complexity = stats.get('complexity', 'medium')
    temperature = determine_temperature(query, complexity, model)
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ì§€ì‹œì‚¬í•­
    gpt_analysis = stats.get('gpt_analysis', {})
    answer_requirements = gpt_analysis.get('answer_requirements', {})
    
    # ë™ì  ì§€ì‹œì‚¬í•­ ìƒì„±
    instructions = []
    if answer_requirements.get('needs_specific_numbers'):
        instructions.append("ì •í™•í•œ ê¸ˆì•¡ê³¼ ìˆ˜ì¹˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.")
    if answer_requirements.get('needs_process_steps'):
        instructions.append("ì ˆì°¨ë¥¼ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”.")
    if answer_requirements.get('needs_timeline'):
        instructions.append("ê¸°í•œê³¼ ì‹œê°„ ìˆœì„œë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”.")
    if answer_requirements.get('needs_exceptions'):
        instructions.append("ì˜ˆì™¸ ì‚¬í•­ê³¼ íŠ¹ë³„í•œ ê²½ìš°ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.")
    
    instruction_text = " ".join(instructions) if instructions else "ëª…í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    
    # ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ì§€ì‹œì‚¬í•­
    category = stats.get('category')
    category_instructions = {
        'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': "ì´ì‚¬íšŒ ì˜ê²° ìš”ê±´, ê³µì‹œ ê¸°í•œ, ë©´ì œ ì¡°ê±´ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.",
        'í˜„í™©ê³µì‹œ': "ê³µì‹œ ì£¼ì²´, ì‹œê¸°, ì œì¶œ ì„œë¥˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.",
        'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': "ê³µì‹œ ëŒ€ìƒ ê±°ë˜, ê¸°í•œ, ì œì¶œ ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”."
    }
    
    if category and category in category_instructions:
        instruction_text += f"\n{category_instructions[category]}"
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥)
2. ìƒì„¸ ì„¤ëª… (ê·¼ê±° ì¡°í•­ í¬í•¨)
3. ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì˜ˆì™¸ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
4. ì‹¤ë¬´ ì ìš© ì‹œ ê¶Œì¥ì‚¬í•­ (í•„ìš”í•œ ê²½ìš°)

{instruction_text}

ì¤‘ìš”: êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°, ë°˜ë“œì‹œ ìµœì‹  ê¸°ì¤€ì„ ëª…ì‹œí•˜ì„¸ìš”."""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

{"ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ" if complexity == 'simple' else "ìƒì„¸í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ"} ë‹µë³€í•´ì£¼ì„¸ìš”."""}
    ]
    
    # API í˜¸ì¶œ ì‹œì‘
    api_start = time.time()
    
    try:
        response = openai.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=1500 if complexity == 'complex' else 1000
        )
        
        answer = response.choices[0].message.content
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
        estimated_tokens = len(context) / 4 + len(answer) / 4
        
        # ë¹„ìš© ê³„ì‚°
        cost = stats['selection_info']['estimated_cost']
        
        # ì˜ˆì‚°ì— ì¶”ê°€
        budget_manager = BudgetManager()
        budget_manager.add_usage(cost)
        
        generation_stats = {
            'generation_time': time.time() - api_start,
            'model': model,
            'temperature': temperature,
            'estimated_tokens': estimated_tokens,
            'cost': cost
        }
        
        return answer, generation_stats
        
    except Exception as e:
        logger.error(f"Answer generation error: {str(e)}")
        
        # í´ë°± ë‹µë³€
        fallback_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì˜¤ë¥˜ ë‚´ìš©: {str(e)}

ë‹¤ìŒ ì°¸ê³  ìë£Œë¥¼ ì§ì ‘ í™•ì¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:
"""
        for i, result in enumerate(results[:3]):
            fallback_answer += f"\n{i+1}. {result.source} (í˜ì´ì§€ {result.page})"
        
        return fallback_answer, {
            'generation_time': time.time() - api_start,
            'error': str(e),
            'model': model,
            'cost': 0
        }

# ===== ì„±ëŠ¥ ì‹œê°í™” í•¨ìˆ˜ë“¤ =====
def create_complexity_gauge(score: float) -> go.Figure:
    """ë³µì¡ë„ë¥¼ ê²Œì´ì§€ ì°¨íŠ¸ë¡œ í‘œì‹œ"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number+delta",
        value = score,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "ì§ˆë¬¸ ë³µì¡ë„"},
        delta = {'reference': 0.5},
        gauge = {
            'axis': {'range': [None, 1]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [0, 0.3], 'color': "lightgreen"},
                {'range': [0.3, 0.7], 'color': "yellow"},
                {'range': [0.7, 1], 'color': "lightcoral"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 0.85
            }
        }
    ))
    
    fig.update_layout(height=200, margin=dict(l=20, r=20, t=40, b=20))
    return fig

def create_budget_pie_chart(used: float, remaining: float) -> go.Figure:
    """ì˜ˆì‚° ì‚¬ìš© í˜„í™©ì„ íŒŒì´ ì°¨íŠ¸ë¡œ í‘œì‹œ"""
    fig = go.Figure(data=[go.Pie(
        labels=['ì‚¬ìš©ë¨', 'ë‚¨ìŒ'],
        values=[used, remaining],
        hole=.3,
        marker_colors=['#ff6b6b', '#51cf66'] if remaining > 0 else ['#ff6b6b', '#868e96']
    )])
    
    fig.update_traces(
        textposition='inside',
        textinfo='percent+label',
        hoverinfo='label+value+percent'
    )
    
    fig.update_layout(
        title=f"ì¼ì¼ ì˜ˆì‚° í˜„í™© (${used:.2f} / $50.00)",
        height=300,
        margin=dict(l=20, r=20, t=40, b=20),
        showlegend=False
    )
    
    return fig

def create_model_usage_chart(usage_stats: Dict) -> go.Figure:
    """ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„ë¥¼ ë§‰ëŒ€ ì°¨íŠ¸ë¡œ í‘œì‹œ"""
    models = list(usage_stats.keys())
    counts = [stats['count'] for stats in usage_stats.values()]
    costs = [stats['total_cost'] for stats in usage_stats.values()]
    
    fig = go.Figure()
    
    # ì‚¬ìš© íšŸìˆ˜
    fig.add_trace(go.Bar(
        name='ì‚¬ìš© íšŸìˆ˜',
        x=models,
        y=counts,
        yaxis='y',
        offsetgroup=1
    ))
    
    # ì´ ë¹„ìš©
    fig.add_trace(go.Bar(
        name='ì´ ë¹„ìš© ($)',
        x=models,
        y=costs,
        yaxis='y2',
        offsetgroup=2
    ))
    
    fig.update_layout(
        title='ëª¨ë¸ë³„ ì‚¬ìš© í˜„í™©',
        xaxis=dict(title='ëª¨ë¸'),
        yaxis=dict(title='ì‚¬ìš© íšŸìˆ˜', side='left'),
        yaxis2=dict(title='ë¹„ìš© ($)', overlaying='y', side='right'),
        hovermode='x unified',
        height=300
    )
    
    return fig

# ===== ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© =====
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ
    
    ì´ í•¨ìˆ˜ëŠ” ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ì–´
    í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
    ìºì‹±ì„ í†µí•´ ì¬ì‹¤í–‰ ì‹œ ë¹ ë¥´ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["manuals_vector_db.index", "all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            st.info("ğŸ’¡ prepare_pdfs_ftc.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
            return None, None, None, None
        
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)"):
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            index = faiss.read_index("manuals_vector_db.index")
            logger.info(f"Loaded FAISS index with {index.ntotal} vectors")
            
            # ì²­í¬ ë°ì´í„° ë¡œë“œ
            with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
                chunks = json.load(f)
            logger.info(f"Loaded {len(chunks)} chunks")
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            try:
                embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
                logger.info("Loaded Korean embedding model")
            except Exception as e:
                st.warning("í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                logger.warning(f"Using fallback embedding model: {str(e)}")
            
            # ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            try:
                reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
                logger.info("Loaded Korean reranker model")
            except:
                reranker_model = None
                logger.warning("Reranker model not loaded")
        
        return embedding_model, reranker_model, index, chunks
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(f"Initialization error: {str(e)}")
        logger.debug(traceback.format_exc())
        return None, None, None, None

# ===== ë©”ì¸ UI =====
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” ì „ì²´ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬ì„±í•˜ê³ 
    ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    
    # í—¤ë” í‘œì‹œ
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ í†µí•© AI Q&A ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    models = load_models_and_data()
    if not all(models):
        st.stop()
    
    embedding_model, reranker_model, index, chunks = models
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    if 'rag_pipeline' not in st.session_state:
        with st.spinner("ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘..."):
            st.session_state.rag_pipeline = OptimizedHybridRAGPipeline(
                embedding_model, reranker_model, index, chunks
            )
    
    rag = st.session_state.rag_pipeline
    
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ëª¨ë¸ ì‚¬ìš© í†µê³„ ì´ˆê¸°í™”
    if 'model_usage_stats' not in st.session_state:
        st.session_state.model_usage_stats = {
            'gpt-4o-mini': {'count': 0, 'total_cost': 0},
            'o4-mini': {'count': 0, 'total_cost': 0},
            'gpt-4o': {'count': 0, 'total_cost': 0}
        }
    
    # ì±„íŒ… ì»¨í…Œì´ë„ˆ
    chat_container = st.container()
    
    with chat_container:
        # ì´ì „ ëŒ€í™” í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "user":
                    st.write(message["content"])
                else:
                    if isinstance(message["content"], dict):
                        # ë‹µë³€ í‘œì‹œ
                        st.write(message["content"]["answer"])
                        
                        # êµ¬ë²„ì „ ê²½ê³  í‘œì‹œ
                        if message["content"].get("has_version_conflicts"):
                            st.markdown("""
                            <div class="version-warning">
                            âš ï¸ <strong>ì£¼ì˜:</strong> ì¼ë¶€ ì°¸ê³  ìë£Œì— êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                            ìµœì‹  ê·œì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.
                            </div>
                            """, unsafe_allow_html=True)
                        
                        # ë©”íƒ€ ì •ë³´ í‘œì‹œ
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            model_used = message["content"].get("model_used", "unknown")
                            model_emoji = {
                                'gpt-4o-mini': 'ğŸŸ¢',
                                'o4-mini': 'ğŸŸ¡',
                                'gpt-4o': 'ğŸ”µ'
                            }.get(model_used, 'âšª')
                            st.caption(f"{model_emoji} {model_used}")
                        
                        with col2:
                            cost = message["content"].get("cost", 0)
                            if cost < 0.01:
                                cost_class = "cost-saved"
                            elif cost < 0.05:
                                cost_class = "cost-normal"
                            else:
                                cost_class = "cost-high"
                            st.caption(f'<span class="cost-efficiency {cost_class}">${cost:.4f}</span>', 
                                     unsafe_allow_html=True)
                        
                        with col3:
                            total_time = message["content"].get("total_time", 0)
                            st.caption(f"â±ï¸ {total_time:.1f}ì´ˆ")
                    else:
                        st.write(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            with st.chat_message("user"):
                st.write(prompt)
            
            # AI ì‘ë‹µ
            with st.chat_message("assistant"):
                total_start_time = time.time()
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                search_start_time = time.time()
                with st.spinner("ğŸ” ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ì¤‘..."):
                    results, search_stats = run_async_in_streamlit(
                        rag.process_query(prompt, top_k=5)
                    )
                search_time = time.time() - search_start_time
                
                # ë‹µë³€ ìƒì„±
                generation_start_time = time.time()
                with st.spinner(f"ğŸ’­ {search_stats.get('selected_model', 'AI')}ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer, generation_stats = run_async_in_streamlit(
                        generate_answer(prompt, results, search_stats)
                    )
                generation_time = time.time() - generation_start_time
                
                # ë‹µë³€ í‘œì‹œ
                st.write(answer)
                
                # êµ¬ë²„ì „ ê²½ê³  í‘œì‹œ
                if search_stats.get('has_version_conflicts'):
                    st.markdown("""
                    <div class="version-warning">
                    âš ï¸ <strong>ì£¼ì˜:</strong> ì¼ë¶€ ì°¸ê³  ìë£Œì— êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                    ìµœì‹  ê·œì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # êµ¬ì²´ì ì¸ ê²½ê³  ë‚´ìš©
                    for warning in search_stats.get('outdated_warnings', []):
                        st.warning(f"êµ¬ë²„ì „: {warning['found']} â†’ í˜„ì¬: {warning['current']} ({warning['regulation']})")
                
                # í†µê³„ ì •ë³´
                total_time = time.time() - total_start_time
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    model_used = search_stats.get('selected_model', 'unknown')
                    model_emoji = {
                        'gpt-4o-mini': 'ğŸŸ¢',
                        'o4-mini': 'ğŸŸ¡',
                        'gpt-4o': 'ğŸ”µ'
                    }.get(model_used, 'âšª')
                    st.metric("ëª¨ë¸", f"{model_emoji} {model_used}")
                
                with col2:
                    cost = generation_stats.get('cost', 0)
                    st.metric("ë¹„ìš©", f"${cost:.4f}")
                
                with col3:
                    st.metric("ê²€ìƒ‰", f"{search_time:.1f}ì´ˆ")
                
                with col4:
                    st.metric("ì „ì²´", f"{total_time:.1f}ì´ˆ")
                
                # ë³µì¡ë„ í‘œì‹œ
                complexity = search_stats.get('complexity', 'unknown')
                complexity_html = f'<span class="complexity-indicator complexity-{complexity}">{complexity.upper()}</span>'
                st.markdown(f"ì§ˆë¬¸ ë³µì¡ë„: {complexity_html}", unsafe_allow_html=True)
                
                # ìƒì„¸ ì •ë³´ (ì ‘ì„ ìˆ˜ ìˆìŒ)
                with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                    # íƒ­ êµ¬ì„±
                    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ë¶„ì„ ê³¼ì •", "ğŸ“š ì°¸ê³  ìë£Œ", "ğŸ¤– AI ë¶„ì„", "âš¡ ì„±ëŠ¥ ì§€í‘œ"])
                    
                    with tab1:
                        # ë³µì¡ë„ ê²Œì´ì§€
                        complexity_score = search_stats.get('complexity_analysis', {}).get('normalized_score', 0)
                        fig = create_complexity_gauge(complexity_score)
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # ëª¨ë¸ ì„ íƒ ì´ìœ 
                        st.info(f"**ì„ íƒ ì´ìœ **: {search_stats.get('selection_info', {}).get('reason', 'N/A')}")
                        
                        # ê²€ìƒ‰ ì „ëµ
                        st.markdown("**ê²€ìƒ‰ ì „ëµ**")
                        st.json({
                            "ì ‘ê·¼ ë°©ì‹": search_stats.get('search_approach', 'N/A'),
                            "ê²€ìƒ‰ ë°©ë²•": search_stats.get('search_method', 'N/A'),
                            "ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜": search_stats.get('searched_chunks', 0)
                        })
                    
                    with tab2:
                        for i, result in enumerate(results[:5]):
                            with st.container():
                                # ì¶œì²˜ ì •ë³´
                                st.markdown(f"**[{i+1}] {result.source}** - í˜ì´ì§€ {result.page}")
                                
                                # ë¬¸ì„œ ë‚ ì§œ ë° ê²½ê³ 
                                if result.document_date:
                                    st.caption(f"ğŸ“… ë¬¸ì„œ ë‚ ì§œ: {result.document_date}")
                                
                                if result.metadata.get('has_outdated_info'):
                                    st.error("âš ï¸ ì´ ë¬¸ì„œì—ëŠ” êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                                
                                # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                                content_preview = result.content[:300] + "..." if len(result.content) > 300 else result.content
                                st.text(content_preview)
                                
                                # ê´€ë ¨ë„ ì ìˆ˜
                                st.caption(f"ê´€ë ¨ë„ ì ìˆ˜: {result.score:.2f}")
                                st.divider()
                    
                    with tab3:
                        # GPT ë¶„ì„ ê²°ê³¼
                        gpt_analysis = search_stats.get('gpt_analysis', {})
                        
                        st.markdown("**ì§ˆë¬¸ ë¶„ì„**")
                        st.json(gpt_analysis.get('query_analysis', {}))
                        
                        st.markdown("**ë²•ì  ê°œë…**")
                        st.json(gpt_analysis.get('legal_concepts', []))
                        
                        st.markdown("**ë‹µë³€ ìš”êµ¬ì‚¬í•­**")
                        requirements = gpt_analysis.get('answer_requirements', {})
                        req_text = []
                        if requirements.get('needs_specific_numbers'):
                            req_text.append("âœ“ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ í•„ìš”")
                        if requirements.get('needs_process_steps'):
                            req_text.append("âœ“ ë‹¨ê³„ë³„ ì ˆì°¨ í•„ìš”")
                        if requirements.get('needs_timeline'):
                            req_text.append("âœ“ ì‹œê°„ ìˆœì„œ í•„ìš”")
                        st.write("\n".join(req_text) if req_text else "íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­ ì—†ìŒ")
                    
                    with tab4:
                        # ì„±ëŠ¥ ì§€í‘œ
                        metrics_data = {
                            "ë¶„ì„ ì‹œê°„": f"{search_stats.get('analysis_time', 0):.2f}ì´ˆ",
                            "ê²€ìƒ‰ ì‹œê°„": f"{search_time:.2f}ì´ˆ",
                            "ë‹µë³€ ìƒì„± ì‹œê°„": f"{generation_time:.2f}ì´ˆ",
                            "ì´ ì²˜ë¦¬ ì‹œê°„": f"{total_time:.2f}ì´ˆ",
                            "ì˜ˆìƒ í† í° ìˆ˜": generation_stats.get('estimated_tokens', 0),
                            "ì˜¨ë„ ì„¤ì •": generation_stats.get('temperature', 0)
                        }
                        
                        for key, value in metrics_data.items():
                            st.metric(key, value)
                
                # ëª¨ë¸ ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
                if model_used in st.session_state.model_usage_stats:
                    st.session_state.model_usage_stats[model_used]['count'] += 1
                    st.session_state.model_usage_stats[model_used]['total_cost'] += cost
                
                # ì‘ë‹µ ë°ì´í„° ì €ì¥
                response_data = {
                    "answer": answer,
                    "model_used": model_used,
                    "cost": cost,
                    "total_time": total_time,
                    "complexity": complexity,
                    "has_version_conflicts": search_stats.get('has_version_conflicts', False),
                    "search_stats": search_stats,
                    "generation_stats": generation_stats
                }
                st.session_state.messages.append({"role": "assistant", "content": response_data})
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ’° ë¹„ìš© ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ")
        
        # ì˜ˆì‚° í˜„í™©
        budget_manager = BudgetManager()
        budget_status = budget_manager.get_current_status()
        
        # ì˜ˆì‚° ìƒíƒœ ì•Œë¦¼
        if budget_status['budget_exhausted']:
            st.error("âš ï¸ ì¼ì¼ ì˜ˆì‚°ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        elif budget_status['is_budget_critical']:
            st.warning("âš ï¸ ì˜ˆì‚°ì´ 20% ë¯¸ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤!")
        
        # íŒŒì´ ì°¨íŠ¸
        fig = create_budget_pie_chart(budget_status['used'], budget_status['remaining'])
        st.plotly_chart(fig, use_container_width=True)
        
        # ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„
        st.subheader("ğŸ“Š ëª¨ë¸ë³„ ì‚¬ìš© í˜„í™©")
        
        # ë§‰ëŒ€ ì°¨íŠ¸
        if any(stats['count'] > 0 for stats in st.session_state.model_usage_stats.values()):
            fig = create_model_usage_chart(st.session_state.model_usage_stats)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ì•„ì§ ì‚¬ìš© ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìƒì„¸ í†µê³„ í…Œì´ë¸”
        stats_df = pd.DataFrame(st.session_state.model_usage_stats).T
        if not stats_df.empty:
            stats_df.columns = ['ì‚¬ìš© íšŸìˆ˜', 'ì´ ë¹„ìš©($)']
            stats_df['í‰ê·  ë¹„ìš©($)'] = stats_df['ì´ ë¹„ìš©($)'] / stats_df['ì‚¬ìš© íšŸìˆ˜'].replace(0, 1)
            st.dataframe(stats_df.round(4))
        
        st.divider()
        
        # ì˜ˆì‹œ ì§ˆë¬¸
        st.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        
        st.subheader("ğŸŸ¢ ê°„ë‹¨í•œ ì§ˆë¬¸ (gpt-4o-mini)")
        example_simple = [
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?",
            "ì´ì‚¬íšŒ ì˜ê²° ê¸ˆì•¡ ê¸°ì¤€ì€?",
            "í˜„í™©ê³µì‹œëŠ” ì–¸ì œ í•´ì•¼ í•˜ë‚˜ìš”?"
        ]
        for example in example_simple:
            if st.button(example, key=f"simple_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.subheader("ğŸŸ¡ í‘œì¤€ ì§ˆë¬¸ (o4-mini)")
        example_standard = [
            "ê³„ì—´ì‚¬ì™€ ìê¸ˆê±°ë˜ ì‹œ ì ˆì°¨ëŠ”?",
            "ë¹„ìƒì¥ì‚¬ ì£¼ì‹ ì–‘ë„ ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ”?",
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë©´ì œ ì¡°ê±´ì€?"
        ]
        for example in example_standard:
            if st.button(example, key=f"standard_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.subheader("ğŸ”µ ë³µì¡í•œ ì§ˆë¬¸")
        example_complex = [
            "AíšŒì‚¬ê°€ Bê³„ì—´ì‚¬ì— ìê¸ˆì„ ëŒ€ì—¬í•˜ë©´ì„œ ë™ì‹œì— Cê³„ì—´ì‚¬ì˜ ì§€ë¶„ì„ ì·¨ë“í•˜ëŠ” ê²½ìš° ì ìš©ë˜ëŠ” ê·œì œëŠ”?",
            "ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜í•  ë•Œ ê²€í† í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        ]
        for example in example_complex:
            if st.button(example, key=f"complex_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.divider()
        
        # ì‹œìŠ¤í…œ ì •ë³´
        with st.expander("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´"):
            st.info("""
            **ëª¨ë¸ íŠ¹ì„±**
            - ğŸŸ¢ gpt-4o-mini: ê°€ì¥ ë¹ ë¥´ê³  ê²½ì œì 
            - ğŸŸ¡ o4-mini: ë›°ì–´ë‚œ ì¶”ë¡  ëŠ¥ë ¥ (ì£¼ë ¥)
            - ğŸ”µ gpt-4o: íŠ¹ìˆ˜í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©
            
            **ìºì‹± ì •ì±…**
            - ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ìë™ ìºì‹±
            - ìœ ì‚¬í•œ ì§ˆë¬¸ì€ ìºì‹œ í™œìš©
            - êµ¬ë²„ì „ ì •ë³´ëŠ” ìºì‹± ì œì™¸
            """)
        
        # ë¦¬ì…‹ ë²„íŠ¼
        if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", type="secondary"):
            st.session_state.messages = []
            st.rerun()
    
    # í˜ì´ì§€ í•˜ë‹¨
    st.divider()
    st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ìƒì„±í•œ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬í•­ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    st.caption("ğŸ“… ì‹œìŠ¤í…œ ê¸°ì¤€ì¼: 2025ë…„ 1ì›” (ìµœì‹  ê·œì • ë°˜ì˜)")
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
