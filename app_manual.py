# íŒŒì¼ ì´ë¦„: app_manual.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - í†µí•© ê°œì„  ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
import re
from collections import defaultdict, Counter
import time
from dataclasses import dataclass
import os
import hashlib

# ===== 1. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ =====
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ê¹”ë”í•œ UIë¥¼ ìœ„í•œ CSS (ê¸°ìˆ ì  ì •ë³´ ìˆ¨ê¹€)
st.markdown("""
<style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display: none;}
    
    /* ë©”ì¸ í—¤ë” ìŠ¤íƒ€ì¼ */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 900px;
        margin: 0 auto;
    }
    
    /* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ ê°œì„  */
    [data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* ë‹µë³€ ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .stChatMessage {
        border-radius: 10px;
        margin-bottom: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# ===== 2. ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict

# ===== 3. ì§ˆë¬¸ ë¶„ë¥˜ê¸° (ê°œì„ ì‚¬í•­ 1: ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ê²€ìƒ‰) =====
class QuestionClassifier:
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë§¤ë‰´ì–¼ì„ ìš°ì„  ê²€ìƒ‰í• ì§€ ê²°ì •"""
    
    def __init__(self):
        # ê° ì¹´í…Œê³ ë¦¬ë³„ í•µì‹¬ í‚¤ì›Œë“œì™€ íŒ¨í„´
        self.categories = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': {
                'keywords': ['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 'ë‚´ë¶€ê±°ë˜', 'ì´ì‚¬íšŒ ì˜ê²°', 'ì´ì‚¬íšŒ', 'ì˜ê²°', 
                           'ê³„ì—´ì‚¬', 'ê³„ì—´íšŒì‚¬', 'íŠ¹ìˆ˜ê´€ê³„ì¸', 'ìê¸ˆ', 'ëŒ€ì—¬', 'ì°¨ì…', 'ë³´ì¦',
                           'ìê¸ˆê±°ë˜', 'ìœ ê°€ì¦ê¶Œ', 'ìì‚°ê±°ë˜', '50ì–µ', 'ê±°ë˜ê¸ˆì•¡'],
                'patterns': [r'ì´ì‚¬íšŒ.*ì˜ê²°', r'ê³„ì—´.*ê±°ë˜', r'ë‚´ë¶€.*ê±°ë˜'],
                'manual_pattern': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜.*ë§¤ë‰´ì–¼',
                'priority': 1
            },
            'í˜„í™©ê³µì‹œ': {
                'keywords': ['í˜„í™©ê³µì‹œ', 'ê¸°ì—…ì§‘ë‹¨', 'ì†Œì†íšŒì‚¬', 'ë™ì¼ì¸', 'ì¹œì¡±', 
                           'ì§€ë¶„ìœ¨', 'ì„ì›', 'ìˆœí™˜ì¶œì', 'ìƒí˜¸ì¶œì', 'ì§€ë°°êµ¬ì¡°',
                           'ê³„ì—´í¸ì…', 'ê³„ì—´ì œì™¸', 'ì£¼ì£¼í˜„í™©', 'ì„ì›í˜„í™©'],
                'patterns': [r'ê¸°ì—…ì§‘ë‹¨.*í˜„í™©', r'ì†Œì†.*íšŒì‚¬', r'ì§€ë¶„.*ë³€ë™'],
                'manual_pattern': 'ê¸°ì—…ì§‘ë‹¨í˜„í™©ê³µì‹œ.*ë§¤ë‰´ì–¼',
                'priority': 2
            },
            'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': {
                'keywords': ['ë¹„ìƒì¥', 'ì¤‘ìš”ì‚¬í•­', 'ì£¼ì‹', 'ì–‘ë„', 'ì–‘ìˆ˜', 'í•©ë³‘', 
                           'ë¶„í• ', 'ì˜ì—…ì–‘ë„', 'ì„ì›ë³€ê²½', 'ì¦ì', 'ê°ì',
                           'ì •ê´€ë³€ê²½', 'í•´ì‚°', 'ì²­ì‚°'],
                'patterns': [r'ë¹„ìƒì¥.*ê³µì‹œ', r'ì£¼ì‹.*ì–‘ë„', r'ì¤‘ìš”.*ì‚¬í•­'],
                'manual_pattern': 'ë¹„ìƒì¥ì‚¬.*ì¤‘ìš”ì‚¬í•­.*ë§¤ë‰´ì–¼',
                'priority': 3
            }
        }
    
    def classify(self, question: str) -> Tuple[str, float]:
        """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì‹ ë¢°ë„ë¥¼ ë°˜í™˜"""
        question_lower = question.lower()
        scores = {}
        
        for category, info in self.categories.items():
            score = 0
            matched_keywords = []
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ì ìš©)
            for i, keyword in enumerate(info['keywords']):
                if keyword in question_lower:
                    # ì•ìª½ í‚¤ì›Œë“œì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
                    weight = 1.0 if i < 5 else 0.7
                    score += weight
                    matched_keywords.append(keyword)
            
            # íŒ¨í„´ ë§¤ì¹­ (ì •ê·œí‘œí˜„ì‹)
            for pattern in info.get('patterns', []):
                if re.search(pattern, question_lower):
                    score += 1.5
            
            scores[category] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ì„ íƒ
        if scores:
            best_category = max(scores, key=scores.get)
            max_possible_score = len(self.categories[best_category]['keywords']) + \
                               len(self.categories[best_category].get('patterns', [])) * 1.5
            confidence = min(scores[best_category] / max_possible_score, 1.0)
            
            # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ None ë°˜í™˜
            if confidence < 0.15:
                return None, 0.0
                
            return best_category, confidence
        
        return None, 0.0

# ===== 4. ìµœì í™”ëœ RAG íŒŒì´í”„ë¼ì¸ (ê°œì„ ì‚¬í•­ 3, 6, 7: ì†ë„ ê°œì„  + ìºì‹±) =====
class OptimizedRAGPipeline:
    """ì†ë„ì™€ ì •í™•ë„ë¥¼ ê·¹ëŒ€í™”í•œ RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, embedding_model, reranker_model, index, chunks):
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunks = chunks
        self.classifier = QuestionClassifier()
        
        # ë§¤ë‰´ì–¼ë³„ ì²­í¬ ì¸ë±ìŠ¤ ë¯¸ë¦¬ êµ¬ì¶• (ë¹ ë¥¸ í•„í„°ë§)
        self.manual_indices = self._build_manual_indices()
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ê°œì„ ì‚¬í•­ 7)
        self.search_cache = {}
        self.cache_max_size = 100
        
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx, chunk in enumerate(self.chunks):
            source = chunk.get('source', '').lower()
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜'].append(idx)
            elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                indices['í˜„í™©ê³µì‹œ'].append(idx)
            elif 'ë¹„ìƒì¥' in source:
                indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­'].append(idx)
            else:
                indices['ê¸°íƒ€'].append(idx)
        
        return dict(indices)
    
    def search(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """í†µí•© ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸ (ê°œì„ ì‚¬í•­ 7)
        cache_key = hashlib.md5(f"{query}_{top_k}".encode()).hexdigest()
        if cache_key in self.search_cache:
            cached = self.search_cache[cache_key]
            stats = cached['stats'].copy()
            stats['cache_hit'] = True
            stats['search_time'] = 0.001
            return cached['results'], stats
        
        # 1. ì§ˆë¬¸ ë¶„ë¥˜ (ê°œì„ ì‚¬í•­ 1)
        category, confidence = self.classifier.classify(query)
        
        # 2. ê²€ìƒ‰ ì „ëµ ê²°ì •
        if category and confidence > 0.3:
            search_strategy = 'targeted'
            primary_indices = self.manual_indices.get(category, [])
            secondary_indices = []
            for cat, indices in self.manual_indices.items():
                if cat != category and cat != 'ê¸°íƒ€':
                    secondary_indices.extend(indices)
        else:
            search_strategy = 'general'
            primary_indices = list(range(len(self.chunks)))
            secondary_indices = []
        
        # 3. ìµœì í™”ëœ ë²¡í„° ê²€ìƒ‰ (ê°œì„ ì‚¬í•­ 3)
        results = self._perform_optimized_search(
            query, primary_indices, secondary_indices, top_k
        )
        
        # 4. í†µê³„ ìƒì„±
        search_time = time.time() - start_time
        stats = {
            'category': category,
            'confidence': confidence,
            'strategy': search_strategy,
            'search_time': search_time,
            'primary_searched': len(primary_indices),
            'total_chunks': len(self.chunks),
            'cache_hit': False
        }
        
        # 5. ë¹ ë¥¸ ê²€ìƒ‰ì€ ìºì‹œì— ì €ì¥
        if search_time < 2.0 and len(self.search_cache) < self.cache_max_size:
            self.search_cache[cache_key] = {
                'results': results,
                'stats': stats,
                'timestamp': time.time()
            }
        
        return results, stats
    
    def _perform_optimized_search(self, query: str, primary_indices: List[int], 
                                 secondary_indices: List[int], top_k: int) -> List[SearchResult]:
        """ìµœì í™”ëœ FAISS ê²€ìƒ‰ (ê°œì„ ì‚¬í•­ 3 í•µì‹¬)"""
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„± (í•œ ë²ˆë§Œ!)
        query_vector = self.embedding_model.encode([query])
        query_vector = np.array(query_vector, dtype=np.float32)
        
        # FAISS ì¸ë±ìŠ¤ ì§ì ‘ í™œìš©
        k_search = min(len(self.chunks), top_k * 20)
        scores, indices = self.index.search(query_vector, k_search)
        
        results = []
        seen_chunks = set()
        
        # ìš°ì„ ìˆœìœ„ ì¸ë±ìŠ¤ì—ì„œ ë¨¼ì € ê²°ê³¼ ìˆ˜ì§‘
        if primary_indices:
            primary_set = set(primary_indices)
            for idx, score in zip(indices[0], scores[0]):
                if idx in primary_set and idx not in seen_chunks:
                    seen_chunks.add(idx)
                    chunk = self.chunks[idx]
                    result = SearchResult(
                        chunk_id=chunk.get('chunk_id', str(idx)),
                        content=chunk['content'],
                        score=float(score),
                        source=chunk['source'],
                        page=chunk['page'],
                        chunk_type=chunk.get('chunk_type', 'unknown'),
                        metadata=json.loads(chunk.get('metadata', '{}'))
                    )
                    results.append(result)
                    if len(results) >= top_k:
                        break
        
        # ë¶€ì¡±í•˜ë©´ ë³´ì¡° ì¸ë±ìŠ¤ì—ì„œ ì¶”ê°€
        if len(results) < top_k and secondary_indices:
            secondary_set = set(secondary_indices)
            for idx, score in zip(indices[0], scores[0]):
                if idx in secondary_set and idx not in seen_chunks:
                    seen_chunks.add(idx)
                    chunk = self.chunks[idx]
                    result = SearchResult(
                        chunk_id=chunk.get('chunk_id', str(idx)),
                        content=chunk['content'],
                        score=float(score) * 0.8,  # ë³´ì¡° ê²°ê³¼ëŠ” ì ìˆ˜ ê°ì†Œ
                        source=chunk['source'],
                        page=chunk['page'],
                        chunk_type=chunk.get('chunk_type', 'unknown'),
                        metadata=json.loads(chunk.get('metadata', '{}'))
                    )
                    results.append(result)
                    if len(results) >= top_k:
                        break
        
        return results

# ===== 5. ë™ì  Temperature ë‹µë³€ ìƒì„± (ê°œì„ ì‚¬í•­ 4, 5) =====
def determine_temperature(query: str) -> float:
    """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ temperature ê²°ì •"""
    query_lower = query.lower()
    
    # ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ (ë‚®ì€ temperature)
    if any(keyword in query_lower for keyword in ['ì–¸ì œ', 'ë©°ì¹ ', 'ê¸°í•œ', 'ë‚ ì§œ', 'ê¸ˆì•¡', '%']):
        return 0.1
    
    # ì •ì˜ë‚˜ ë²”ìœ„ (ì¤‘ê°„ temperature)
    elif any(keyword in query_lower for keyword in ['ì •ì˜', 'ë²”ìœ„', 'í¬í•¨', 'í•´ë‹¹', 'ì˜ë¯¸']):
        return 0.3
    
    # ë³µì¡í•œ íŒë‹¨ (ë†’ì€ temperature)
    elif any(keyword in query_lower for keyword in ['ì–´ë–»ê²Œ', 'ê²½ìš°', 'ë§Œì•½', 'ì˜ˆì™¸', 'ê°€ëŠ¥']):
        return 0.5
    
    # ì „ëµì  ì¡°ì–¸ (ë” ë†’ì€ temperature)
    elif any(keyword in query_lower for keyword in ['ì „ëµ', 'ëŒ€ì‘', 'ë¦¬ìŠ¤í¬', 'ì£¼ì˜', 'ê¶Œì¥']):
        return 0.7
    
    return 0.3  # ê¸°ë³¸ê°’

def generate_answer(query: str, results: List[SearchResult], category: str = None) -> str:
    """GPT-4oë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±"""
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for i, result in enumerate(results[:5]):
        context_parts.append(f"""
[ì°¸ê³  {i+1}] {result.source} (í˜ì´ì§€ {result.page})
{result.content}
""")
    
    context = "\n---\n".join(context_parts)
    
    # ë™ì  temperature ê²°ì • (ê°œì„ ì‚¬í•­ 5)
    temperature = determine_temperature(query)
    
    # ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ì§€ì‹œì‚¬í•­
    category_instructions = {
        'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': "íŠ¹íˆ ì´ì‚¬íšŒ ì˜ê²° ìš”ê±´, ê³µì‹œ ê¸°í•œ, ë©´ì œ ì¡°ê±´ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.",
        'í˜„í™©ê³µì‹œ': "ê³µì‹œ ì£¼ì²´, ì‹œê¸°, ì œì¶œ ì„œë¥˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.",
        'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': "ê³µì‹œ ëŒ€ìƒ ê±°ë˜, ê¸°í•œ, ì œì¶œ ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”."
    }
    
    extra_instruction = category_instructions.get(category, "") if category else ""
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë§Œì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥)
2. ìƒì„¸ ì„¤ëª… (ê·¼ê±° ì¡°í•­ í¬í•¨)
3. ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì˜ˆì™¸ì‚¬í•­ (ìˆëŠ” ê²½ìš°)"""
    
    if temperature >= 0.5:
        system_prompt += "\në‹¤ì–‘í•œ ê´€ì ê³¼ ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."
    
    # GPT-4o í˜¸ì¶œ (ê°œì„ ì‚¬í•­ 4)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

{extra_instruction}

{"ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ" if temperature < 0.3 else "ìƒì„¸í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ"} ë‹µë³€í•´ì£¼ì„¸ìš”."""}
    ]
    
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=temperature,
        max_tokens=1500
    )
    
    return response.choices[0].message.content

# ===== 6. ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© =====
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    try:
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["manuals_vector_db.index", "all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            st.info("ğŸ’¡ prepare_pdfs_ftc.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
            return None, None, None, None
        
        # ë°ì´í„° ë¡œë“œ
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)"):
            # ë²¡í„° ì¸ë±ìŠ¤ì™€ ì²­í¬ ë°ì´í„° ë¡œë“œ
            index = faiss.read_index("manuals_vector_db.index")
            with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
                chunks = json.load(f)
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            try:
                embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            except Exception as e:
                st.warning("í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            
            # ì¬ì •ë ¬ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            try:
                reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
            except:
                reranker_model = None
        
        return embedding_model, reranker_model, index, chunks
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None, None, None, None

# ===== 7. ë©”ì¸ UI (ê°œì„ ì‚¬í•­ 2: ê¹”ë”í•œ UI) =====
def main():
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ ì „ë¬¸ Q&A ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë¡œë“œ
    models = load_models_and_data()
    if not all(models):
        st.stop()
    
    embedding_model, reranker_model, index, chunks = models
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = OptimizedRAGPipeline(embedding_model, reranker_model, index, chunks)
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ì»¨í…Œì´ë„ˆ
    chat_container = st.container()
    
    with chat_container:
        # ì´ì „ ëŒ€í™” í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "user":
                    st.write(message["content"])
                else:
                    # AI ì‘ë‹µ í‘œì‹œ
                    if isinstance(message["content"], dict):
                        st.write(message["content"]["answer"])
                        
                        # ì‹œê°„ ì •ë³´ í‘œì‹œ (ê°œì„ ì‚¬í•­ 6)
                        if "total_time" in message["content"]:
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ğŸ” ê²€ìƒ‰", f"{message['content']['search_time']:.1f}ì´ˆ")
                            with col2:
                                st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{message['content']['generation_time']:.1f}ì´ˆ")
                            with col3:
                                st.metric("â±ï¸ ì „ì²´", f"{message['content']['total_time']:.1f}ì´ˆ")
                    else:
                        st.write(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.write(prompt)
            
            # AI ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                # ì „ì²´ ì‹œê°„ ì¸¡ì • ì‹œì‘ (ê°œì„ ì‚¬í•­ 6)
                total_start_time = time.time()
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                search_start_time = time.time()
                with st.spinner("ğŸ” ê´€ë ¨ ìë£Œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                    results, stats = rag.search(prompt, top_k=5)
                search_time = time.time() - search_start_time
                
                # ë‹µë³€ ìƒì„±
                generation_start_time = time.time()
                with st.spinner("ğŸ’­ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer = generate_answer(prompt, results, stats.get('category'))
                generation_time = time.time() - generation_start_time
                
                # ì „ì²´ ì‹œê°„ ê³„ì‚°
                total_time = time.time() - total_start_time
                
                # ë‹µë³€ í‘œì‹œ
                st.write(answer)
                
                # ì‹œê°„ ì •ë³´ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ” ê²€ìƒ‰", f"{search_time:.1f}ì´ˆ")
                with col2:
                    st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{generation_time:.1f}ì´ˆ")
                with col3:
                    st.metric("â±ï¸ ì „ì²´", f"{total_time:.1f}ì´ˆ")
                
                # ì„±ëŠ¥ ë¶„ì„ (ì ‘ì„ ìˆ˜ ìˆê²Œ)
                with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                    # ê²€ìƒ‰ í†µê³„
                    if stats.get('category'):
                        st.info(f"ğŸ“‚ **{stats['category']}** ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ (ì‹ ë¢°ë„: {stats['confidence']:.0%})")
                        if stats.get('cache_hit'):
                            st.success("âš¡ ìºì‹œì—ì„œ ì¦‰ì‹œ ì‘ë‹µ!")
                        else:
                            st.info(f"ğŸ” {stats['primary_searched']}ê°œ ë¬¸ì„œ ìš°ì„  ê²€ìƒ‰ (ì „ì²´ {stats['total_chunks']}ê°œ ì¤‘)")
                    
                    # ì°¸ê³  ìë£Œ
                    st.subheader("ğŸ“š ì°¸ê³  ìë£Œ")
                    for i, result in enumerate(results[:3]):
                        st.caption(f"**{result.source}** - í˜ì´ì§€ {result.page}")
                        with st.container():
                            st.text(result.content[:300] + "..." if len(result.content) > 300 else result.content)
                    
                    # ì„±ëŠ¥ í‰ê°€
                    if total_time < 3:
                        st.success("âš¡ ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ ì†ë„!")
                    elif total_time < 5:
                        st.info("âœ… ì ì ˆí•œ ì‘ë‹µ ì†ë„")
                    else:
                        st.warning("â° ì‘ë‹µ ì‹œê°„ì´ ë‹¤ì†Œ ê¸¸ì—ˆìŠµë‹ˆë‹¤")
                
                # ì„¸ì…˜ì— ì €ì¥
                response_data = {
                    "answer": answer,
                    "search_time": search_time,
                    "generation_time": generation_time,
                    "total_time": total_time
                }
                st.session_state.messages.append({"role": "assistant", "content": response_data})
    
    # í•˜ë‹¨ ì•ˆë‚´
    st.divider()
    st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ìƒì„±í•œ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬í•­ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°” (ì˜ˆì‹œ ì§ˆë¬¸)
    with st.sidebar:
        st.header("ğŸ’¡ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸")
        
        st.subheader("ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜")
        if st.button("ì´ì‚¬íšŒ ì˜ê²°ì´ í•„ìš”í•œ ê±°ë˜ ê¸ˆì•¡ì€?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ì—ì„œ ì´ì‚¬íšŒ ì˜ê²°ì´ í•„ìš”í•œ ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ì€?"
            st.rerun()
        if st.button("ê³µì‹œ ê¸°í•œì€ ì–¸ì œê¹Œì§€ì¸ê°€ìš”?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œì€?"
            st.rerun()
            
        st.subheader("í˜„í™©ê³µì‹œ")
        if st.button("ê¸°ì—…ì§‘ë‹¨ í˜„í™©ê³µì‹œ ì‹œê¸°ëŠ”?"):
            st.session_state.new_question = "ê¸°ì—…ì§‘ë‹¨ í˜„í™©ê³µì‹œëŠ” ì–¸ì œ í•´ì•¼ í•˜ë‚˜ìš”?"
            st.rerun()
            
        st.subheader("ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­")
        if st.button("ì£¼ì‹ ì–‘ë„ ì‹œ ê³µì‹œ ì˜ë¬´ëŠ”?"):
            st.session_state.new_question = "ë¹„ìƒì¥íšŒì‚¬ ì£¼ì‹ ì–‘ë„ ì‹œ ê³µì‹œ ì˜ë¬´ê°€ ìˆë‚˜ìš”?"
            st.rerun()
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
