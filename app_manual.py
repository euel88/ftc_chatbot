# íŒŒì¼ ì´ë¦„: app_manual.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - ìµœì í™” ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set, TypedDict, Protocol, Iterator, Generator, OrderedDict, Any, Union
import re
from collections import defaultdict, Counter, OrderedDict
import time
from dataclasses import dataclass, field
import os
import hashlib
from enum import Enum
import asyncio
import logging
from contextlib import contextmanager
import traceback
import gc
import concurrent.futures
from functools import lru_cache
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta

# ===== ë¡œê¹… ì„¤ì • =====
# ë¡œê¹…ì€ ì‹œìŠ¤í…œì˜ ì‘ë™ ìƒí™©ì„ ê¸°ë¡í•˜ëŠ” ì¼ê¸°ì¥ê³¼ ê°™ìŠµë‹ˆë‹¤.
# ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë•Œ ì›ì¸ì„ ì°¾ê±°ë‚˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    logger = logging.getLogger('ftc_chatbot')
    logger.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ - ë¡œê·¸ë¥¼ íŒŒì¼ì— ì €ì¥
    file_handler = logging.FileHandler('ftc_chatbot_debug.log')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ - ê°œë°œ ì¤‘ í™•ì¸ìš©
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# ===== ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ í´ë˜ìŠ¤ =====
# ì˜ˆì™¸ í´ë˜ìŠ¤ëŠ” í”„ë¡œê·¸ë¨ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì˜¤ë¥˜ ìƒí™©ì„ 
# ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.
class RAGPipelineError(Exception):
    """RAG íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

class IndexError(RAGPipelineError):
    """ì¸ë±ìŠ¤ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class EmbeddingError(RAGPipelineError):
    """ì„ë² ë”© ìƒì„± ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class ModelSelectionError(RAGPipelineError):
    """ëª¨ë¸ ì„ íƒ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

# ===== ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € =====
@contextmanager
def error_context(operation_name: str, fallback_value=None):
    """
    ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    
    ì´ëŠ” ë§ˆì¹˜ ì•ˆì „ë§ê³¼ ê°™ì•„ì„œ, ì‘ì—… ì¤‘ ë¬¸ì œê°€ ë°œìƒí•´ë„
    í”„ë¡œê·¸ë¨ì´ ì™„ì „íˆ ì¤‘ë‹¨ë˜ì§€ ì•Šê³  ì ì ˆíˆ ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """
    try:
        logger.debug(f"Starting operation: {operation_name}")
        yield
    except Exception as e:
        logger.error(f"Error in {operation_name}: {str(e)}")
        logger.debug(f"Full traceback:\n{traceback.format_exc()}")
        
        if 'st' in globals():
            st.error(f"âš ï¸ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {operation_name}")
            with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                st.code(traceback.format_exc())
        
        if fallback_value is not None:
            return fallback_value
        raise

# ===== í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ =====
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS ìŠ¤íƒ€ì¼ - UIë¥¼ ë³´ê¸° ì¢‹ê²Œ ë§Œë“œëŠ” ë””ìì¸ ì„¤ì •
st.markdown("""
<style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display: none;}
    
    /* ë©”ì¸ í—¤ë” ìŠ¤íƒ€ì¼ */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 900px;
        margin: 0 auto;
    }
    
    /* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ ê°œì„  */
    [data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* ë³µì¡ë„ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .complexity-indicator {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 16px;
        font-size: 0.85rem;
        font-weight: 500;
        margin-left: 8px;
    }
    
    .complexity-simple {
        background-color: #d4edda;
        color: #155724;
    }
    
    .complexity-medium {
        background-color: #fff3cd;
        color: #856404;
    }
    
    .complexity-complex {
        background-color: #f8d7da;
        color: #721c24;
    }
    
    /* ë¹„ìš© íš¨ìœ¨ì„± í‘œì‹œ */
    .cost-efficiency {
        display: inline-block;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.8rem;
        margin-left: 4px;
    }
    
    .cost-saved {
        background-color: #d4edda;
        color: #155724;
    }
    
    .cost-normal {
        background-color: #e2e3e5;
        color: #383d41;
    }
    
    .cost-high {
        background-color: #f8d7da;
        color: #721c24;
    }
</style>
""", unsafe_allow_html=True)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# ===== íƒ€ì… ì •ì˜ =====
# íƒ€ì… ì •ì˜ëŠ” ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ëª…í™•íˆ í•˜ì—¬ ì½”ë“œì˜ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.
class ChunkDict(TypedDict):
    """ì²­í¬ ë°ì´í„°ì˜ íƒ€ì… ì •ì˜"""
    chunk_id: str
    content: str
    source: str
    page: int
    chunk_type: str
    metadata: str

class AnalysisResult(TypedDict):
    """ë¶„ì„ ê²°ê³¼ì˜ íƒ€ì… ì •ì˜"""
    query_analysis: dict
    complexity_score: float
    recommended_model: str
    search_strategy: dict

# ===== ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')

class QueryComplexity(Enum):
    """ì§ˆë¬¸ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"

# ===== LRU ìºì‹œ êµ¬í˜„ =====
class LRUCache:
    """
    ì‹œê°„ ê¸°ë°˜ ë§Œë£Œë¥¼ ì§€ì›í•˜ëŠ” LRU ìºì‹œ êµ¬í˜„
    
    ìºì‹œëŠ” ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë³´ê´€í•˜ì—¬
    ë°˜ë³µì ì¸ ê³„ì‚°ì„ í”¼í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
    """
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl  # Time To Live (ì´ˆ ë‹¨ìœ„)
        
    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ë§Œë£Œëœ í•­ëª©ì€ ì œê±°"""
        if key not in self.cache:
            return None
            
        value, timestamp = self.cache[key]
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
            
        # ìµœê·¼ ì‚¬ìš©ëœ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™
        self.cache.move_to_end(key)
        return value
        
    def put(self, key: str, value):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if key in self.cache:
            del self.cache[key]
            
        # ìºì‹œê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(self.cache) >= self.max_size:
            self.cache.popitem(last=False)
            
        self.cache[key] = (value, time.time())
        
    def clear_expired(self):
        """ë§Œë£Œëœ ëª¨ë“  í•­ëª© ì œê±°"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]

# ===== ë¹„ë™ê¸° ì‹¤í–‰ í—¬í¼ =====
def run_async_in_streamlit(coro):
    """
    Streamlit í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
    
    Streamlitì€ ê¸°ë³¸ì ìœ¼ë¡œ ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ë¯€ë¡œ,
    ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    """
    try:
        loop = asyncio.get_running_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, coro)
            return future.result()
    except RuntimeError:
        return asyncio.run(coro)

# ===== ë¹„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ =====
class BudgetManager:
    """
    API ì‚¬ìš© ë¹„ìš©ì„ ì¶”ì í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ì´ëŠ” ë§ˆì¹˜ ê°€ê³„ë¶€ë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒê³¼ ê°™ì•„ì„œ,
    ì–¼ë§ˆë‚˜ ì‚¬ìš©í–ˆê³  ì–¼ë§ˆë‚˜ ë‚¨ì•˜ëŠ”ì§€ë¥¼ í•­ìƒ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """
    def __init__(self, daily_budget: float = 50.0):
        self.daily_budget = daily_budget
        self.reset_time = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        # ëª¨ë¸ë³„ ë¹„ìš© ì •ë³´ (1M í† í°ë‹¹ ë‹¬ëŸ¬)
        self.model_costs = {
            'gpt-4o-mini': {
                'input': 0.15,
                'cached': 0.075,
                'output': 0.60
            },
            'o4-mini': {
                'input': 1.10,
                'cached': 0.275,
                'output': 4.40
            },
            'gpt-4o': {
                'input': 2.50,
                'cached': 1.25,
                'output': 10.00
            }
        }
    
    def calculate_cost(self, model: str, input_tokens: int, 
                      output_tokens: int, cached: bool = False) -> float:
        """í† í° ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ìš© ê³„ì‚°"""
        costs = self.model_costs[model]
        input_cost = costs['cached' if cached else 'input'] * (input_tokens / 1_000_000)
        output_cost = costs['output'] * (output_tokens / 1_000_000)
        return input_cost + output_cost
    
    def get_current_status(self) -> Dict:
        """í˜„ì¬ ì˜ˆì‚° ìƒí™© ë°˜í™˜"""
        # ì„¸ì…˜ ìƒíƒœì—ì„œ ì˜¤ëŠ˜ì˜ ì‚¬ìš©ëŸ‰ ê°€ì ¸ì˜¤ê¸°
        if 'daily_cost' not in st.session_state:
            st.session_state.daily_cost = 0.0
            
        # ë‚ ì§œê°€ ë°”ë€Œì—ˆìœ¼ë©´ ë¦¬ì…‹
        current_date = datetime.now().date()
        if 'last_reset_date' not in st.session_state:
            st.session_state.last_reset_date = current_date
        elif st.session_state.last_reset_date != current_date:
            st.session_state.daily_cost = 0.0
            st.session_state.last_reset_date = current_date
            
        used = st.session_state.daily_cost
        remaining = self.daily_budget - used
        
        return {
            'used': used,
            'remaining': remaining,
            'remaining_ratio': remaining / self.daily_budget,
            'is_budget_critical': remaining < self.daily_budget * 0.2
        }
    
    def add_usage(self, cost: float):
        """ì‚¬ìš© ë¹„ìš© ì¶”ê°€"""
        st.session_state.daily_cost = st.session_state.get('daily_cost', 0.0) + cost

# ===== ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ =====
class SimplifiedModelSelector:
    """
    ì„¸ ê°€ì§€ ëª¨ë¸(gpt-4o-mini, o4-mini, gpt-4o)ë§Œì„ ì‚¬ìš©í•˜ëŠ” 
    ë‹¨ìˆœí™”ë˜ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ
    
    ì´ ì‹œìŠ¤í…œì€ ë§ˆì¹˜ ì˜ì‚¬ê°€ í™˜ìì˜ ì¦ìƒì„ ë³´ê³  
    ì ì ˆí•œ ê²€ì‚¬ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.model_profiles = {
            'gpt-4o-mini': {
                'cost_per_1k': 0.00015,
                'strengths': ['speed', 'simple_queries', 'fact_checking'],
                'max_tokens': 2000,
                'decision_threshold': 0.3,
                'performance_score': 0.6
            },
            'o4-mini': {
                'cost_per_1k': 0.0011,
                'strengths': ['reasoning', 'analysis', 'multi_step'],
                'max_tokens': 4000,
                'decision_threshold': 0.85,
                'performance_score': 0.85  # ì¶”ë¡  ëŠ¥ë ¥ì´ ë›°ì–´ë‚¨
            },
            'gpt-4o': {
                'cost_per_1k': 0.0025,
                'strengths': ['long_context', 'creative', 'fallback'],
                'max_tokens': 8000,
                'decision_threshold': 0.95,
                'performance_score': 0.75  # o4-minië³´ë‹¤ ë¹„ì‹¸ì§€ë§Œ ì¶”ë¡ ì€ ì•½í•¨
            }
        }
        
        self.budget_manager = BudgetManager()
        
    def select_model(self, query: str, initial_assessment: Dict) -> Tuple[str, Dict]:
        """
        ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
        
        ì„ íƒ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        1. ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€
        2. í˜„ì¬ ì˜ˆì‚° ìƒí™© í™•ì¸
        3. ê° ëª¨ë¸ì˜ ê°•ì ê³¼ ë¹„ìš©ì„ ê³ ë ¤í•˜ì—¬ ìµœì  ì„ íƒ
        """
        
        # ì§ˆë¬¸ì˜ íŠ¹ì„±ì„ ì ìˆ˜í™” (0~1)
        complexity_score = self._calculate_complexity_score(query, initial_assessment)
        
        # í˜„ì¬ ì˜ˆì‚° ìƒí™©
        budget_status = self.budget_manager.get_current_status()
        
        # ëª…í™•í•œ ê·œì¹™ ê¸°ë°˜ ì„ íƒ
        if complexity_score < self.model_profiles['gpt-4o-mini']['decision_threshold']:
            selected_model = 'gpt-4o-mini'
            reason = "ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸ ë˜ëŠ” ì •ì˜ ì§ˆë¬¸"
            
        elif complexity_score < self.model_profiles['o4-mini']['decision_threshold']:
            selected_model = 'o4-mini'
            reason = "ì¶”ë¡ ê³¼ ë¶„ì„ì´ í•„ìš”í•œ í‘œì¤€ ì§ˆë¬¸"
            
        else:
            # íŠ¹ë³„í•œ ê²½ìš°ë¥¼ í™•ì¸
            if self._requires_long_context(query):
                selected_model = 'gpt-4o'
                reason = "ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ê°€ í•„ìš”í•œ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤"
            elif self._is_creative_task(query):
                selected_model = 'gpt-4o'
                reason = "ì°½ì˜ì  í•´ì„ì´ í•„ìš”í•œ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤"
            else:
                # ëŒ€ë¶€ë¶„ì˜ ë³µì¡í•œ ì§ˆë¬¸ë„ o4-miniê°€ ë” íš¨ê³¼ì 
                selected_model = 'o4-mini'
                reason = "ë³µì¡í•˜ì§€ë§Œ o4-miniì˜ ì¶”ë¡  ëŠ¥ë ¥ìœ¼ë¡œ ì¶©ë¶„"
        
        # ì˜ˆì‚°ì´ ë¶€ì¡±í•œ ê²½ìš° í•˜ìœ„ ëª¨ë¸ë¡œ ëŒ€ì²´
        if budget_status['is_budget_critical']:
            if selected_model == 'gpt-4o':
                selected_model = 'o4-mini'
                reason += " (ì˜ˆì‚° ì œì•½ìœ¼ë¡œ ëŒ€ì²´)"
            elif selected_model == 'o4-mini' and budget_status['remaining_ratio'] < 0.1:
                selected_model = 'gpt-4o-mini'
                reason += " (ì˜ˆì‚° ì œì•½ìœ¼ë¡œ ëŒ€ì²´)"
        
        selection_info = {
            'model': selected_model,
            'reason': reason,
            'complexity_score': complexity_score,
            'budget_remaining': budget_status['remaining_ratio'],
            'estimated_cost': self._estimate_query_cost(selected_model)
        }
        
        return selected_model, selection_info
    
    def _calculate_complexity_score(self, query: str, assessment: Dict) -> float:
        """
        ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ 0ì—ì„œ 1 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        ë³µì¡ë„ í‰ê°€ ìš”ì†Œ:
        - ì§ˆë¬¸ì˜ ê¸¸ì´
        - íŠ¹ì • í‚¤ì›Œë“œì˜ ì¡´ì¬
        - ë¬¸ì¥ êµ¬ì¡°ì˜ ë³µì¡ì„±
        - ì—¬ëŸ¬ ì£¼ì²´ë‚˜ ì¡°ê±´ì˜ ì–¸ê¸‰
        """
        score = 0.0
        
        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (0~0.3)
        query_length = len(query)
        if query_length < 50:
            score += 0.1
        elif query_length < 150:
            score += 0.2
        else:
            score += 0.3
            
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ (0~0.4)
        complex_keywords = ['ë§Œì•½', 'ê²½ìš°', 'ë™ì‹œì—', 'ì—¬ëŸ¬', 'ë¹„êµ', 'ë¶„ì„', 'ì „ëµ', 'ì¢…í•©ì ']
        keyword_count = sum(1 for keyword in complex_keywords if keyword in query)
        score += min(keyword_count * 0.1, 0.4)
        
        # êµ¬ì¡°ì  ë³µì¡ë„ (0~0.3)
        if '?' in query and query.count('?') > 1:
            score += 0.1
        if any(conj in query for conj in ['ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜']):
            score += 0.1
        if re.search(r'[A-Z].*[A-Z]', query):  # ì—¬ëŸ¬ ì£¼ì²´ê°€ ì–¸ê¸‰ë¨
            score += 0.1
            
        return min(score, 1.0)
    
    def _requires_long_context(self, query: str) -> bool:
        """ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ê°€ í•„ìš”í•œì§€ íŒë‹¨"""
        return len(query) > 1000 or 'ì „ì²´' in query or 'ëª¨ë“ ' in query
    
    def _is_creative_task(self, query: str) -> bool:
        """ì°½ì˜ì  ì‘ì—…ì¸ì§€ íŒë‹¨"""
        creative_keywords = ['ì‹œë‚˜ë¦¬ì˜¤', 'ìŠ¤í† ë¦¬', 'ì°½ì˜', 'ì œì•ˆ', 'ì•„ì´ë””ì–´']
        return any(keyword in query for keyword in creative_keywords)
    
    def _estimate_query_cost(self, model: str) -> float:
        """ì§ˆë¬¸ ì²˜ë¦¬ ì˜ˆìƒ ë¹„ìš©"""
        avg_tokens = 3000  # í‰ê·  í† í° ìˆ˜
        return self.model_profiles[model]['cost_per_1k'] * (avg_tokens / 1000) * 2  # ì…ì¶œë ¥ ëª¨ë‘ ê³ ë ¤

# ===== ìºì‹± ì „ëµ ì‹œìŠ¤í…œ =====
class SmartCacheStrategy:
    """
    ëª¨ë¸ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ìºì‹± ì „ëµ
    
    ìºì‹±ì€ ë§ˆì¹˜ ìì£¼ ì°¾ëŠ” ì±…ì„ ì±…ìƒ ìœ„ì— ì˜¬ë ¤ë†“ëŠ” ê²ƒê³¼ ê°™ì•„ì„œ,
    ë°˜ë³µì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ë¹ ë¥´ê³  ì €ë ´í•˜ê²Œ ë‹µë³€í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.cache_configs = {
            'gpt-4o-mini': {
                'ttl': 7200,  # 2ì‹œê°„ - ì €ë ´í•˜ë¯€ë¡œ ì˜¤ë˜ ë³´ê´€
                'similarity_threshold': 0.85,
                'cache_benefit_ratio': 0.5  # 50% ë¹„ìš© ì ˆê°
            },
            'o4-mini': {
                'ttl': 3600,  # 1ì‹œê°„
                'similarity_threshold': 0.9,
                'cache_benefit_ratio': 0.75  # 75% ë¹„ìš© ì ˆê°
            },
            'gpt-4o': {
                'ttl': 2400,  # 40ë¶„
                'similarity_threshold': 0.92,
                'cache_benefit_ratio': 0.5
            }
        }
        
        self.query_cache = LRUCache(max_size=200, ttl=7200)
        self.embedding_cache = LRUCache(max_size=500, ttl=10800)
    
    def should_use_cache(self, query: str, model: str, 
                        similar_cached_query: Optional[Dict]) -> bool:
        """ìºì‹œ ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •"""
        if not similar_cached_query:
            return False
        
        config = self.cache_configs[model]
        
        # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ì„ ë„˜ëŠ”ì§€ í™•ì¸
        similarity = similar_cached_query.get('similarity', 0)
        if similarity < config['similarity_threshold']:
            return False
        
        # ìºì‹œ ë‚˜ì´ í™•ì¸
        cache_age = time.time() - similar_cached_query.get('timestamp', 0)
        if cache_age > config['ttl']:
            return False
        
        return True
    
    def get_cached_result(self, query: str) -> Optional[Dict]:
        """ìºì‹œëœ ê²°ê³¼ ë°˜í™˜"""
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì§ˆë¬¸ ë¨¼ì € í™•ì¸
        cache_key = hashlib.md5(query.encode()).hexdigest()
        exact_match = self.query_cache.get(cache_key)
        if exact_match:
            return exact_match
        
        # ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ìœ ì‚¬ë„ ê³„ì‚° í•„ìš”)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ì œê³µ
        return None
    
    def store_result(self, query: str, result: Dict, model: str):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_key = hashlib.md5(query.encode()).hexdigest()
        result['cached_at'] = time.time()
        result['model'] = model
        self.query_cache.put(cache_key, result)

# ===== ì§ˆë¬¸ ë¶„ì„ê¸° =====
class EnhancedQueryAnalyzer:
    """
    GPT-4o-minië¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ ì§ˆë¬¸ ë¶„ì„ê¸°
    
    ì´ í´ë˜ìŠ¤ëŠ” ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€,
    ì–´ë–»ê²Œ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.analysis_cache = LRUCache(max_size=100, ttl=3600)
        
    async def analyze_query(self, query: str) -> Dict:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½"""
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(query.encode()).hexdigest()
        cached = self.analysis_cache.get(cache_key)
        if cached:
            return cached
        
        prompt = f"""
        ë‹¹ì‹ ì€ ê³µì •ê±°ë˜ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {query}
        
        ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "query_type": "simple/standard/complex",
            "main_topic": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜/í˜„í™©ê³µì‹œ/ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­/ê¸°íƒ€",
            "required_info": ["í•„ìš”í•œ ì •ë³´ 1", "í•„ìš”í•œ ì •ë³´ 2"],
            "search_keywords": ["ê²€ìƒ‰ í‚¤ì›Œë“œ 1", "ê²€ìƒ‰ í‚¤ì›Œë“œ 2"],
            "expected_answer_type": "fact/process/analysis/comparison"
        }}
        """
        
        try:
            response = openai.chat.completions.create(
                model="gpt-4o-mini",  # ë¶„ì„ì—ëŠ” ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=500,
                response_format={"type": "json_object"}
            )
            
            analysis = json.loads(response.choices[0].message.content)
            
            # ìºì‹œ ì €ì¥
            self.analysis_cache.put(cache_key, analysis)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Query analysis error: {str(e)}")
            # í´ë°± ë¶„ì„
            return {
                "query_type": "standard",
                "main_topic": "ê¸°íƒ€",
                "required_info": [],
                "search_keywords": query.split()[:5],
                "expected_answer_type": "fact"
            }

# ===== í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ =====
class OptimizedHybridRAGPipeline:
    """
    ë¹„ìš© ìµœì í™”ê°€ ì ìš©ëœ í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸
    
    ì´ í´ë˜ìŠ¤ëŠ” ì „ì²´ ì‹œìŠ¤í…œì˜ í•µì‹¬ìœ¼ë¡œ, ì§ˆë¬¸ì„ ë°›ì•„
    ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•˜ê³ , ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ë©°, ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, embedding_model, reranker_model, index, chunks):
        if not chunks:
            raise ValueError("No chunks provided to pipeline")
        
        if index.ntotal == 0:
            raise ValueError("FAISS index is empty")
        
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunks = chunks
        
        # ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œë“¤
        self.model_selector = SimplifiedModelSelector()
        self.query_analyzer = EnhancedQueryAnalyzer()
        self.cache_strategy = SmartCacheStrategy()
        self.budget_manager = BudgetManager()
        
        # ë§¤ë‰´ì–¼ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.manual_indices = self._build_manual_indices()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = []
        
        logger.info(f"Pipeline initialized with {len(chunks)} chunks")
    
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx, chunk in enumerate(self.chunks):
            source = chunk.get('source', '').lower()
            
            if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜'].append(idx)
            elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                indices['í˜„í™©ê³µì‹œ'].append(idx)
            elif 'ë¹„ìƒì¥' in source:
                indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­'].append(idx)
            else:
                indices['ê¸°íƒ€'].append(idx)
        
        return dict(indices)
    
    async def process_query(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """
        ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        
        ì²˜ë¦¬ ê³¼ì •:
        1. ìºì‹œ í™•ì¸
        2. ì§ˆë¬¸ ë¶„ì„
        3. ëª¨ë¸ ì„ íƒ
        4. ê²€ìƒ‰ ìˆ˜í–‰
        5. ë‹µë³€ ìƒì„±
        """
        start_time = time.time()
        
        # 1. ìºì‹œ í™•ì¸
        cached_result = self.cache_strategy.get_cached_result(query)
        if cached_result:
            logger.info(f"Cache hit for query: {query[:50]}...")
            return cached_result['results'], {
                'cache_hit': True,
                'total_time': 0.1,
                'model_used': cached_result['model']
            }
        
        # 2. ì§ˆë¬¸ ë¶„ì„
        analysis = await self.query_analyzer.analyze_query(query)
        
        # 3. ëª¨ë¸ ì„ íƒ
        selected_model, selection_info = self.model_selector.select_model(query, analysis)
        
        # 4. ê²€ìƒ‰ ìˆ˜í–‰
        search_results = await self._perform_search(query, analysis, top_k)
        
        # 5. í†µê³„ ì •ë³´ êµ¬ì„±
        stats = {
            'query_analysis': analysis,
            'selected_model': selected_model,
            'selection_info': selection_info,
            'search_time': time.time() - start_time,
            'cache_hit': False,
            'total_results': len(search_results)
        }
        
        # 6. ìºì‹œ ì €ì¥ (ê°„ë‹¨í•œ ì§ˆë¬¸ë§Œ)
        if analysis['query_type'] == 'simple':
            self.cache_strategy.store_result(query, {
                'results': search_results,
                'stats': stats
            }, selected_model)
        
        return search_results, stats
    
    async def _perform_search(self, query: str, analysis: Dict, top_k: int) -> List[SearchResult]:
        """ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰"""
        # ê´€ë ¨ ë§¤ë‰´ì–¼ í™•ì¸
        main_topic = analysis.get('main_topic', 'ê¸°íƒ€')
        relevant_indices = self.manual_indices.get(main_topic, [])
        
        if not relevant_indices:
            relevant_indices = list(range(min(len(self.chunks), 300)))
        
        # ê²€ìƒ‰ì–´ í™•ì¥
        search_keywords = analysis.get('search_keywords', [])
        enhanced_query = f"{query} {' '.join(search_keywords)}"
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_model.encode([enhanced_query])
        query_vector = np.array(query_embedding, dtype=np.float32)
        
        # FAISS ê²€ìƒ‰
        k_search = min(len(relevant_indices), max(1, top_k * 3))
        scores, indices = self.index.search(query_vector, k_search)
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        seen_chunks = set()
        
        for idx, score in zip(indices[0], scores[0]):
            if idx in relevant_indices and idx not in seen_chunks:
                seen_chunks.add(idx)
                chunk = self.chunks[idx]
                
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
                
                if len(results) >= top_k:
                    break
        
        # ë¦¬ë­í‚¹ (ì˜µì…˜)
        if self.reranker_model and len(results) > 0:
            # ë¦¬ë­í‚¹ ë¡œì§ (í•„ìš”ì‹œ êµ¬í˜„)
            pass
        
        return results

# ===== ë‹µë³€ ìƒì„± í•¨ìˆ˜ =====
async def generate_answer(query: str, results: List[SearchResult], stats: Dict) -> Tuple[str, Dict]:
    """
    ì„ íƒëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    
    ì´ í•¨ìˆ˜ëŠ” ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ
    ì™„ì„±ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # ì„ íƒëœ ëª¨ë¸ í™•ì¸
    model = stats.get('selected_model', 'gpt-4o-mini')
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for i, result in enumerate(results[:5]):  # ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
        context_parts.append(f"""
[ì°¸ê³  {i+1}] {result.source} (í˜ì´ì§€ {result.page})
{result.content}
""")
    
    context = "\n---\n".join(context_parts)
    
    # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    query_type = stats.get('query_analysis', {}).get('query_type', 'standard')
    
    if query_type == 'simple':
        instruction = "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        max_tokens = 500
    elif query_type == 'complex':
        instruction = "ë‹¨ê³„ë³„ë¡œ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        max_tokens = 1500
    else:
        instruction = "ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
        max_tokens = 1000
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

{instruction}

ë‹µë³€ êµ¬ì¡°:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥)
2. ìƒì„¸ ì„¤ëª… (í•„ìš”ì‹œ)
3. ì£¼ì˜ì‚¬í•­ (ìˆëŠ” ê²½ìš°)"""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}"""}
    ]
    
    # API í˜¸ì¶œ ì‹œì‘ ì‹œê°„
    api_start = time.time()
    
    try:
        # ëª¨ë¸ì— ë”°ë¥¸ temperature ì„¤ì •
        temperature = 0.1 if query_type == 'simple' else 0.3
        
        response = openai.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        answer = response.choices[0].message.content
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (ì‹¤ì œë¡œëŠ” responseì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        estimated_tokens = len(context) / 4 + len(answer) / 4
        
        # ë¹„ìš© ê³„ì‚°
        cost = stats['selection_info']['estimated_cost']
        
        # ì˜ˆì‚°ì— ì¶”ê°€
        if 'budget_manager' in globals():
            budget_manager = BudgetManager()
            budget_manager.add_usage(cost)
        
        generation_stats = {
            'generation_time': time.time() - api_start,
            'model': model,
            'estimated_tokens': estimated_tokens,
            'cost': cost
        }
        
        return answer, generation_stats
        
    except Exception as e:
        logger.error(f"Answer generation error: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", {
            'generation_time': time.time() - api_start,
            'error': str(e)
        }

# ===== ì„±ëŠ¥ ì‹œê°í™” í•¨ìˆ˜ë“¤ =====
def create_complexity_gauge(score: float) -> go.Figure:
    """ë³µì¡ë„ë¥¼ ê²Œì´ì§€ ì°¨íŠ¸ë¡œ í‘œì‹œ"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number",
        value = score,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "ì§ˆë¬¸ ë³µì¡ë„"},
        gauge = {
            'axis': {'range': [None, 1]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [0, 0.3], 'color': "lightgreen"},
                {'range': [0.3, 0.7], 'color': "yellow"},
                {'range': [0.7, 1], 'color': "lightcoral"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 0.85
            }
        }
    ))
    
    fig.update_layout(height=200, margin=dict(l=20, r=20, t=40, b=20))
    return fig

def create_budget_pie_chart(used: float, remaining: float) -> go.Figure:
    """ì˜ˆì‚° ì‚¬ìš© í˜„í™©ì„ íŒŒì´ ì°¨íŠ¸ë¡œ í‘œì‹œ"""
    fig = go.Figure(data=[go.Pie(
        labels=['ì‚¬ìš©ë¨', 'ë‚¨ìŒ'],
        values=[used, remaining],
        hole=.3,
        marker_colors=['#ff6b6b', '#51cf66']
    )])
    
    fig.update_traces(
        textposition='inside',
        textinfo='percent+label',
        hoverinfo='label+value+percent'
    )
    
    fig.update_layout(
        title="ì¼ì¼ ì˜ˆì‚° í˜„í™©",
        height=300,
        margin=dict(l=20, r=20, t=40, b=20),
        showlegend=False
    )
    
    return fig

# ===== ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© =====
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    try:
        required_files = ["manuals_vector_db.index", "all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            st.info("ğŸ’¡ prepare_pdfs_ftc.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
            return None, None, None, None
        
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)"):
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            index = faiss.read_index("manuals_vector_db.index")
            
            # ì²­í¬ ë°ì´í„° ë¡œë“œ
            with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
                chunks = json.load(f)
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            try:
                embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            except Exception as e:
                st.warning("í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            
            # ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            try:
                reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
            except:
                reranker_model = None
                logger.warning("Reranker model not loaded")
        
        return embedding_model, reranker_model, index, chunks
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(f"Initialization error: {str(e)}")
        return None, None, None, None

# ===== ë©”ì¸ UI =====
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜"""
    
    # í—¤ë” í‘œì‹œ
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ í†µí•© AI Q&A ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    models = load_models_and_data()
    if not all(models):
        st.stop()
    
    embedding_model, reranker_model, index, chunks = models
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    if 'rag_pipeline' not in st.session_state:
        st.session_state.rag_pipeline = OptimizedHybridRAGPipeline(
            embedding_model, reranker_model, index, chunks
        )
    
    rag = st.session_state.rag_pipeline
    
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ì»¨í…Œì´ë„ˆ
    chat_container = st.container()
    
    with chat_container:
        # ì´ì „ ëŒ€í™” í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "user":
                    st.write(message["content"])
                else:
                    if isinstance(message["content"], dict):
                        st.write(message["content"]["answer"])
                        
                        # ë©”íƒ€ ì •ë³´ í‘œì‹œ
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            model_used = message["content"].get("model_used", "unknown")
                            model_emoji = {
                                'gpt-4o-mini': 'ğŸŸ¢',
                                'o4-mini': 'ğŸŸ¡',
                                'gpt-4o': 'ğŸ”µ'
                            }.get(model_used, 'âšª')
                            st.caption(f"{model_emoji} {model_used}")
                        
                        with col2:
                            cost = message["content"].get("cost", 0)
                            if cost < 0.01:
                                cost_class = "cost-saved"
                            elif cost < 0.05:
                                cost_class = "cost-normal"
                            else:
                                cost_class = "cost-high"
                            st.caption(f'<span class="cost-efficiency {cost_class}">${cost:.4f}</span>', 
                                     unsafe_allow_html=True)
                        
                        with col3:
                            total_time = message["content"].get("total_time", 0)
                            st.caption(f"â±ï¸ {total_time:.1f}ì´ˆ")
                    else:
                        st.write(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            with st.chat_message("user"):
                st.write(prompt)
            
            # AI ì‘ë‹µ
            with st.chat_message("assistant"):
                total_start_time = time.time()
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                with st.spinner("ğŸ” ìµœì ì˜ AI ëª¨ë¸ì„ ì„ íƒí•˜ê³  ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ì¤‘..."):
                    results, search_stats = run_async_in_streamlit(
                        rag.process_query(prompt, top_k=5)
                    )
                
                # ë‹µë³€ ìƒì„±
                with st.spinner("ğŸ’­ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer, generation_stats = run_async_in_streamlit(
                        generate_answer(prompt, results, search_stats)
                    )
                
                # ë‹µë³€ í‘œì‹œ
                st.write(answer)
                
                # í†µê³„ ì •ë³´
                total_time = time.time() - total_start_time
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    model_used = search_stats.get('selected_model', 'unknown')
                    model_emoji = {
                        'gpt-4o-mini': 'ğŸŸ¢',
                        'o4-mini': 'ğŸŸ¡',
                        'gpt-4o': 'ğŸ”µ'
                    }.get(model_used, 'âšª')
                    st.caption(f"{model_emoji} {model_used}")
                
                with col2:
                    cost = generation_stats.get('cost', 0)
                    if cost < 0.01:
                        cost_class = "cost-saved"
                    elif cost < 0.05:
                        cost_class = "cost-normal"
                    else:
                        cost_class = "cost-high"
                    st.caption(f'<span class="cost-efficiency {cost_class}">${cost:.4f}</span>', 
                             unsafe_allow_html=True)
                
                with col3:
                    st.caption(f"â±ï¸ {total_time:.1f}ì´ˆ")
                
                # ìƒì„¸ ì •ë³´ (ì ‘ì„ ìˆ˜ ìˆìŒ)
                with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                    st.subheader("ğŸ“Š ì²˜ë¦¬ ê³¼ì •")
                    
                    # ë³µì¡ë„ ê²Œì´ì§€
                    complexity_score = search_stats.get('selection_info', {}).get('complexity_score', 0)
                    fig = create_complexity_gauge(complexity_score)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # ëª¨ë¸ ì„ íƒ ì´ìœ 
                    st.info(f"**ì„ íƒ ì´ìœ **: {search_stats.get('selection_info', {}).get('reason', 'N/A')}")
                    
                    # ì°¸ê³  ìë£Œ
                    st.subheader("ğŸ“š ì°¸ê³  ìë£Œ")
                    for i, result in enumerate(results[:3]):
                        st.caption(f"**{result.source}** - í˜ì´ì§€ {result.page}")
                        with st.container():
                            content_preview = result.content[:300] + "..." if len(result.content) > 300 else result.content
                            st.text(content_preview)
                
                # ì‘ë‹µ ë°ì´í„° ì €ì¥
                response_data = {
                    "answer": answer,
                    "model_used": model_used,
                    "cost": generation_stats.get('cost', 0),
                    "total_time": total_time,
                    "complexity_score": complexity_score
                }
                st.session_state.messages.append({"role": "assistant", "content": response_data})
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ’° ë¹„ìš© ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ")
        
        # ì˜ˆì‚° í˜„í™©
        budget_manager = BudgetManager()
        budget_status = budget_manager.get_current_status()
        
        # íŒŒì´ ì°¨íŠ¸
        fig = create_budget_pie_chart(budget_status['used'], budget_status['remaining'])
        st.plotly_chart(fig, use_container_width=True)
        
        # ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„
        if 'model_usage_stats' not in st.session_state:
            st.session_state.model_usage_stats = {
                'gpt-4o-mini': {'count': 0, 'total_cost': 0},
                'o4-mini': {'count': 0, 'total_cost': 0},
                'gpt-4o': {'count': 0, 'total_cost': 0}
            }
        
        st.subheader("ğŸ“Š ëª¨ë¸ë³„ ì‚¬ìš© í˜„í™©")
        stats_df = pd.DataFrame(st.session_state.model_usage_stats).T
        if not stats_df.empty:
            stats_df.columns = ['ì‚¬ìš© íšŸìˆ˜', 'ì´ ë¹„ìš©($)']
            st.dataframe(stats_df)
        
        st.divider()
        
        # ì˜ˆì‹œ ì§ˆë¬¸
        st.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        
        st.subheader("ğŸŸ¢ ê°„ë‹¨í•œ ì§ˆë¬¸")
        example_simple = [
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?",
            "ì´ì‚¬íšŒ ì˜ê²° ê¸ˆì•¡ ê¸°ì¤€ì€?",
            "í˜„í™©ê³µì‹œëŠ” ì–¸ì œ í•´ì•¼ í•˜ë‚˜ìš”?"
        ]
        for example in example_simple:
            if st.button(example, key=f"simple_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.subheader("ğŸŸ¡ í‘œì¤€ ì§ˆë¬¸")
        example_standard = [
            "ê³„ì—´ì‚¬ì™€ ìê¸ˆê±°ë˜ ì‹œ ì ˆì°¨ëŠ”?",
            "ë¹„ìƒì¥ì‚¬ ì£¼ì‹ ì–‘ë„ ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ”?",
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë©´ì œ ì¡°ê±´ì€?"
        ]
        for example in example_standard:
            if st.button(example, key=f"standard_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.subheader("ğŸ”µ ë³µì¡í•œ ì§ˆë¬¸")
        example_complex = [
            "AíšŒì‚¬ê°€ Bê³„ì—´ì‚¬ì— ìê¸ˆì„ ëŒ€ì—¬í•˜ë©´ì„œ ë™ì‹œì— Cê³„ì—´ì‚¬ì˜ ì§€ë¶„ì„ ì·¨ë“í•˜ëŠ” ê²½ìš° ì ìš©ë˜ëŠ” ê·œì œëŠ”?",
            "ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜í•  ë•Œ ê²€í† í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        ]
        for example in example_complex:
            if st.button(example, key=f"complex_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.divider()
        
        # ë¹„ìš© ì ˆê° íŒ
        st.info("""
        ğŸ’¡ **ë¹„ìš© ì ˆê° íŒ**
        - ê°„ë‹¨í•œ ì •ì˜ëŠ” ìë™ìœ¼ë¡œ ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©
        - ìœ ì‚¬í•œ ì§ˆë¬¸ì€ ìºì‹œ í™œìš©
        - o4-miniê°€ ëŒ€ë¶€ë¶„ì˜ ë¶„ì„ì— ìµœì 
        """)
    
    # í˜ì´ì§€ í•˜ë‹¨
    st.divider()
    st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ìƒì„±í•œ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬í•­ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        # ì…ë ¥ì°½ì— ì§ˆë¬¸ ì„¤ì •í•˜ëŠ” ë°©ë²•ì´ streamlitì—ì„œëŠ” ì§ì ‘ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ
        # ë©”ì‹œì§€ì— ì¶”ê°€í•˜ê³  rerun
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
