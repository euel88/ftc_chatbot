# ===== ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ (FAISS ëŒ€ì²´) =====
class SimpleVectorSearch:
    """FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰
    
    ì´ í´ë˜ìŠ¤ëŠ” NumPyë§Œì„ ì‚¬ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ì˜
    ë²¡í„° ê²€ìƒ‰ì„ êµ¬í˜„í•©ë‹ˆë‹¤. FAISSë³´ë‹¤ëŠ” ëŠë¦¬ì§€ë§Œ,
    ì–´ë–¤ í™˜ê²½ì—ì„œë„ ì‘ë™í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, embeddings: np.ndarray):
        """
        Args:
            embeddings: ë¬¸ì„œ ì„ë² ë”© ë°°ì—´ (n_docs, embedding_dim)
        """
        self.embeddings = embeddings
        # ì •ê·œí™”ëœ ì„ë² ë”© ì €ì¥ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”)
        self.normalized_embeddings = self._normalize_vectors(embeddings)
        logger.info(f"SimpleVectorSearch initialized with {len(embeddings)} documents")
    
    def _normalize_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """ë²¡í„° ì •ê·œí™” (L2 norm = 1)"""
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        norms = np.where(norms == 0, 1, norms)
        return vectors / norms
    
    def search(self, query_vector: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        
        Args:
            query_vector: ì¿¼ë¦¬ ë²¡í„° (1, embedding_dim)
            k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
        
        Returns:
            scores: ìœ ì‚¬ë„ ì ìˆ˜ ë°°ì—´
            indices: ë¬¸ì„œ ì¸ë±ìŠ¤ ë°°ì—´
        """
        # ì¿¼ë¦¬ ë²¡í„° ì •ê·œí™”
        query_norm = self._normalize_vectors(query_vector.reshape(1, -1))
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ ë²¡í„°ì˜ ë‚´ì )
        similarities = np.dot(self.normalized_embeddings, query_norm.T).squeeze()
        
        # ìƒìœ„ kê°œ ì„ íƒ
        top_k_indices = np.argpartition(similarities, -k)[-k:]
        top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]
        
        # FAISSì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        scores = similarities[top_k_indices].reshape(1, -1)
        indices = top_k_indices.reshape(1, -1)
        
        return scores, indices# íŒŒì¼ ì´ë¦„: app_improved.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - í•˜ì´ë¸Œë¦¬ë“œ ê°œì„  ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set, TypedDict, Protocol, Iterator, Generator, OrderedDict, Any, Union
import re
from collections import defaultdict, Counter, OrderedDict
import time
from dataclasses import dataclass, field
import os
import hashlib
from enum import Enum
import asyncio
import logging
from contextlib import contextmanager
import traceback
import gc
import concurrent.futures
from functools import lru_cache, wraps
import pickle
from pathlib import Path

# ì„ íƒì  import - ì—†ì–´ë„ ì•±ì´ ì‹¤í–‰ë˜ë„ë¡ ì²˜ë¦¬
try:
    from cryptography.fernet import Fernet
    CRYPTOGRAPHY_AVAILABLE = True
except ImportError:
    CRYPTOGRAPHY_AVAILABLE = False
    logging.warning("cryptography module not available - encryption features disabled")

try:
    import ijson
    IJSON_AVAILABLE = True
except ImportError:
    IJSON_AVAILABLE = False
    logging.warning("ijson module not available - using standard JSON loading")

try:
    import nest_asyncio
    nest_asyncio.apply()
    NEST_ASYNCIO_AVAILABLE = True
except ImportError:
    NEST_ASYNCIO_AVAILABLE = False
    logging.warning("nest_asyncio not available - async features may be limited")

# ===== ë¡œê¹… ì„¤ì • =====
def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    logger = logging.getLogger('ftc_chatbot')
    logger.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler('ftc_chatbot_debug.log')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# ===== ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ í´ë˜ìŠ¤ =====
class RAGPipelineError(Exception):
    """RAG íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

class IndexError(RAGPipelineError):
    """ì¸ë±ìŠ¤ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class EmbeddingError(RAGPipelineError):
    """ì„ë² ë”© ìƒì„± ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class GPTAnalysisError(RAGPipelineError):
    """GPT ë¶„ì„ ì‹¤íŒ¨ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class APIKeyError(RAGPipelineError):
    """API í‚¤ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

# ===== ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € =====
@contextmanager
def error_context(operation_name: str, fallback_value=None):
    """ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    
    ì´ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ëŠ” ì‘ì—… ì¤‘ ë°œìƒí•˜ëŠ” ì—ëŸ¬ë¥¼ ìš°ì•„í•˜ê²Œ ì²˜ë¦¬í•˜ê³ ,
    ì‚¬ìš©ìì—ê²Œ ì¹œìˆ™í•œ ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì£¼ë©´ì„œë„ ê°œë°œìë¥¼ ìœ„í•œ ìƒì„¸ ì •ë³´ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
    """
    try:
        logger.debug(f"Starting operation: {operation_name}")
        yield
    except Exception as e:
        logger.error(f"Error in {operation_name}: {str(e)}")
        logger.debug(f"Full traceback:\n{traceback.format_exc()}")
        
        if 'st' in globals():
            st.error(f"âš ï¸ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {operation_name}")
            with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                st.code(traceback.format_exc())
        
        if fallback_value is not None:
            return fallback_value
        raise

# ===== ë³´ì•ˆ ê°•í™”ëœ API ê´€ë¦¬ì =====
class SecureAPIManager:
    """API í‚¤ì™€ í˜¸ì¶œì„ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” API í‚¤ì˜ ì•ˆì „í•œ ì €ì¥ê³¼ ë¡œë“œ, ê·¸ë¦¬ê³  API í˜¸ì¶œì˜ 
    ì†ë„ ì œí•œ ë° ë¹„ìš© ì¶”ì ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. Streamlit Cloud í™˜ê²½ì—
    ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    """
    
    def __init__(self):
        self._api_key = None
        self._last_call_time = {}
        self._call_counts = {}
        self._cost_tracker = defaultdict(float)
        
        # ê° ëª¨ë¸ì˜ ì†ë„ ì œí•œ ì„¤ì •
        self._rate_limits = {
            'gpt-4o': {'calls_per_minute': 60, 'tokens_per_minute': 150000},
            'gpt-4o-mini': {'calls_per_minute': 500, 'tokens_per_minute': 200000},
            'o4-mini': {'calls_per_minute': 30, 'tokens_per_minute': 100000}  # ì¶”ë¡  ëª¨ë¸ì€ ë” ë³´ìˆ˜ì ìœ¼ë¡œ
        }
        
        # ëª¨ë¸ë³„ ë¹„ìš© (1K í† í°ë‹¹)
        self._model_costs = {
            'gpt-4o': {'input': 0.0025, 'output': 0.01},
            'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006},
            'o4-mini': {'input': 0.001, 'output': 0.004}  # ì¶”ë¡  ëª¨ë¸ì˜ ì˜ˆìƒ ë¹„ìš©
        }
    
    def load_api_key(self) -> str:
        """API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ
        
        ìš°ì„ ìˆœìœ„:
        1. Streamlit secrets (ê°œë°œ/í”„ë¡œë•ì…˜ í™˜ê²½)
        2. í™˜ê²½ ë³€ìˆ˜ (ë¡œì»¬ í…ŒìŠ¤íŠ¸)
        3. ì•”í˜¸í™”ëœ íŒŒì¼ (ê³ ê¸‰ ë³´ì•ˆ - ì„ íƒì )
        """
        # Streamlit secrets í™•ì¸ (ìµœìš°ì„ )
        try:
            if 'OPENAI_API_KEY' in st.secrets:
                self._api_key = st.secrets["OPENAI_API_KEY"]
                logger.info("API key loaded from Streamlit secrets")
                return self._api_key
        except Exception as e:
            logger.debug(f"Streamlit secrets not available: {e}")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        if os.environ.get('OPENAI_API_KEY'):
            self._api_key = os.environ.get('OPENAI_API_KEY')
            logger.info("API key loaded from environment variable")
            return self._api_key
        
        # ì•”í˜¸í™”ëœ íŒŒì¼ í™•ì¸ (ì„ íƒì  - cryptography ëª¨ë“ˆì´ ìˆì„ ë•Œë§Œ)
        try:
            from cryptography.fernet import Fernet
            encrypted_key_path = Path('.api_key.enc')
            if encrypted_key_path.exists():
                cipher_key = os.environ.get('API_CIPHER_KEY')
                if cipher_key:
                    cipher = Fernet(cipher_key.encode())
                    with open(encrypted_key_path, 'rb') as f:
                        encrypted_key = f.read()
                    self._api_key = cipher.decrypt(encrypted_key).decode()
                    logger.info("API key loaded from encrypted file")
                    return self._api_key
        except ImportError:
            logger.debug("cryptography module not available, skipping encrypted file option")
        except Exception as e:
            logger.error(f"Failed to decrypt API key: {e}")
        
        raise APIKeyError("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    def rate_limit(self, model: str = 'gpt-4o'):
        """API í˜¸ì¶œ ì†ë„ ì œí•œ ë°ì½”ë ˆì´í„°
        
        ì´ ë°ì½”ë ˆì´í„°ëŠ” OpenAI APIì˜ ì†ë„ ì œí•œì„ ì¤€ìˆ˜í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
        ë„ˆë¬´ ë¹ ë¥¸ ì—°ì† í˜¸ì¶œì„ ë°©ì§€í•˜ì—¬ API ì˜¤ë¥˜ë¥¼ ì˜ˆë°©í•©ë‹ˆë‹¤.
        """
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                current_time = time.time()
                model_key = f"{model}_last_call"
                
                # ë§ˆì§€ë§‰ í˜¸ì¶œ ì‹œê°„ í™•ì¸
                if model_key in self._last_call_time:
                    elapsed = current_time - self._last_call_time[model_key]
                    min_interval = 60.0 / self._rate_limits[model]['calls_per_minute']
                    
                    if elapsed < min_interval:
                        sleep_time = min_interval - elapsed
                        logger.debug(f"Rate limiting: sleeping for {sleep_time:.2f}s")
                        time.sleep(sleep_time)
                
                # í˜¸ì¶œ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # ë§ˆì§€ë§‰ í˜¸ì¶œ ì‹œê°„ ì—…ë°ì´íŠ¸
                self._last_call_time[model_key] = time.time()
                
                # í˜¸ì¶œ íšŸìˆ˜ ì¶”ì 
                self._track_usage(model, args, kwargs, result)
                
                return result
            return wrapper
        return decorator
    
    def _track_usage(self, model: str, args, kwargs, result):
        """API ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ì¶”ì """
        try:
            # í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œë¡œëŠ” tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥)
            if hasattr(result, 'usage'):
                input_tokens = result.usage.prompt_tokens
                output_tokens = result.usage.completion_tokens
                
                # ë¹„ìš© ê³„ì‚°
                input_cost = (input_tokens / 1000) * self._model_costs[model]['input']
                output_cost = (output_tokens / 1000) * self._model_costs[model]['output']
                total_cost = input_cost + output_cost
                
                # ë¹„ìš© ì¶”ì 
                self._cost_tracker[model] += total_cost
                self._cost_tracker['total'] += total_cost
                
                logger.debug(f"API call to {model}: {input_tokens} input + {output_tokens} output tokens = ${total_cost:.4f}")
        except Exception as e:
            logger.warning(f"Failed to track usage: {e}")
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ ì‚¬ìš© í†µê³„ ë°˜í™˜"""
        return {
            'costs': dict(self._cost_tracker),
            'last_calls': dict(self._last_call_time),
            'estimated_monthly_cost': self._cost_tracker['total'] * 30  # ëŒ€ëµì ì¸ ì›” ë¹„ìš© ì¶”ì •
        }

# ===== ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë¡œë” =====
class ChunkLoader:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë¡œë”© ì‹œìŠ¤í…œ
    
    ì´ í´ë˜ìŠ¤ëŠ” ëŒ€ìš©ëŸ‰ JSON íŒŒì¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ijsonì´ ìˆìœ¼ë©´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì„, ì—†ìœ¼ë©´ ì¼ë°˜ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, filepath: str):
        self.filepath = filepath
        self._chunks = None
        self._chunk_cache = OrderedDict()
        self._cache_size = 1000
        self._use_streaming = False
        
        # ijson ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        try:
            import ijson
            self._use_streaming = True
            logger.info("Using streaming JSON parser (ijson)")
        except ImportError:
            self._use_streaming = False
            logger.info("ijson not available, using standard JSON loading")
        
        self._initialize()
    
    def _initialize(self):
        """ì´ˆê¸°í™” - ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ ë°©ì‹ ì„ íƒ"""
        if self._use_streaming:
            try:
                self._build_streaming_index()
            except Exception as e:
                logger.warning(f"Streaming index failed: {e}, falling back to standard loading")
                self._use_streaming = False
                self._load_all_chunks()
        else:
            self._load_all_chunks()
    
    def _load_all_chunks(self):
        """ì „ì²´ ì²­í¬ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ (í´ë°± ë°©ì‹)"""
        logger.info(f"Loading all chunks from {self.filepath}")
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                self._chunks = json.load(f)
            logger.info(f"Loaded {len(self._chunks)} chunks into memory")
        except Exception as e:
            logger.error(f"Failed to load chunks: {e}")
            self._chunks = []
    
    def _build_streaming_index(self):
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¸ë±ìŠ¤ êµ¬ì¶• (ijson í•„ìš”)"""
        import ijson
        logger.info(f"Building streaming index for {self.filepath}")
        self._index = {}
        
        with open(self.filepath, 'rb') as f:
            parser = ijson.items(f, 'item')
            for idx, item in enumerate(parser):
                # ì¸ë±ìŠ¤ë§Œ ì €ì¥í•˜ê³  ì‹¤ì œ ë°ì´í„°ëŠ” ë‚˜ì¤‘ì— ë¡œë“œ
                self._index[idx] = idx
        
        logger.info(f"Streaming index built: {len(self._index)} chunks found")
    
    def get_chunk(self, idx: int) -> Dict:
        """íŠ¹ì • ì¸ë±ìŠ¤ì˜ ì²­í¬ë¥¼ ê°€ì ¸ì˜´"""
        # ìºì‹œ í™•ì¸
        if idx in self._chunk_cache:
            self._chunk_cache.move_to_end(idx)
            return self._chunk_cache[idx]
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì´ ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ë°˜í™˜
        if not self._use_streaming:
            if self._chunks and 0 <= idx < len(self._chunks):
                chunk = self._chunks[idx]
                self._add_to_cache(idx, chunk)
                return chunk
            else:
                raise IndexError(f"Chunk index {idx} out of range")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŠ¹ì • ì²­í¬ ë¡œë“œ
        try:
            import ijson
            with open(self.filepath, 'rb') as f:
                parser = ijson.items(f, 'item')
                for i, item in enumerate(parser):
                    if i == idx:
                        self._add_to_cache(idx, item)
                        return item
            raise IndexError(f"Chunk index {idx} not found")
        except Exception as e:
            logger.error(f"Failed to load chunk {idx}: {e}")
            # í´ë°±: ì „ì²´ ë¡œë“œ ì‹œë„
            if not self._chunks:
                self._load_all_chunks()
            if self._chunks and 0 <= idx < len(self._chunks):
                return self._chunks[idx]
            raise
    
    def _add_to_cache(self, idx: int, chunk: Dict):
        """ìºì‹œì— ì²­í¬ ì¶”ê°€ (LRU ì •ì±…)"""
        if len(self._chunk_cache) >= self._cache_size:
            self._chunk_cache.popitem(last=False)
        self._chunk_cache[idx] = chunk
    
    def iter_chunks(self, indices: List[int] = None) -> Iterator[Dict]:
        """í•„ìš”í•œ ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜"""
        if not self._use_streaming and self._chunks:
            # ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ê²½ìš°
            if indices is None:
                indices = range(len(self._chunks))
            for idx in indices:
                if 0 <= idx < len(self._chunks):
                    yield self._chunks[idx]
        else:
            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹
            if indices is None:
                indices = range(self.get_total_chunks())
            for idx in indices:
                yield self.get_chunk(idx)
    
    def get_total_chunks(self) -> int:
        """ì „ì²´ ì²­í¬ ìˆ˜ ë°˜í™˜"""
        if not self._use_streaming and self._chunks:
            return len(self._chunks)
        elif hasattr(self, '_index'):
            return len(self._index)
        else:
            # ì¹´ìš´íŠ¸ë¥¼ ìœ„í•´ í•œ ë²ˆ ìŠ¤ìº”
            if not self._chunks:
                self._load_all_chunks()
            return len(self._chunks) if self._chunks else 0

# ===== íƒ€ì… ì •ì˜ =====
class ModelSelection(Enum):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤"""
    GPT4O = "gpt-4o"
    GPT4O_MINI = "gpt-4o-mini"
    O4_MINI = "o4-mini"  # ì¶”ë¡  íŠ¹í™” ëª¨ë¸

class ChunkDict(TypedDict):
    """ì²­í¬ ë°ì´í„°ì˜ íƒ€ì… ì •ì˜"""
    chunk_id: str
    content: str
    source: str
    page: int
    chunk_type: str
    metadata: str

class AnalysisResult(TypedDict):
    """GPT ë¶„ì„ ê²°ê³¼ì˜ íƒ€ì… ì •ì˜"""
    query_analysis: dict
    legal_concepts: list
    search_strategy: dict
    answer_requirements: dict
    model_used: str  # ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í–ˆëŠ”ì§€ ê¸°ë¡

# ===== ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')
    
    @property
    def is_latest(self) -> bool:
        """ìµœì‹  ìë£Œ ì—¬ë¶€ í™•ì¸"""
        return self.metadata.get('is_latest', False)

@dataclass
class SearchError:
    """ê²€ìƒ‰ ì¤‘ ë°œìƒí•œ ì—ëŸ¬ ì •ë³´"""
    error_type: str
    message: str
    timestamp: float
    context: Dict[str, Any]
    severity: str  # 'critical', 'warning', 'info'

class QueryComplexity(Enum):
    """ì§ˆë¬¸ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"

# ===== í•˜ì´ë¸Œë¦¬ë“œ GPT ì „ëµ =====
class HybridGPTStrategy:
    """ì‘ì—… ë³µì¡ë„ì— ë”°ë¼ ì ì ˆí•œ GPT ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ì „ëµ
    
    ì´ í´ë˜ìŠ¤ëŠ” ê° ì‘ì—…ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
    ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.
    """
    
    def __init__(self, api_manager: SecureAPIManager):
        self.api_manager = api_manager
        
        # ëª¨ë¸ë³„ íŠ¹ì„± ì •ì˜
        self.model_characteristics = {
            ModelSelection.GPT4O: {
                "strength": "ë²”ìš©ì„±, ê¸´ ì»¨í…ìŠ¤íŠ¸, ì°½ì˜ì„±",
                "weakness": "ë†’ì€ ë¹„ìš©, ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼",
                "best_for": ["ê¸´ ë¬¸ì„œ ìš”ì•½", "ë³µì¡í•œ ì„¤ëª…", "ì°½ì˜ì  ë‹µë³€"]
            },
            ModelSelection.GPT4O_MINI: {
                "strength": "ë¹ ë¥¸ ì†ë„, ë‚®ì€ ë¹„ìš©, ë‹¨ìˆœ ì‘ì—…ì— ì¶©ë¶„",
                "weakness": "ì œí•œëœ ì»¨í…ìŠ¤íŠ¸, ë³µì¡í•œ ì¶”ë¡  ì•½í•¨",
                "best_for": ["ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸", "ì§§ì€ ë‹µë³€", "ë¶„ë¥˜ ì‘ì—…"]
            },
            ModelSelection.O4_MINI: {
                "strength": "ë›°ì–´ë‚œ ì¶”ë¡  ëŠ¥ë ¥, ë…¼ë¦¬ì  ë¶„ì„",
                "weakness": "ì°½ì˜ì„± ë¶€ì¡±, ì†ë„ ë³´í†µ",
                "best_for": ["ë³µì¡í•œ ë…¼ë¦¬ ë¶„ì„", "ë‹¤ë‹¨ê³„ ì¶”ë¡ ", "ì¡°ê±´ë¶€ íŒë‹¨"]
            }
        }
    
    def select_model_for_analysis(self, query: str, complexity: QueryComplexity) -> ModelSelection:
        """ì§ˆë¬¸ ë¶„ì„ì„ ìœ„í•œ ëª¨ë¸ ì„ íƒ
        
        ì§ˆë¬¸ì˜ ë³µì¡ë„ì™€ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
        """
        query_lower = query.lower()
        
        # ì¶”ë¡ ì´ ë§ì´ í•„ìš”í•œ íŒ¨í„´ ê°ì§€
        reasoning_patterns = [
            r'ë§Œ[ì•½ì¼].*ê²½ìš°',
            r'ë™ì‹œì—.*ê·¸ë¦¬ê³ ',
            r'[AB].*ë©´ì„œ.*[CD]',
            r'ê°ê°.*ì–´ë–»ê²Œ',
            r'ì¢…í•©ì ìœ¼ë¡œ',
            r'ì—¬ëŸ¬.*ê³ ë ¤'
        ]
        
        needs_reasoning = any(re.search(pattern, query_lower) for pattern in reasoning_patterns)
        
        # ë‹¨ìˆœ ì •ë³´ ìš”ì²­ íŒ¨í„´
        simple_patterns = [
            r'^.*ê¸°í•œ[ì€ì´]',
            r'^.*ê¸ˆì•¡[ì€ì´]',
            r'^.*ë‚ ì§œ[ëŠ”ê°€]',
            r'^ì–¸ì œ'
        ]
        
        is_simple = any(re.search(pattern, query_lower) for pattern in simple_patterns)
        
        # ëª¨ë¸ ì„ íƒ ë¡œì§
        if needs_reasoning and complexity in [QueryComplexity.MEDIUM, QueryComplexity.COMPLEX]:
            logger.info(f"Selected O4_MINI for reasoning-intensive analysis")
            return ModelSelection.O4_MINI
        elif is_simple or complexity == QueryComplexity.SIMPLE:
            logger.info(f"Selected GPT4O_MINI for simple analysis")
            return ModelSelection.GPT4O_MINI
        else:
            logger.info(f"Selected GPT4O for general analysis")
            return ModelSelection.GPT4O
    
    def select_model_for_answer(self, 
                               query: str, 
                               search_results: List[SearchResult],
                               analysis: Dict) -> ModelSelection:
        """ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ëª¨ë¸ ì„ íƒ
        
        ì§ˆë¬¸ì˜ íŠ¹ì„±ê³¼ ê²€ìƒ‰ ê²°ê³¼ì˜ ë³µì¡ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì„ íƒí•©ë‹ˆë‹¤.
        """
        # ë¶„ì„ ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        requirements = analysis.get('answer_requirements', {})
        complexity = analysis.get('query_analysis', {}).get('actual_complexity', 'medium')
        
        # ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸ì¸ ê²½ìš°
        if (requirements.get('needs_specific_numbers') and 
            not requirements.get('needs_multiple_perspectives') and
            complexity == 'simple'):
            return ModelSelection.GPT4O_MINI
        
        # ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°
        if (requirements.get('needs_multiple_perspectives') or
            requirements.get('needs_exceptions') or
            'ì¢…í•©' in query or 'ë¦¬ìŠ¤í¬' in query):
            return ModelSelection.O4_MINI
        
        # ì¼ë°˜ì ì¸ ê²½ìš°
        return ModelSelection.GPT4O
    
    def estimate_cost(self, model: ModelSelection, estimated_tokens: int) -> float:
        """ì˜ˆìƒ ë¹„ìš© ê³„ì‚°"""
        costs = self.api_manager._model_costs[model.value]
        # ì…ë ¥ê³¼ ì¶œë ¥ì„ ëŒ€ëµ 7:3ìœ¼ë¡œ ê°€ì •
        input_cost = (estimated_tokens * 0.7 / 1000) * costs['input']
        output_cost = (estimated_tokens * 0.3 / 1000) * costs['output']
        return input_cost + output_cost

# ===== í–¥ìƒëœ ì§ˆë¬¸ ë¶„ì„ê¸° =====
class EnhancedQueryAnalyzer:
    """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì „ëµì„ ì‚¬ìš©í•˜ëŠ” ì§ˆë¬¸ ë¶„ì„ê¸°
    
    ì´ í´ë˜ìŠ¤ëŠ” o4-miniì˜ ì¶”ë¡  ëŠ¥ë ¥ê³¼ GPT-4oì˜ ë²”ìš©ì„±ì„ 
    ì ì ˆíˆ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, api_manager: SecureAPIManager, hybrid_strategy: HybridGPTStrategy):
        self.api_manager = api_manager
        self.hybrid_strategy = hybrid_strategy
        self.analysis_cache = LRUCache(max_size=100, ttl=3600)
    
    @api_manager.rate_limit('gpt-4o')  # ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” ë™ì ìœ¼ë¡œ ë³€ê²½ë¨
    async def analyze_query(self, query: str, available_chunks_info: Dict) -> AnalysisResult:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
        
        ë¨¼ì € ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•œ í›„, ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•˜ì—¬
        ìƒì„¸í•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(f"{query}_{json.dumps(available_chunks_info)}".encode()).hexdigest()
        cached = self.analysis_cache.get(cache_key)
        if cached:
            logger.info("Using cached analysis")
            return cached
        
        # 1ë‹¨ê³„: ë³µì¡ë„ í‰ê°€
        complexity_assessor = ComplexityAssessor()
        complexity, confidence, _ = complexity_assessor.assess(query)
        
        # 2ë‹¨ê³„: ëª¨ë¸ ì„ íƒ
        selected_model = self.hybrid_strategy.select_model_for_analysis(query, complexity)
        
        # 3ë‹¨ê³„: ì„ íƒëœ ëª¨ë¸ë¡œ ë¶„ì„ ìˆ˜í–‰
        if selected_model == ModelSelection.O4_MINI:
            analysis = await self._analyze_with_reasoning_model(query, available_chunks_info, complexity)
        else:
            analysis = await self._analyze_with_standard_model(query, available_chunks_info, complexity, selected_model)
        
        # 4ë‹¨ê³„: ì‚¬ìš©ëœ ëª¨ë¸ ê¸°ë¡
        analysis['model_used'] = selected_model.value
        analysis['model_selection_reason'] = f"Complexity: {complexity.value}, Confidence: {confidence:.2f}"
        
        # ìºì‹œ ì €ì¥
        self.analysis_cache.put(cache_key, analysis)
        
        return analysis
    
    async def _analyze_with_reasoning_model(self, query: str, chunks_info: Dict, complexity: QueryComplexity) -> AnalysisResult:
        """o4-minië¥¼ ì‚¬ìš©í•œ ì¶”ë¡  ì¤‘ì‹¬ ë¶„ì„
        
        ì¶”ë¡  ëª¨ë¸ì˜ ê°•ì ì„ í™œìš©í•˜ì—¬ ë³µì¡í•œ ì§ˆë¬¸ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
        """
        prompt = f"""
        ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ:
        - ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼: {chunks_info.get('ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 0)}ê°œ ì„¹ì…˜
        - í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼: {chunks_info.get('í˜„í™©ê³µì‹œ', 0)}ê°œ ì„¹ì…˜
        - ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼: {chunks_info.get('ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­', 0)}ê°œ ì„¹ì…˜
        
        ë¶„ì„ ê³¼ì •:
        
        1. ì§ˆë¬¸ êµ¬ì¡° ë¶„í•´
           - í•µì‹¬ ì£¼ì²´ë“¤ ì‹ë³„ (íšŒì‚¬, ê³„ì—´ì‚¬ ë“±)
           - ê±°ë˜ ìœ í˜• íŒŒì•…
           - ì‹œê°„ì  ìš”ì†Œ í™•ì¸
        
        2. ë²•ì  ìŸì  ë„ì¶œ
           - ê° ê±°ë˜/í–‰ìœ„ë³„ ì ìš© ë²•ê·œ
           - í•„ìˆ˜ í™•ì¸ ì‚¬í•­
           - ì ì¬ì  ë¦¬ìŠ¤í¬
        
        3. ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
           - ìš°ì„  ê²€ìƒ‰í•  ë§¤ë‰´ì–¼ê³¼ ì´ìœ 
           - í•µì‹¬ í‚¤ì›Œë“œ ë„ì¶œ ê³¼ì •
           - í•„ìš”í•œ ì •ë³´ì˜ ê¹Šì´
        
        ê° ë‹¨ê³„ì—ì„œ "ì™œ" ê·¸ëŸ° íŒë‹¨ì„ í–ˆëŠ”ì§€ ëª…í™•íˆ ì„¤ëª…í•˜ê³ ,
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        
        {{
            "query_analysis": {{
                "core_intent": "í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œ ì§ˆë¬¸ì˜ í•µì‹¬",
                "actual_complexity": "{complexity.value}",
                "complexity_reason": "ë³µì¡ë„ íŒë‹¨ì˜ êµ¬ì²´ì  ê·¼ê±°",
                "reasoning_chain": ["ì¶”ë¡  ë‹¨ê³„ 1", "ì¶”ë¡  ë‹¨ê³„ 2", ...]
            }},
            "legal_concepts": [
                {{
                    "concept": "ê´€ë ¨ ë²•ë¥  ê°œë…",
                    "relevance": "primary/secondary",
                    "specific_aspects": ["êµ¬ì²´ì  ê²€í†  ì‚¬í•­ë“¤"],
                    "why_relevant": "ì´ ê°œë…ì´ ì¤‘ìš”í•œ ì´ìœ "
                }}
            ],
            "search_strategy": {{
                "approach": "direct_lookup/focused_search/comprehensive_analysis",
                "primary_manual": "ì£¼ ê²€ìƒ‰ ëŒ€ìƒ ë§¤ë‰´ì–¼",
                "search_keywords": ["ë„ì¶œëœ í‚¤ì›Œë“œë“¤"],
                "keyword_derivation": "í‚¤ì›Œë“œ ë„ì¶œ ê³¼ì • ì„¤ëª…",
                "expected_chunks_needed": ìˆ«ì,
                "rationale": "ì´ ì „ëµì˜ ë…¼ë¦¬ì  ê·¼ê±°"
            }},
            "answer_requirements": {{
                "needs_specific_numbers": true/false,
                "needs_process_steps": true/false,
                "needs_timeline": true/false,
                "needs_exceptions": true/false,
                "needs_multiple_perspectives": true/false,
                "critical_points": ["ë‹µë³€ì— ê¼­ í¬í•¨ë˜ì–´ì•¼ í•  ìš”ì†Œë“¤"]
            }}
        }}
        """
        
        try:
            response = await openai.chat.completions.create(
                model="o4-mini",  # ì¶”ë¡  íŠ¹í™” ëª¨ë¸ ì‚¬ìš©
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,  # ì¶”ë¡  ëª¨ë¸ì€ ë‚®ì€ temperatureê°€ íš¨ê³¼ì 
                max_tokens=2000,
                response_format={"type": "json_object"}
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"o4-mini analysis failed: {e}")
            # í´ë°±ìœ¼ë¡œ GPT-4o ì‚¬ìš©
            return await self._analyze_with_standard_model(query, chunks_info, complexity, ModelSelection.GPT4O)
    
    async def _analyze_with_standard_model(self, query: str, chunks_info: Dict, 
                                         complexity: QueryComplexity, model: ModelSelection) -> AnalysisResult:
        """í‘œì¤€ GPT ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¶„ì„"""
        prompt = f"""
        ê³µì •ê±°ë˜ë²• ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.
        
        ì§ˆë¬¸: {query}
        ë³µì¡ë„: {complexity.value}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ:
        {json.dumps(chunks_info, ensure_ascii=False)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "query_analysis": {{
                "core_intent": "ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„",
                "actual_complexity": "{complexity.value}",
                "complexity_reason": "ë³µì¡ë„ íŒë‹¨ ì´ìœ "
            }},
            "legal_concepts": [
                {{
                    "concept": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜/í˜„í™©ê³µì‹œ/ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­",
                    "relevance": "primary/secondary",
                    "specific_aspects": ["ê´€ë ¨ ì¸¡ë©´ë“¤"]
                }}
            ],
            "search_strategy": {{
                "approach": "direct_lookup/focused_search/comprehensive_analysis",
                "primary_manual": "ì£¼ ê²€ìƒ‰ ë§¤ë‰´ì–¼",
                "search_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
                "expected_chunks_needed": 10,
                "rationale": "ì „ëµ ì„ íƒ ì´ìœ "
            }},
            "answer_requirements": {{
                "needs_specific_numbers": true/false,
                "needs_process_steps": true/false,
                "needs_timeline": true/false,
                "needs_exceptions": true/false,
                "needs_multiple_perspectives": true/false
            }}
        }}
        """
        
        response = await openai.chat.completions.create(
            model=model.value,
            messages=[{"role": "user", "content": prompt}],
            temperature=0 if model == ModelSelection.GPT4O_MINI else 0.3,
            max_tokens=1000,
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)

# ===== ë³µì¡ë„ í‰ê°€ê¸° =====
class ComplexityAssessor:
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •"""
    
    def __init__(self):
        self.simple_indicators = [
            r'ì–¸ì œ', r'ë©°ì¹ ', r'ê¸°í•œ', r'ë‚ ì§œ', r'ê¸ˆì•¡', r'%', r'ì–¼ë§ˆ',
            r'ì •ì˜[ê°€ëŠ”]?', r'ë¬´ì—‡', r'ëœ»[ì´ì€]?', r'ì˜ë¯¸[ê°€ëŠ”]?'
        ]
        
        self.complex_indicators = [
            r'ë™ì‹œì—', r'ì—¬ëŸ¬', r'ë³µí•©', r'ì—°ê´€', r'ì˜í–¥',
            r'ë§Œ[ì•½ì¼].*ê²½ìš°', r'[AB].*ë™ì‹œ.*[CD]', r'ê±°ë˜.*ì—¬ëŸ¬',
            r'ì „ì²´ì ', r'ì¢…í•©ì ', r'ë¶„ì„', r'ê²€í† ', r'í‰ê°€',
            r'ë¦¬ìŠ¤í¬', r'ìœ„í—˜', r'ëŒ€ì‘', r'ì „ëµ'
        ]
        
        self.medium_indicators = [
            r'ì–´ë–»ê²Œ', r'ë°©ë²•', r'ì ˆì°¨', r'ê³¼ì •',
            r'ì£¼ì˜', r'ì˜ˆì™¸', r'íŠ¹ë³„', r'ê³ ë ¤'
        ]
    
    def assess(self, query: str) -> Tuple[QueryComplexity, float, Dict]:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ê³  ê´€ë ¨ ì •ë³´ ë°˜í™˜"""
        query_lower = query.lower()
        
        simple_score = sum(1 for pattern in self.simple_indicators 
                         if re.search(pattern, query_lower))
        complex_score = sum(2 for pattern in self.complex_indicators 
                          if re.search(pattern, query_lower))
        medium_score = sum(1.5 for pattern in self.medium_indicators 
                         if re.search(pattern, query_lower))
        
        # ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        if len(query) > 150:
            complex_score += 2
        elif len(query) > 100:
            complex_score += 1
        elif len(query) < 30:
            simple_score += 0.5
            
        # ë³µìˆ˜ ì§ˆë¬¸ ì—¬ë¶€
        if query.count('?') > 1 or re.search(r'ê·¸ë¦¬ê³ .*[?]', query_lower):
            complex_score += 1.5
            
        total_score = simple_score + medium_score + complex_score
        
        # ë³µì¡ë„ ê²°ì •
        if total_score == 0:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.5
        elif complex_score > simple_score * 2:
            complexity = QueryComplexity.COMPLEX
            confidence = min(complex_score / (total_score + 1), 0.9)
        elif simple_score > complex_score * 2:
            complexity = QueryComplexity.SIMPLE
            confidence = min(simple_score / (total_score + 1), 0.9)
        else:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.6
            
        analysis = {
            'simple_score': simple_score,
            'medium_score': medium_score,
            'complex_score': complex_score,
            'query_length': len(query),
            'confidence': confidence
        }
        
        return complexity, confidence, analysis

# ===== LRU ìºì‹œ êµ¬í˜„ =====
class LRUCache:
    """ì‹œê°„ ê¸°ë°˜ ë§Œë£Œë¥¼ ì§€ì›í•˜ëŠ” LRU ìºì‹œ êµ¬í˜„"""
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl
        
    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ë§Œë£Œëœ í•­ëª©ì€ ì œê±°"""
        if key not in self.cache:
            return None
            
        value, timestamp = self.cache[key]
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
            
        # LRU: ìµœê·¼ ì‚¬ìš© í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™
        self.cache.move_to_end(key)
        return value
        
    def put(self, key: str, value):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if key in self.cache:
            del self.cache[key]
            
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            self.cache.popitem(last=False)
            
        self.cache[key] = (value, time.time())
        
    def clear_expired(self):
        """ë§Œë£Œëœ ëª¨ë“  í•­ëª© ì œê±°"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]

# ===== ê²€ìƒ‰ ì „ëµ ì¸í„°í˜ì´ìŠ¤ =====
class SearchStrategy(Protocol):
    """ê²€ìƒ‰ ì „ëµì˜ ì¸í„°í˜ì´ìŠ¤"""
    def prepare_indices(self, manual: str, limit: int) -> List[int]:
        """ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ì¤€ë¹„"""
        ...
    
    def enhance_query(self, query: str, keywords: List[str]) -> str:
        """ì¿¼ë¦¬ í–¥ìƒ"""
        ...
    
    def filter_results(self, results: List[SearchResult], requirements: Dict) -> List[SearchResult]:
        """ê²°ê³¼ í•„í„°ë§"""
        ...

# ===== ê¸°ë³¸ ê²€ìƒ‰ ì‹¤í–‰ê¸° =====
class BaseSearchExecutor:
    """ê³µí†µ ê²€ìƒ‰ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ê¸°ë³¸ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ëª¨ë“  ê²€ìƒ‰ ì „ëµì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
    ì½”ë“œ ì¤‘ë³µì„ ì¤„ì´ê³  ì¼ê´€ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, index: faiss.Index, chunk_loader: ChunkLoader, embedding_model):
        self.index = index
        self.chunk_loader = chunk_loader
        self.embedding_model = embedding_model
        self._embedding_cache = {}
    
    async def execute_search(self,
                           query: str,
                           indices: List[int],
                           top_k: int,
                           strategy: SearchStrategy) -> Tuple[List[SearchResult], Dict]:
        """ê³µí†µ ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§"""
        start_time = time.time()
        stats = {
            'search_method': strategy.__class__.__name__,
            'errors': []
        }
        
        try:
            # 1. ì¸ë±ìŠ¤ ê²€ì¦
            if not indices:
                raise ValueError("No indices provided for search")
            
            # 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ìºì‹œ í™œìš©)
            query_vector = self._get_cached_embedding(query)
            
            # 3. FAISS ê²€ìƒ‰ ì‹¤í–‰
            k_search = min(len(indices), max(1, top_k * 3))
            scores, search_indices = self._safe_faiss_search(query_vector, k_search)
            
            # 4. ê²°ê³¼ ë³€í™˜
            results = await self._convert_to_search_results(
                scores[0], search_indices[0], set(indices)
            )
            
            # 5. ì „ëµë³„ í›„ì²˜ë¦¬
            if hasattr(strategy, 'filter_results'):
                results = strategy.filter_results(results, {})
            
            # 6. ì •ë ¬ ë° ì œí•œ
            results.sort(key=lambda x: x.score, reverse=True)
            results = results[:top_k]
            
            stats.update({
                'search_time': time.time() - start_time,
                'searched_chunks': len(indices),
                'results_count': len(results),
                'cache_hit': query in self._embedding_cache
            })
            
            return results, stats
            
        except Exception as e:
            logger.error(f"Search execution error: {str(e)}")
            stats['errors'].append({
                'type': type(e).__name__,
                'message': str(e)
            })
            stats['search_time'] = time.time() - start_time
            return [], stats
    
    def _get_cached_embedding(self, query: str) -> np.ndarray:
        """ìºì‹œëœ ì„ë² ë”© ë˜ëŠ” ìƒˆë¡œ ìƒì„±"""
        if query not in self._embedding_cache:
            embedding = self.embedding_model.encode([query])
            self._embedding_cache[query] = np.array(embedding, dtype=np.float32)
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self._embedding_cache) > 1000:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self._embedding_cache))
                del self._embedding_cache[oldest_key]
        
        return self._embedding_cache[query]
    
    def _safe_faiss_search(self, query_vector: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """ì•ˆì „í•œ FAISS ê²€ìƒ‰"""
        try:
            return self.index.search(query_vector, k)
        except Exception as e:
            logger.error(f"FAISS search failed: {e}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return np.array([[0.0]]), np.array([[-1]])
    
    async def _convert_to_search_results(self, 
                                       scores: np.ndarray, 
                                       indices: np.ndarray,
                                       valid_indices: set) -> List[SearchResult]:
        """ì¸ë±ìŠ¤ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
        
        ChunkLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•œ ì²­í¬ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
        """
        results = []
        
        for idx, score in zip(indices, scores):
            if idx < 0 or idx >= self.chunk_loader.get_total_chunks():
                continue
            
            if idx not in valid_indices:
                continue
            
            try:
                # ChunkLoaderë¥¼ í†µí•´ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunk = self.chunk_loader.get_chunk(idx)
                result = SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk.get('content', ''),
                    score=float(score),
                    source=chunk.get('source', 'Unknown'),
                    page=chunk.get('page', 0),
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                )
                results.append(result)
            except Exception as e:
                logger.warning(f"Failed to create SearchResult for index {idx}: {e}")
                continue
        
        return results

# ===== ë¬¸ì„œ ë²„ì „ ê´€ë¦¬ =====
class DocumentVersionManager:
    """ë¬¸ì„œì˜ ë²„ì „ê³¼ ìµœì‹ ì„±ì„ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.regulation_changes = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜_ê¸ˆì•¡ê¸°ì¤€': [
                {'date': '2023-01-01', 'old_value': '50ì–µì›', 'new_value': '100ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 100ì–µì› ì´ìƒ'},
                {'date': '2020-01-01', 'old_value': '30ì–µì›', 'new_value': '50ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 50ì–µì› ì´ìƒ'}
            ],
            'ê³µì‹œ_ê¸°í•œ': [
                # í˜„í–‰ ê·œì •: ì˜ì—…ì¼ 7ì¼
                {'date': '2019-01-01', 'old_value': '7ì¼', 'new_value': 'ì˜ì—…ì¼ 7ì¼',
                 'description': 'ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œ (ì˜ì—…ì¼ ê¸°ì¤€ìœ¼ë¡œ ëª…í™•í™”)'}
            ]
        }
        
        self.critical_patterns = {
            'ê¸ˆì•¡': r'(\d+)ì–µ\s*ì›',
            'ë¹„ìœ¨': r'(\d+(?:\.\d+)?)\s*%',
            'ê¸°í•œ': r'(\d+)\s*ì¼',
            'ë‚ ì§œ': r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        }
    
    def extract_document_date(self, chunk: Dict) -> Optional[str]:
        """ë¬¸ì„œì—ì„œ ì‘ì„±/ê°œì • ë‚ ì§œ ì¶”ì¶œ"""
        content = chunk.get('content', '')
        metadata = json.loads(chunk.get('metadata', '{}'))
        
        # ë©”íƒ€ë°ì´í„°ì— ë‚ ì§œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if 'document_date' in metadata:
            return metadata['document_date']
        
        # ë‚´ìš©ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*ê°œì •',
            r'ì‹œí–‰ì¼\s*:\s*(\d{4})ë…„\s*(\d{1,2})ì›”',
            r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                return self._normalize_date(match.group(0))
        
        return None
    
    def _normalize_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # ìˆ«ìë§Œ ì¶”ì¶œ
        numbers = re.findall(r'\d+', date_str)
        if len(numbers) >= 2:
            year = numbers[0] if len(numbers[0]) == 4 else '20' + numbers[0]
            month = numbers[1].zfill(2)
            day = numbers[2].zfill(2) if len(numbers) > 2 else '01'
            return f"{year}-{month}-{day}"
        return None
    
    def check_for_outdated_info(self, content: str, document_date: str = None) -> List[Dict]:
        """êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        warnings = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ í™•ì¸
        amount_match = re.search(r'(\d+)ì–µ\s*ì›.*ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', content)
        if amount_match:
            amount = int(amount_match.group(1))
            if amount == 50:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '50ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
            elif amount == 30:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '30ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
        
        # ê³µì‹œ ê¸°í•œ í™•ì¸ - ì˜ì—…ì¼ ëª…ì‹œ ì—¬ë¶€ ì²´í¬
        deadline_match = re.search(r'ì˜ê²°.*?(\d+)\s*ì¼.*?ê³µì‹œ', content)
        if deadline_match:
            # "ì˜ì—…ì¼" ëª…ì‹œ ì—¬ë¶€ í™•ì¸
            context = content[max(0, deadline_match.start()-20):deadline_match.end()+20]
            if 'ì˜ì—…ì¼' not in context and 'ì˜ì—…ì¼' not in content[max(0, deadline_match.start()-50):deadline_match.end()+50]:
                # ì˜ì—…ì¼ì´ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° ê²½ê³ 
                warnings.append({
                    'type': 'unclear_deadline',
                    'found': f'{deadline_match.group(1)}ì¼',
                    'current': 'ì˜ì—…ì¼ 7ì¼',
                    'regulation': 'ê³µì‹œ ê¸°í•œ',
                    'changed_date': '2019-01-01',
                    'severity': 'warning',
                    'note': 'ì˜ì—…ì¼ ê¸°ì¤€ì„ì„ ëª…í™•íˆ í•´ì•¼ í•¨'
                })
        
        return warnings

# ===== ì¶©ëŒ í•´ê²° ì‹œìŠ¤í…œ =====
class ConflictResolver:
    """ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, version_manager: DocumentVersionManager):
        self.version_manager = version_manager
    
    def resolve_conflicts(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        
        # ê° ê²°ê³¼ì˜ ë‚ ì§œì™€ êµ¬ë²„ì „ ì •ë³´ í™•ì¸
        for result in results:
            doc_date = result.document_date
            warnings = self.version_manager.check_for_outdated_info(result.content, doc_date)
            
            if warnings:
                result.metadata['warnings'] = warnings
                result.metadata['has_outdated_info'] = True
            else:
                result.metadata['has_outdated_info'] = False
        
        # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ ë° ì¶©ëŒ í™•ì¸
        critical_info = self._extract_critical_info(results, query)
        if critical_info:
            conflicts = self._find_conflicts(critical_info)
            if conflicts:
                results = self._prioritize_latest_info(results, conflicts)
        
        # ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda r: (
            not r.metadata.get('has_outdated_info', False),  # ìµœì‹  ì •ë³´ ìš°ì„ 
            r.document_date or '1900-01-01',  # ìµœì‹  ë‚ ì§œ ìš°ì„ 
            r.score  # ê´€ë ¨ë„ ì ìˆ˜
        ), reverse=True)
        
        return results
    
    def _extract_critical_info(self, results: List[SearchResult], query: str) -> Dict:
        """ê²°ê³¼ì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ"""
        critical_info = defaultdict(list)
        
        for i, result in enumerate(results):
            # ê¸ˆì•¡ ì •ë³´
            amounts = re.findall(r'(\d+)ì–µ\s*ì›', result.content)
            for amount in amounts:
                critical_info['amounts'].append({
                    'value': amount + 'ì–µì›',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            # ë¹„ìœ¨ ì •ë³´
            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', result.content)
            for pct in percentages:
                critical_info['percentages'].append({
                    'value': pct + '%',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            # ê¸°í•œ ì •ë³´
            deadlines = re.findall(r'(\d+)\s*ì¼', result.content)
            for deadline in deadlines:
                critical_info['deadlines'].append({
                    'value': deadline + 'ì¼',
                    'result_index': i,
                    'context': result.content[:100]
                })
        
        return dict(critical_info)
    
    def _find_conflicts(self, critical_info: Dict) -> List[Dict]:
        """ì¤‘ìš” ì •ë³´ ê°„ ì¶©ëŒ ì°¾ê¸°"""
        conflicts = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ì¶©ëŒ í™•ì¸
        if 'amounts' in critical_info:
            amount_values = set()
            for item in critical_info['amounts']:
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in item['context']:
                    amount_values.add(item['value'])
            
            if len(amount_values) > 1 and ('50ì–µì›' in amount_values or '30ì–µì›' in amount_values):
                conflicts.append({
                    'type': 'amount_conflict',
                    'values': list(amount_values),
                    'correct_value': '100ì–µì›'
                })
        
        # ê³µì‹œ ê¸°í•œ ëª…í™•ì„± í™•ì¸ (ì¶©ëŒì´ ì•„ë‹Œ ëª…í™•ì„± ì²´í¬ë¡œ ë³€ê²½)
        if 'deadlines' in critical_info:
            unclear_deadlines = []
            for item in critical_info['deadlines']:
                if 'ê³µì‹œ' in item['context'] and 'ì˜ê²°' in item['context']:
                    # ì˜ì—…ì¼ ëª…ì‹œ ì—¬ë¶€ í™•ì¸
                    if 'ì˜ì—…ì¼' not in item['context']:
                        unclear_deadlines.append(item['value'])
            
            if unclear_deadlines:
                conflicts.append({
                    'type': 'deadline_clarity',
                    'values': list(unclear_deadlines),
                    'correct_value': 'ì˜ì—…ì¼ 7ì¼',
                    'issue': 'ì˜ì—…ì¼ ê¸°ì¤€ì„ì„ ëª…ì‹œí•´ì•¼ í•¨'
                })
        
        return conflicts
    
    def _prioritize_latest_info(self, results: List[SearchResult], conflicts: List[Dict]) -> List[SearchResult]:
        """ì¶©ëŒì´ ìˆì„ ë•Œ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        for conflict in conflicts:
            if conflict['type'] == 'amount_conflict':
                # êµ¬ë²„ì „ ê¸ˆì•¡ì´ í¬í•¨ëœ ê²°ê³¼ì˜ ì ìˆ˜ ê°ì†Œ
                for i, result in enumerate(results):
                    if any(old_val in result.content for old_val in ['50ì–µì›', '30ì–µì›']):
                        results[i].score *= 0.5  # ì ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
                        results[i].metadata['score_reduced'] = True
                        results[i].metadata['reduction_reason'] = 'outdated_amount'
            
            elif conflict['type'] == 'deadline_clarity':
                # ì˜ì—…ì¼ì´ ëª…ì‹œë˜ì§€ ì•Šì€ ê²°ê³¼ì˜ ì ìˆ˜ ì•½ê°„ ê°ì†Œ
                for i, result in enumerate(results):
                    deadline_match = re.search(r'ì˜ê²°.*?(\d+)\s*ì¼.*?ê³µì‹œ', result.content)
                    if deadline_match:
                        context = result.content[max(0, deadline_match.start()-50):deadline_match.end()+50]
                        if 'ì˜ì—…ì¼' not in context:
                            results[i].score *= 0.85  # ì ìˆ˜ë¥¼ 15% ê°ì†Œ
                            results[i].metadata['score_reduced'] = True
                            results[i].metadata['reduction_reason'] = 'unclear_deadline_specification'
                            results[i].metadata['clarification_needed'] = 'ì˜ì—…ì¼ ê¸°ì¤€ì„ì„ ëª…ì‹œ í•„ìš”'
        
        return results

# ===== ê²€ìƒ‰ ì „ëµ êµ¬í˜„ =====
class DirectSearchStrategy(SearchStrategy):
    """ì§ì ‘ ê²€ìƒ‰ ì „ëµ - ë‹¨ìˆœí•œ ì§ˆë¬¸ì— ì í•©"""
    def __init__(self, manual_indices: Dict[str, List[int]]):
        self.manual_indices = manual_indices
    
    def prepare_indices(self, manual: str, limit: int = 100) -> List[int]:
        return self.manual_indices.get(manual, [])[:limit]
    
    def enhance_query(self, query: str, keywords: List[str]) -> str:
        return f"{query} {' '.join(keywords)}"

class FocusedSearchStrategy(SearchStrategy):
    """ì§‘ì¤‘ ê²€ìƒ‰ ì „ëµ - íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ê²€ìƒ‰"""
    def __init__(self, manual_indices: Dict[str, List[int]], chunk_loader: ChunkLoader):
        self.manual_indices = manual_indices
        self.chunk_loader = chunk_loader
    
    def prepare_indices(self, manual: str, limit: int = 200) -> List[int]:
        return self.manual_indices.get(manual, [])[:limit]
    
    def enhance_query(self, query: str, keywords: List[str]) -> str:
        return f"{query} {' '.join(keywords)}"
    
    def filter_results(self, results: List[SearchResult], requirements: Dict) -> List[SearchResult]:
        """ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ê²°ê³¼ í•„í„°ë§"""
        if requirements.get('needs_specific_numbers'):
            # ìˆ«ìê°€ í¬í•¨ëœ ê²°ê³¼ ìš°ì„ 
            number_results = []
            other_results = []
            
            for result in results:
                if re.search(r'\d+ì–µ|\d+%', result.content):
                    result.score *= 1.2  # ê°€ì¤‘ì¹˜ ë¶€ì—¬
                    number_results.append(result)
                else:
                    other_results.append(result)
            
            return number_results + other_results
        
        return results

# ===== ê²¬ê³ í•œ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ =====
class RobustSearchPipeline:
    """ì—ëŸ¬ ì²˜ë¦¬ê°€ ê°•í™”ëœ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸
    
    ì´ í´ë˜ìŠ¤ëŠ” ê²€ìƒ‰ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì—ëŸ¬ë¥¼
    ìš°ì•„í•˜ê²Œ ì²˜ë¦¬í•˜ê³ , ì‚¬ìš©ìì—ê²Œ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, base_executor: BaseSearchExecutor):
        self.base_executor = base_executor
        self.error_history = []
        self.max_retries = 3
        self.timeout_seconds = 30
    
    async def search_with_retry(self, 
                               query: str,
                               indices: List[int],
                               top_k: int,
                               strategy: SearchStrategy) -> Tuple[List[SearchResult], Dict]:
        """ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ì•ˆì „í•œ ê²€ìƒ‰"""
        for attempt in range(self.max_retries):
            try:
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                results, stats = await asyncio.wait_for(
                    self.base_executor.execute_search(query, indices, top_k, strategy),
                    timeout=self.timeout_seconds
                )
                
                # ê²°ê³¼ ê²€ì¦
                if self._validate_results(results, stats):
                    return results, stats
                else:
                    raise ValueError("Invalid search results")
                    
            except asyncio.TimeoutError:
                error = SearchError(
                    error_type="timeout",
                    message=f"Search timed out after {self.timeout_seconds}s",
                    timestamp=time.time(),
                    context={"query": query, "attempt": attempt + 1},
                    severity="warning"
                )
                self._log_error(error)
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    continue
                    
            except Exception as e:
                error = SearchError(
                    error_type=type(e).__name__,
                    message=str(e),
                    timestamp=time.time(),
                    context={
                        "query": query, 
                        "attempt": attempt + 1,
                        "traceback": traceback.format_exc()
                    },
                    severity="critical" if attempt == self.max_retries - 1 else "warning"
                )
                self._log_error(error)
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(1)
                    continue
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
        return self._get_fallback_results(query), self._get_error_stats()
    
    def _validate_results(self, results: List[SearchResult], stats: Dict) -> bool:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ìœ íš¨ì„± ê²€ì¦"""
        if not isinstance(results, list):
            return False
        
        if not stats or not isinstance(stats, dict):
            return False
        
        # ê²°ê³¼ê°€ ë„ˆë¬´ ì ê±°ë‚˜ ë§ì€ ê²½ìš°
        if len(results) == 0 and stats.get('searched_chunks', 0) > 0:
            logger.warning("No results despite searching chunks")
            return False
        
        # ê° ê²°ê³¼ì˜ ìœ íš¨ì„± ê²€ì¦
        for result in results:
            if not isinstance(result, SearchResult):
                return False
            if not result.content or len(result.content) < 10:
                return False
            if result.score < 0 or result.score > 1000:
                return False
        
        return True
    
    def _log_error(self, error: SearchError):
        """ì—ëŸ¬ ë¡œê¹… ë° ê¸°ë¡"""
        self.error_history.append(error)
        
        # ìµœê·¼ 100ê°œ ì—ëŸ¬ë§Œ ìœ ì§€
        if len(self.error_history) > 100:
            self.error_history = self.error_history[-100:]
        
        # ë¡œê¹…
        if error.severity == "critical":
            logger.error(f"{error.error_type}: {error.message}")
        else:
            logger.warning(f"{error.error_type}: {error.message}")
    
    def _get_fallback_results(self, query: str) -> List[SearchResult]:
        """ì—ëŸ¬ ì‹œ ë°˜í™˜í•  ê¸°ë³¸ ê²°ê³¼"""
        return [
            SearchResult(
                chunk_id="error_fallback",
                content="ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                score=0.0,
                source="System",
                page=0,
                chunk_type="error",
                metadata={"error": True, "query": query}
            )
        ]
    
    def _get_error_stats(self) -> Dict:
        """ì—ëŸ¬ ë°œìƒ ì‹œ í†µê³„ ì •ë³´"""
        recent_errors = [e for e in self.error_history[-10:]]
        return {
            "error": True,
            "error_count": len(self.error_history),
            "recent_errors": [
                {"type": e.error_type, "time": e.timestamp} 
                for e in recent_errors
            ],
            "search_time": 0,
            "searched_chunks": 0
        }

# ===== ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ (Python 3.13 í˜¸í™˜) =====
class ImprovedHybridRAGPipeline:
    """Python 3.13ì—ì„œë„ ì‘ë™í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸
    
    ì´ í´ë˜ìŠ¤ëŠ” FAISSë‚˜ sentence-transformersê°€ ì—†ì–´ë„ ì‘ë™í•˜ë„ë¡
    ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. OpenAI embeddingsë‚˜ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, embedding_model, reranker_model, index, chunk_loader: ChunkLoader,
                 api_manager: SecureAPIManager):
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunk_loader = chunk_loader
        self.api_manager = api_manager
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ì´ˆê¸°í™”
        self.hybrid_strategy = HybridGPTStrategy(api_manager)
        self.query_analyzer = EnhancedQueryAnalyzer(api_manager, self.hybrid_strategy)
        
        # ë¬¸ì„œ ë²„ì „ ê´€ë¦¬
        self.version_manager = DocumentVersionManager()
        self.conflict_resolver = ConflictResolver(self.version_manager)
        
        # ë§¤ë‰´ì–¼ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.manual_indices = self._build_manual_indices()
        self.chunks_info = {
            category: len(indices) 
            for category, indices in self.manual_indices.items()
        }
        
        # ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (FAISS ë˜ëŠ” ëŒ€ì²´ ì‹œìŠ¤í…œ)
        self._initialize_search_system()
        
        # ìºì‹œ
        self.search_cache = LRUCache(max_size=50, ttl=1800)
        
        logger.info(f"Pipeline initialized with {chunk_loader.get_total_chunks()} chunks")
    
    def _initialize_search_system(self):
        """ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - FAISSê°€ ì—†ìœ¼ë©´ ëŒ€ì²´ ì‹œìŠ¤í…œ ì‚¬ìš©"""
        if self.index is not None:
            # FAISS ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            self.use_faiss = True
            logger.info("Using FAISS for vector search")
        else:
            # FAISSê°€ ì—†ìœ¼ë©´ ëŒ€ì²´ ì‹œìŠ¤í…œ ì‚¬ìš©
            self.use_faiss = False
            logger.info("FAISS not available, initializing alternative search")
            
            # ì„ë² ë”©ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            embeddings_file = "chunk_embeddings.npy"
            if os.path.exists(embeddings_file):
                try:
                    embeddings = np.load(embeddings_file)
                    self.simple_search = SimpleVectorSearch(embeddings)
                    logger.info(f"Loaded {len(embeddings)} pre-computed embeddings")
                except Exception as e:
                    logger.warning(f"Failed to load embeddings: {e}")
                    self._create_simple_search()
            else:
                self._create_simple_search()
    
    def _create_simple_search(self):
        """ê°„ë‹¨í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒì„±"""
        logger.info("Creating embeddings for simple search...")
        
        # ëª¨ë“  ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
        total_chunks = self.chunk_loader.get_total_chunks()
        # ë°ëª¨/í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²˜ìŒ 1000ê°œë§Œ ì²˜ë¦¬ (ì „ì²´ ì²˜ë¦¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
        max_chunks = min(total_chunks, 1000)
        
        embeddings = []
        
        if self.embedding_model is not None:
            # sentence-transformers ì‚¬ìš© ê°€ëŠ¥
            for i in range(max_chunks):
                try:
                    chunk = self.chunk_loader.get_chunk(i)
                    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
                    text = chunk['content'][:500]
                    embedding = self.embedding_model.encode([text])
                    embeddings.append(embedding[0])
                except Exception as e:
                    logger.warning(f"Failed to create embedding for chunk {i}: {e}")
                    # ì‹¤íŒ¨í•œ ê²½ìš° ëœë¤ ë²¡í„° ì‚¬ìš© (ì„ì‹œ)
                    embeddings.append(np.random.randn(384))  # ì¼ë°˜ì ì¸ ì„ë² ë”© ì°¨ì›
        else:
            # OpenAI embeddings ì‚¬ìš©
            logger.info("Using OpenAI embeddings (this may take a while)...")
            for i in range(max_chunks):
                try:
                    chunk = self.chunk_loader.get_chunk(i)
                    text = chunk['content'][:500]
                    embedding = self.api_manager.get_embedding(text)
                    embeddings.append(embedding)
                except Exception as e:
                    logger.warning(f"Failed to create OpenAI embedding for chunk {i}: {e}")
                    embeddings.append(np.random.randn(1536))  # OpenAI ì„ë² ë”© ì°¨ì›
        
        embeddings_array = np.array(embeddings)
        
        # ì„ë² ë”© ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´)
        try:
            np.save("chunk_embeddings.npy", embeddings_array)
            logger.info("Saved embeddings for future use")
        except Exception as e:
            logger.warning(f"Failed to save embeddings: {e}")
        
        self.simple_search = SimpleVectorSearch(embeddings_array)
    
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        # ì²­í¬ ë©”íƒ€ë°ì´í„°ë§Œ ë¹ ë¥´ê²Œ ìŠ¤ìº”
        for idx in range(self.chunk_loader.get_total_chunks()):
            try:
                chunk = self.chunk_loader.get_chunk(idx)
                source = chunk.get('source', '').lower()
                
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                    indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜'].append(idx)
                elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                    indices['í˜„í™©ê³µì‹œ'].append(idx)
                elif 'ë¹„ìƒì¥' in source:
                    indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­'].append(idx)
                else:
                    indices['ê¸°íƒ€'].append(idx)
                    
                # ë¬¸ì„œ ë‚ ì§œ ì¶”ì¶œ ë° ì €ì¥
                doc_date = self.version_manager.extract_document_date(chunk)
                if doc_date:
                    metadata = json.loads(chunk.get('metadata', '{}'))
                    metadata['document_date'] = doc_date
                    chunk['metadata'] = json.dumps(metadata)
                    
            except Exception as e:
                logger.warning(f"Failed to process chunk {idx}: {e}")
                continue
        
        return dict(indices)
    
    def _get_query_embedding(self, query: str) -> np.ndarray:
        """ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±"""
        if self.embedding_model is not None:
            # sentence-transformers ì‚¬ìš©
            return self.embedding_model.encode([query])[0]
        else:
            # OpenAI embeddings ì‚¬ìš©
            return np.array(self.api_manager.get_embedding(query))
    
    def _perform_vector_search(self, query_vector: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ - FAISS ë˜ëŠ” ëŒ€ì²´ ì‹œìŠ¤í…œ ì‚¬ìš©"""
        if self.use_faiss:
            # FAISS ì‚¬ìš©
            query_vector_2d = query_vector.reshape(1, -1).astype(np.float32)
            return self.index.search(query_vector_2d, k)
        else:
            # SimpleVectorSearch ì‚¬ìš©
            return self.simple_search.search(query_vector, k)
    
    async def process_query(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """ì¿¼ë¦¬ ì²˜ë¦¬ - Python 3.13 í˜¸í™˜"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(f"{query}_{top_k}".encode()).hexdigest()
        cached = self.search_cache.get(cache_key)
        if cached:
            logger.info(f"Cache hit for query: {query[:50]}...")
            return cached['results'], cached['stats']
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (í´ë°±)
        if not self.use_faiss and not hasattr(self, 'simple_search'):
            logger.warning("No vector search available, using keyword search")
            results = self._keyword_search(query, top_k)
            stats = {
                'search_method': 'keyword',
                'search_time': time.time() - start_time,
                'searched_chunks': len(results)
            }
            return results, stats
        
        # ë²¡í„° ê²€ìƒ‰
        try:
            query_vector = self._get_query_embedding(query)
            scores, indices = self._perform_vector_search(query_vector, top_k * 3)
            
            # ê²°ê³¼ ë³€í™˜
            results = []
            for idx, score in zip(indices[0], scores[0]):
                if 0 <= idx < self.chunk_loader.get_total_chunks():
                    chunk = self.chunk_loader.get_chunk(idx)
                    result = SearchResult(
                        chunk_id=str(idx),
                        content=chunk['content'],
                        score=float(score),
                        source=chunk.get('source', 'Unknown'),
                        page=chunk.get('page', 0),
                        chunk_type=chunk.get('chunk_type', 'text'),
                        metadata=json.loads(chunk.get('metadata', '{}'))
                    )
                    results.append(result)
                    if len(results) >= top_k:
                        break
            
            # ì¶©ëŒ í•´ê²°
            results = self.conflict_resolver.resolve_conflicts(results, query)
            
            stats = {
                'search_method': 'vector',
                'search_time': time.time() - start_time,
                'searched_chunks': len(indices[0])
            }
            
            # ìºì‹œ ì €ì¥
            self.search_cache.put(cache_key, {'results': results, 'stats': stats})
            
            return results, stats
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            # í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            results = self._keyword_search(query, top_k)
            stats = {
                'search_method': 'keyword_fallback',
                'search_time': time.time() - start_time,
                'searched_chunks': len(results),
                'error': str(e)
            }
            return results, stats
    
    def _keyword_search(self, query: str, top_k: int) -> List[SearchResult]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (í´ë°±ìš©)"""
        query_words = set(query.lower().split())
        scored_chunks = []
        
        for i in range(min(self.chunk_loader.get_total_chunks(), 1000)):
            try:
                chunk = self.chunk_loader.get_chunk(i)
                content_lower = chunk['content'].lower()
                
                # ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                score = sum(1 for word in query_words if word in content_lower)
                
                if score > 0:
                    scored_chunks.append((i, score, chunk))
            except Exception as e:
                logger.warning(f"Error in keyword search for chunk {i}: {e}")
                continue
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_chunks.sort(key=lambda x: x[1], reverse=True)
        
        # SearchResult ìƒì„±
        results = []
        for idx, score, chunk in scored_chunks[:top_k]:
            result = SearchResult(
                chunk_id=str(idx),
                content=chunk['content'],
                score=float(score),
                source=chunk.get('source', 'Unknown'),
                page=chunk.get('page', 0),
                chunk_type=chunk.get('chunk_type', 'text'),
                metadata=json.loads(chunk.get('metadata', '{}'))
            )
            results.append(result)
        
        return results

# ===== ê°œì„ ëœ ë‹µë³€ ìƒì„± í•¨ìˆ˜ =====
async def generate_answer_with_hybrid_model(query: str, 
                                          results: List[SearchResult], 
                                          stats: Dict,
                                          api_manager: SecureAPIManager,
                                          hybrid_strategy: HybridGPTStrategy) -> str:
    """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì „ëµì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
    
    ì§ˆë¬¸ì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ì—¬
    ê³ í’ˆì§ˆì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ëª¨ë¸ ì„ íƒ
    gpt_analysis = stats.get('gpt_analysis', {})
    selected_model = hybrid_strategy.select_model_for_answer(query, results, gpt_analysis)
    
    logger.info(f"Selected {selected_model.value} for answer generation")
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for i, result in enumerate(results[:5]):
        # êµ¬ë²„ì „ ì •ë³´ ê²½ê³  í¬í•¨
        warnings = result.metadata.get('warnings', [])
        warning_text = ""
        if warnings:
            warning_text = "\nâš ï¸ ì£¼ì˜: ì´ ë¬¸ì„œì—ëŠ” ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        context_parts.append(f"""
[ì°¸ê³  {i+1}] {result.source} (í˜ì´ì§€ {result.page}){warning_text}
{result.content}
""")
    
    context = "\n---\n".join(context_parts)
    
    # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
    if selected_model == ModelSelection.O4_MINI:
        # ì¶”ë¡  ëª¨ë¸ìš© ìƒì„¸ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ë…¼ë¦¬ì  ì¶”ë¡ ì— ë›°ì–´ë‚œ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ì¶”ë¡ ì„ í†µí•´ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ êµ¬ì¡°:
1. í•µì‹¬ ë‹µë³€ (ì§ì ‘ì ì´ê³  ëª…í™•í•˜ê²Œ)
2. ë²•ì  ê·¼ê±°ì™€ ì¶”ë¡  ê³¼ì •
   - "ì²«ì§¸, ..." (ê° ë…¼ì ì„ ë‹¨ê³„ë³„ë¡œ)
   - "ë‘˜ì§¸, ..."
   - "ë”°ë¼ì„œ, ..."
3. ì‹¤ë¬´ ì ìš© ì§€ì¹¨
4. ì£¼ì˜ì‚¬í•­ ë° ì˜ˆì™¸

íŠ¹íˆ ë³µì¡í•œ ìƒí™©ì—ì„œëŠ” ê° ìš”ì†Œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•œ í›„ ì¢…í•©í•˜ì„¸ìš”."""
        
        user_prompt = f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

[ì¶”ê°€ ê³ ë ¤ì‚¬í•­]
- ìµœì‹  ê°œì •ì‚¬í•­ ë°˜ì˜ (ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸°ì¤€: 100ì–µì›, ê³µì‹œê¸°í•œ: ì˜ì—…ì¼ 7ì¼)
- ìƒì¶©í•˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ
- ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„"""
        
    elif selected_model == ModelSelection.GPT4O_MINI:
        # ê°„ë‹¨í•œ ëª¨ë¸ìš© ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
        
        user_prompt = f"""[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

í•µì‹¬ë§Œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""
        
    else:  # GPT-4o
        # ë²”ìš© ëª¨ë¸ìš© ê· í˜•ì¡íŒ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥)
2. ìƒì„¸ ì„¤ëª… (ê·¼ê±° ì¡°í•­ í¬í•¨)
3. ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì˜ˆì™¸ì‚¬í•­ (ìˆëŠ” ê²½ìš°)"""
        
        user_prompt = f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."""
    
    # API í˜¸ì¶œ (ì†ë„ ì œí•œ ì ìš©)
    @api_manager.rate_limit(selected_model.value)
    async def call_api():
        return await openai.chat.completions.create(
            model=selected_model.value,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1 if selected_model == ModelSelection.O4_MINI else 0.3,
            max_tokens=1500
        )
    
    response = await call_api()
    answer = response.choices[0].message.content
    
    # êµ¬ë²„ì „ ì •ë³´ ê²½ê³  ì¶”ê°€
    if stats.get('has_outdated_warnings'):
        answer += "\n\nâš ï¸ **ì¤‘ìš”**: ì¼ë¶€ ì°¸ê³  ìë£Œì— ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì‹  ê·œì •ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    
    return answer

# ===== ë¹„ë™ê¸° ì‹¤í–‰ í—¬í¼ =====
def run_async_in_streamlit(coro):
    """Streamlit í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
    
    Streamlit CloudëŠ” ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ì‹¤í–‰í•˜ê³  ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
    ë” ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ë¹„ë™ê¸° ì½”ë“œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        # ë¨¼ì € í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
        try:
            loop = asyncio.get_running_loop()
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆë‹¤ë©´ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            import nest_asyncio
            nest_asyncio.apply()
            return asyncio.run(coro)
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„±
            return asyncio.run(coro)
    except Exception as e:
        logger.error(f"Async execution failed: {str(e)}")
        # ìµœí›„ì˜ ìˆ˜ë‹¨: ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ ì‹œë„
        import inspect
        if inspect.iscoroutine(coro):
            # ì½”ë£¨í‹´ì„ ê°•ì œë¡œ ì‹¤í–‰
            try:
                return asyncio.new_event_loop().run_until_complete(coro)
            except:
                raise RuntimeError(f"Failed to execute async function: {str(e)}")

# ===== í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ =====
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display: none;}
    
    /* ë©”ì¸ í—¤ë” ìŠ¤íƒ€ì¼ */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 900px;
        margin: 0 auto;
    }
    
    /* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ ê°œì„  */
    [data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* ë³µì¡ë„ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .complexity-indicator {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 16px;
        font-size: 0.85rem;
        font-weight: 500;
        margin-left: 8px;
    }
    
    .complexity-simple {
        background-color: #d4edda;
        color: #155724;
    }
    
    .complexity-medium {
        background-color: #fff3cd;
        color: #856404;
    }
    
    .complexity-complex {
        background-color: #f8d7da;
        color: #721c24;
    }
    
    /* ëª¨ë¸ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .model-indicator {
        display: inline-block;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.75rem;
        font-weight: 600;
        margin-left: 4px;
    }
    
    .model-gpt4o {
        background-color: #e3f2fd;
        color: #1565c0;
    }
    
    .model-gpt4o-mini {
        background-color: #f3e5f5;
        color: #6a1b9a;
    }
    
    .model-o4-mini {
        background-color: #e8f5e9;
        color: #2e7d32;
    }
    
    /* ê²½ê³  ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .outdated-warning {
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        border-radius: 4px;
        padding: 0.75rem 1.25rem;
        margin: 0.5rem 0;
        color: #856404;
    }
</style>
""", unsafe_allow_html=True)

# ===== ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© (Python 3.13 í˜¸í™˜ ë²„ì „) =====
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ - Python 3.13 í˜¸í™˜"""
    try:
        # API ê´€ë¦¬ì ì´ˆê¸°í™”
        api_manager = SecureAPIManager()
        api_manager.load_api_key()
        openai.api_key = api_manager._api_key
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            st.info("ğŸ’¡ GitHub ì €ì¥ì†Œì— ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return None
        
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
            # FAISS ì¸ë±ìŠ¤ ì²´í¬ - ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±
            index = None
            index_file = "manuals_vector_db.index"
            
            if os.path.exists(index_file) and FAISS_AVAILABLE:
                try:
                    import faiss
                    index = faiss.read_index(index_file)
                    logger.info("FAISS index loaded successfully")
                except Exception as e:
                    logger.warning(f"Failed to load FAISS index: {e}")
                    index = None
            
            # ì²­í¬ ë¡œë” ì´ˆê¸°í™”
            chunk_loader = ChunkLoader("all_manual_chunks.json")
            
            # ì„ë² ë”© ëª¨ë¸ - Python 3.13ì—ì„œ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëŒ€ì²´ ë°©ì•ˆ ì¤€ë¹„
            embedding_model = None
            
            # sentence-transformersê°€ ì‹¤íŒ¨í•˜ë©´ OpenAI embeddings ì‚¬ìš©
            try:
                from sentence_transformers import SentenceTransformer
                embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("Sentence transformer model loaded")
            except Exception as e:
                logger.warning(f"Sentence transformers not available: {e}")
                logger.info("Will use OpenAI embeddings as fallback")
            
            # RerankerëŠ” ì„ íƒì‚¬í•­
            reranker_model = None
            
            # ì¸ë±ìŠ¤ê°€ ì—†ê³  FAISSë„ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‚¬ìš©
            if index is None and not FAISS_AVAILABLE:
                logger.warning("FAISS not available, using simple search")
                # ê°„ë‹¨í•œ ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ì¤€ë¹„
                st.info("ğŸ’¡ ë²¡í„° ê²€ìƒ‰ ëŒ€ì‹  í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            return embedding_model, reranker_model, index, chunk_loader, api_manager
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(f"System initialization failed: {str(e)}")
        with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
            st.code(traceback.format_exc())
        return None

# ===== ë©”ì¸ UI =====
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜"""
    
    # í—¤ë” í‘œì‹œ
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ AI Q&A ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    models = load_models_and_data()
    if not models or len(models) != 5:
        st.stop()
    
    embedding_model, reranker_model, index, chunk_loader, api_manager = models
    
    # ê°œì„ ëœ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    rag = ImprovedHybridRAGPipeline(
        embedding_model, reranker_model, index, chunk_loader, api_manager
    )
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "api_usage" not in st.session_state:
        st.session_state.api_usage = api_manager.get_usage_stats()
    
    # ì±„íŒ… ì»¨í…Œì´ë„ˆ
    chat_container = st.container()
    
    with chat_container:
        # ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "user":
                    st.write(message["content"])
                else:
                    if isinstance(message["content"], dict):
                        st.write(message["content"]["answer"])
                        
                        # ë³µì¡ë„ ë° ëª¨ë¸ ì •ë³´ í‘œì‹œ
                        complexity = message["content"].get("complexity", "unknown")
                        model_used = message["content"].get("model_used", "unknown")
                        
                        complexity_html = f'<span class="complexity-indicator complexity-{complexity}">{complexity.upper()}</span>'
                        model_html = f'<span class="model-indicator model-{model_used.replace("-", "")}">{model_used}</span>'
                        
                        st.markdown(f"ë³µì¡ë„: {complexity_html} | ì‚¬ìš© ëª¨ë¸: {model_html}", unsafe_allow_html=True)
                        
                        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                        if "total_time" in message["content"]:
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ğŸ” ê²€ìƒ‰", f"{message['content']['search_time']:.1f}ì´ˆ")
                            with col2:
                                st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{message['content']['generation_time']:.1f}ì´ˆ")
                            with col3:
                                st.metric("â±ï¸ ì „ì²´", f"{message['content']['total_time']:.1f}ì´ˆ")
                        
                        # êµ¬ë²„ì „ ì •ë³´ ê²½ê³ 
                        if message["content"].get("has_outdated_warnings"):
                            st.markdown("""
                            <div class="outdated-warning">
                            âš ï¸ ì¼ë¶€ ì°¸ê³  ìë£Œì— ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                            </div>
                            """, unsafe_allow_html=True)
                    else:
                        st.write(message["content"])
        
        # ìƒˆ ì§ˆë¬¸ ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.write(prompt)
            
            # AI ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                total_start_time = time.time()
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                search_start_time = time.time()
                with st.spinner("ğŸ” AIê°€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ì¤‘..."):
                    results, stats = run_async_in_streamlit(rag.process_query(prompt, top_k=5))
                search_time = time.time() - search_start_time
                
                # ë‹µë³€ ìƒì„±
                generation_start_time = time.time()
                with st.spinner("ğŸ’­ AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer = run_async_in_streamlit(
                        generate_answer_with_hybrid_model(
                            prompt, results, stats, api_manager, rag.hybrid_strategy
                        )
                    )
                generation_time = time.time() - generation_start_time
                
                total_time = time.time() - total_start_time
                
                # ë‹µë³€ í‘œì‹œ
                st.write(answer)
                
                # ë¶„ì„ ì •ë³´ í‘œì‹œ
                gpt_analysis = stats.get('gpt_analysis', {})
                complexity = gpt_analysis.get('query_analysis', {}).get('actual_complexity', 'unknown')
                model_used = gpt_analysis.get('model_used', 'unknown')
                
                complexity_html = f'<span class="complexity-indicator complexity-{complexity}">{complexity.upper()}</span>'
                model_html = f'<span class="model-indicator model-{model_used.replace("-", "")}">{model_used}</span>'
                
                st.markdown(f"ì§ˆë¬¸ ë³µì¡ë„: {complexity_html} | ë¶„ì„ ëª¨ë¸: {model_html}", unsafe_allow_html=True)
                
                # êµ¬ë²„ì „ ì •ë³´ ê²½ê³ 
                if stats.get('has_outdated_warnings'):
                    st.markdown("""
                    <div class="outdated-warning">
                    âš ï¸ ì¼ë¶€ ì°¸ê³  ìë£Œì— ê°œì • ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    </div>
                    """, unsafe_allow_html=True)
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ” ê²€ìƒ‰", f"{search_time:.1f}ì´ˆ")
                with col2:
                    st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{generation_time:.1f}ì´ˆ")
                with col3:
                    st.metric("â±ï¸ ì „ì²´", f"{total_time:.1f}ì´ˆ")
                
                # ìƒì„¸ ì •ë³´
                with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                    if gpt_analysis:
                        st.subheader("ğŸ¤– AI ì§ˆë¬¸ ë¶„ì„")
                        
                        # ë¶„ì„ ë‚´ìš© í‘œì‹œ
                        col1, col2 = st.columns(2)
                        with col1:
                            st.json({
                                "í•µì‹¬ ì˜ë„": gpt_analysis.get('query_analysis', {}).get('core_intent', ''),
                                "ì‹¤ì œ ë³µì¡ë„": complexity,
                                "ì„ íƒ ì´ìœ ": gpt_analysis.get('model_selection_reason', ''),
                                "ê²€ìƒ‰ ì „ëµ": gpt_analysis.get('search_strategy', {}).get('approach', '')
                            })
                        
                        with col2:
                            st.json({
                                "ì£¼ìš” ë§¤ë‰´ì–¼": gpt_analysis.get('search_strategy', {}).get('primary_manual', ''),
                                "ê²€ìƒ‰ í‚¤ì›Œë“œ": gpt_analysis.get('search_strategy', {}).get('search_keywords', []),
                                "í•„ìš” ì²­í¬ ìˆ˜": gpt_analysis.get('search_strategy', {}).get('expected_chunks_needed', 0)
                            })
                        
                        # ì¶”ë¡  ì²´ì¸ í‘œì‹œ (o4-mini ì‚¬ìš© ì‹œ)
                        reasoning_chain = gpt_analysis.get('query_analysis', {}).get('reasoning_chain', [])
                        if reasoning_chain:
                            st.subheader("ğŸ§  ì¶”ë¡  ê³¼ì •")
                            for i, step in enumerate(reasoning_chain, 1):
                                st.write(f"{i}. {step}")
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œ
                    st.subheader("ğŸ“š ì°¸ê³  ìë£Œ")
                    for i, result in enumerate(results[:3]):
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.caption(f"**{result.source}** - í˜ì´ì§€ {result.page}")
                        with col2:
                            st.caption(f"ê´€ë ¨ë„: {result.score:.2f}")
                        
                        # ë¬¸ì„œ ë‚ ì§œ ë° ê²½ê³ 
                        if result.document_date:
                            st.caption(f"ğŸ“… ë¬¸ì„œ ë‚ ì§œ: {result.document_date}")
                        
                        if result.metadata.get('has_outdated_info'):
                            warnings = result.metadata.get('warnings', [])
                            for warning in warnings:
                                st.warning(f"âš ï¸ êµ¬ë²„ì „ ì •ë³´: {warning['found']} â†’ í˜„ì¬: {warning['current']}")
                        
                        # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                        with st.container():
                            content = result.content[:300] + "..." if len(result.content) > 300 else result.content
                            st.text(content)
                    
                    # API ì‚¬ìš© í†µê³„
                    st.subheader("ğŸ“Š API ì‚¬ìš© í†µê³„")
                    usage_stats = api_manager.get_usage_stats()
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("GPT-4o ë¹„ìš©", f"${usage_stats['costs'].get('gpt-4o', 0):.4f}")
                    with col2:
                        st.metric("GPT-4o-mini ë¹„ìš©", f"${usage_stats['costs'].get('gpt-4o-mini', 0):.4f}")
                    with col3:
                        st.metric("o4-mini ë¹„ìš©", f"${usage_stats['costs'].get('o4-mini', 0):.4f}")
                    
                    st.info(f"ğŸ’° ì˜ˆìƒ ì›” ë¹„ìš©: ${usage_stats.get('estimated_monthly_cost', 0):.2f}")
                
                # ì‘ë‹µ ë°ì´í„° ì €ì¥
                response_data = {
                    "answer": answer,
                    "search_time": search_time,
                    "generation_time": generation_time,
                    "total_time": total_time,
                    "complexity": complexity,
                    "model_used": model_used,
                    "has_outdated_warnings": stats.get('has_outdated_warnings', False)
                }
                st.session_state.messages.append({"role": "assistant", "content": response_data})
    
    # êµ¬ë¶„ì„ 
    st.divider()
    
    # ë©´ì±… ì¡°í•­
    st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ìƒì„±í•œ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬í•­ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        
        # ë³µì¡ë„ë³„ ì˜ˆì‹œ ì§ˆë¬¸
        st.subheader("ğŸŸ¢ ë‹¨ìˆœ ì§ˆë¬¸")
        if st.button("ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œì€ ë©°ì¹ ì¸ê°€ìš”?"
            st.rerun()
        if st.button("ì´ì‚¬íšŒ ì˜ê²° ê¸ˆì•¡ ê¸°ì¤€ì€?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ì—ì„œ ì´ì‚¬íšŒ ì˜ê²°ì´ í•„ìš”í•œ ê±°ë˜ ê¸ˆì•¡ì€?"
            st.rerun()
            
        st.subheader("ğŸŸ¡ ì¤‘ê°„ ë³µì¡ë„")
        if st.button("ê³„ì—´ì‚¬ ê±°ë˜ ì‹œ ì£¼ì˜ì‚¬í•­ì€?"):
            st.session_state.new_question = "ê³„ì—´ì‚¬ì™€ ìê¸ˆê±°ë˜ë¥¼ í•  ë•Œ ì–´ë–¤ ì ˆì°¨ë¥¼ ê±°ì³ì•¼ í•˜ê³  ì£¼ì˜í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            st.rerun()
        if st.button("ë¹„ìƒì¥ì‚¬ ì£¼ì‹ ì–‘ë„ ì ˆì°¨ëŠ”?"):
            st.session_state.new_question = "ë¹„ìƒì¥íšŒì‚¬ê°€ ì£¼ì‹ì„ ì–‘ë„í•  ë•Œ í•„ìš”í•œ ì ˆì°¨ì™€ ê³µì‹œ ì˜ë¬´ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
            st.rerun()
            
        st.subheader("ğŸ”´ ë³µì¡í•œ ì§ˆë¬¸ (ì¶”ë¡  í•„ìš”)")
        if st.button("ë³µí•© ê±°ë˜ ë¶„ì„"):
            st.session_state.new_question = "AíšŒì‚¬ê°€ Bê³„ì—´ì‚¬ì— ìê¸ˆì„ ëŒ€ì—¬í•˜ë©´ì„œ ë™ì‹œì— Cê³„ì—´ì‚¬ì˜ ì£¼ì‹ì„ ì·¨ë“í•˜ëŠ” ê²½ìš°, ê°ê° ì–´ë–¤ ê·œì œê°€ ì ìš©ë˜ê³  ê³µì‹œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
            st.rerun()
        if st.button("ì¢…í•©ì  ë¦¬ìŠ¤í¬ ê²€í† "):
            st.session_state.new_question = "ìš°ë¦¬ íšŒì‚¬ê°€ ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜ë¥¼ ì§„í–‰í•  ë•Œ ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê·œì œì™€ ê´€ë ¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•´ì•¼ í•  ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ì „ëµì€?"
            st.rerun()
        
        st.divider()
        
        # ì‹œìŠ¤í…œ ì •ë³´
        st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´")
        st.caption("ì´ ì‹œìŠ¤í…œì€ ì§ˆë¬¸ì˜ ë³µì¡ë„ì— ë”°ë¼ ìµœì ì˜ AI ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤:")
        st.caption("â€¢ **o4-mini**: ë³µì¡í•œ ì¶”ë¡ ")
        st.caption("â€¢ **GPT-4o**: ì¢…í•©ì  ë¶„ì„")
        st.caption("â€¢ **GPT-4o-mini**: ë‹¨ìˆœ ì¡°íšŒ")
        
        st.divider()
        
        # ì„±ëŠ¥ í†µê³„
        if st.button("ğŸ“Š ì„±ëŠ¥ í†µê³„ ë³´ê¸°"):
            with st.container():
                st.subheader("API ì‚¬ìš© í˜„í™©")
                usage = api_manager.get_usage_stats()
                
                total_cost = sum(usage['costs'].values())
                st.metric("ì´ ë¹„ìš©", f"${total_cost:.4f}")
                
                for model, cost in usage['costs'].items():
                    if model != 'total':
                        st.caption(f"{model}: ${cost:.4f}")
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
