# íŒŒì¼ ì´ë¦„: app_manual.py (ìµœì¢… ìˆ˜ì •ë³¸)

import streamlit as st
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import json
import openai
import os

# 1. í˜ì´ì§€ ì œëª© ë³€ê²½
st.set_page_config(page_title="ì „ëµê¸°íšë¶€ AI ë‹µë³€ ì±—ë´‡", page_icon="ğŸ¢", layout="centered")

# --- API í‚¤ ì„¤ì • (Streamlitì˜ Secrets ê´€ë¦¬ ê¸°ëŠ¥ ì‚¬ìš©) ---
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except Exception as e:
    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secretsì— í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.")

# --- ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© (ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”) ---
@st.cache_resource
def load_models_and_data():
    """ì‚¬ì „ì— ì¤€ë¹„ëœ ë°ì´í„° íŒŒì¼ê³¼, ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        model_path = "./models/ko-sroberta-multitask"
        model = SentenceTransformer(model_path)
        index = faiss.read_index("manuals_vector_db.index")
        with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
            chunks_with_metadata = json.load(f)
        return model, index, chunks_with_metadata
    except FileNotFoundError:
        return None, None, None

def get_relevant_manual_chunks(user_question, k=5):
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë§¤ë‰´ì–¼ ë¬¸ë‹¨ kê°œë¥¼ DBì—ì„œ ì°¾ìŠµë‹ˆë‹¤."""
    question_vector = model.encode([user_question])
    distances, indices = index.search(np.array(question_vector, dtype=np.float32), k)
    relevant_chunks = [chunks_with_metadata[i] for i in indices[0]]
    return relevant_chunks

def generate_answer_with_llm(user_question, relevant_chunks):
    """ê²€ìƒ‰ëœ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ê·¼ê±°ë¡œ LLM(GPT)ì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    context_str = ""
    for chunk in relevant_chunks:
        context_str += f"ì¶œì²˜: {chunk['source']}\në‚´ìš©: {chunk['content']}\n\n"
    
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ì‚¬ë‚´ ë§¤ë‰´ì–¼ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì£¼ì–´ì§„ [ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´] ë‚´ì—ì„œë§Œ ë‹µë³€í•˜ê³ , ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
        {"role": "user", "content": f"[ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´]\n{context_str}\n---\n[ì§ˆë¬¸]\n{user_question}"}
    ]
    
    try:
        response = openai.chat.completions.create(model="gpt-4o", messages=messages, temperature=0.7)
        return response.choices[0].message.content
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# 1. ë©”ì¸ íƒ€ì´í‹€ ë³€ê²½
st.title("ğŸ¢ ì „ëµê¸°íšë¶€ AI ë‹µë³€ ì±—ë´‡")

# ë°ì´í„° ë¡œë”© ì‹œë„
model, index, chunks_with_metadata = load_models_and_data()

if model is None:
    st.error("ì±—ë´‡ ë°ì´í„° íŒŒì¼('prepare_pdfs.py' ì‹¤í–‰ í•„ìš”)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.success("ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤!", icon="âœ…")
    
    st.markdown("íšŒì‚¬ ë‚´ë¶€ ë§¤ë‰´ì–¼ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
    user_question = st.text_input("ì§ˆë¬¸ ì…ë ¥:", placeholder="ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸°ì¤€ ê¸ˆì•¡ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")

    if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
        if user_question:
            with st.spinner("AIê°€ ë§¤ë‰´ì–¼ì„ ê²€í† í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                relevant_chunks = get_relevant_manual_chunks(user_question)
                answer = generate_answer_with_llm(user_question, relevant_chunks)
                
                # 3. ë‹µë³€ ë§ˆì§€ë§‰ì— ê²½ê³  ë¬¸êµ¬ ì¶”ê°€
                warning_message = "\n\n---\nâ—†ë³¸ ë‹µë³€ì€ ì „ëµê¸°íšë¶€ê°€ í•™ìŠµì‹œí‚¨ AIë¥¼ í†µí•´ ì œê³µí•˜ëŠ” ë‹µë³€ìœ¼ë¡œ, ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                answer += warning_message

                # 2. AI ë‹µë³€ í—¤ë” ë³€ê²½
                st.markdown("#### ğŸ’¬ ì „ëµê¸°íšë¶€ AI ë‹µë³€")
                st.write(answer)

                st.markdown("---")
                with st.expander("AIê°€ ì°¸ê³ í•œ ë§¤ë‰´ì–¼ ë‚´ìš© ë³´ê¸°"):
                    sources = sorted(list(set([chunk['source'] for chunk in relevant_chunks])))
                    st.markdown(f"**ì°¸ê³  ë§¤ë‰´ì–¼:** {', '.join(sources)}")
                    st.json(relevant_chunks)
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()
    st.caption("ì£¼ì˜: ì´ ì±—ë´‡ì˜ ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ìµœì¢… í™•ì¸ì€ ê³µì‹ ë¬¸ì„œë¥¼ í†µí•´ ì§„í–‰í•´ì£¼ì„¸ìš”.")
