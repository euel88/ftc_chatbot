# íŒŒì¼ ì´ë¦„: app_manual.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - ê°œì„ ëœ ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set, TypedDict, Protocol, Iterator, Generator, OrderedDict, Any, Union
import re
from collections import defaultdict, Counter, OrderedDict
import time
from dataclasses import dataclass, field
import os
import hashlib
from enum import Enum
import asyncio
import logging
from contextlib import contextmanager
import traceback
import gc
import concurrent.futures
from functools import lru_cache
from datetime import datetime, timedelta

# ===== ë¡œê¹… ì„¤ì • =====
def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •
    
    ë¡œê¹…ì€ í”„ë¡œê·¸ë¨ì˜ ì‘ë™ ìƒí™©ì„ ê¸°ë¡í•˜ëŠ” ì¼ê¸°ì¥ê³¼ ê°™ìŠµë‹ˆë‹¤.
    ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë•Œ ì›ì¸ì„ ì°¾ê±°ë‚˜, ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.
    """
    logger = logging.getLogger('ftc_chatbot')
    logger.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ - ì˜êµ¬ì ì¸ ë¡œê·¸ ê¸°ë¡
    file_handler = logging.FileHandler('ftc_chatbot_debug.log')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ - ê°œë°œ ì¤‘ ì‹¤ì‹œê°„ í™•ì¸
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# ===== ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ í´ë˜ìŠ¤ =====
class RAGPipelineError(Exception):
    """RAG íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

class IndexError(RAGPipelineError):
    """ì¸ë±ìŠ¤ ê´€ë ¨ ì˜¤ë¥˜ - FAISS ì¸ë±ìŠ¤ ë¬¸ì œ ì‹œ ë°œìƒ"""
    pass

class EmbeddingError(RAGPipelineError):
    """ì„ë² ë”© ìƒì„± ê´€ë ¨ ì˜¤ë¥˜ - ë²¡í„° ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë°œìƒ"""
    pass

class GPTAnalysisError(RAGPipelineError):
    """GPT ë¶„ì„ ì‹¤íŒ¨ ê´€ë ¨ ì˜¤ë¥˜ - API í˜¸ì¶œ ë¬¸ì œ ì‹œ ë°œìƒ"""
    pass

class ModelSelectionError(RAGPipelineError):
    """ëª¨ë¸ ì„ íƒ ê´€ë ¨ ì˜¤ë¥˜ - ì ì ˆí•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒ"""
    pass

# ===== ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € =====
@contextmanager
def error_context(operation_name: str, fallback_value=None):
    """ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    
    ì´ëŠ” ë§ˆì¹˜ ì‘ì—…ì¥ì˜ ì•ˆì „ë§ê³¼ ê°™ì•„ì„œ, ì‘ì—… ì¤‘ ë¬¸ì œê°€ ë°œìƒí•´ë„
    ì „ì²´ ì‹œìŠ¤í…œì´ ë©ˆì¶”ì§€ ì•Šê³  ì ì ˆíˆ ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """
    try:
        logger.debug(f"Starting operation: {operation_name}")
        yield
    except Exception as e:
        logger.error(f"Error in {operation_name}: {str(e)}")
        logger.debug(f"Full traceback:\n{traceback.format_exc()}")
        
        # Streamlit UIì— ì—ëŸ¬ í‘œì‹œ
        if 'st' in globals():
            st.error(f"âš ï¸ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {operation_name}")
            with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                st.code(traceback.format_exc())
        
        # í´ë°± ê°’ì´ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ì˜ˆì™¸ ì¬ë°œìƒ
        if fallback_value is not None:
            return fallback_value
        raise

# ===== í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ =====
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS ìŠ¤íƒ€ì¼ - UIì˜ ì‹œê°ì  ë””ìì¸ì„ ì •ì˜
st.markdown("""
<style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display: none;}
    
    /* ë©”ì¸ í—¤ë” ìŠ¤íƒ€ì¼ */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 900px;
        margin: 0 auto;
    }
    
    /* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ ê°œì„  */
    [data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* ë³µì¡ë„ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .complexity-indicator {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 16px;
        font-size: 0.85rem;
        font-weight: 500;
        margin-left: 8px;
    }
    
    .complexity-simple {
        background-color: #d4edda;
        color: #155724;
    }
    
    .complexity-medium {
        background-color: #fff3cd;
        color: #856404;
    }
    
    .complexity-complex {
        background-color: #f8d7da;
        color: #721c24;
    }
    
    /* ê²½ê³  ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .version-warning {
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        color: #856404;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    
    /* ì¤‘ìš”ë„ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .importance-indicator {
        display: inline-block;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.8rem;
        margin-left: 4px;
    }
    
    .importance-critical {
        background-color: #dc3545;
        color: white;
    }
    
    .importance-high {
        background-color: #fd7e14;
        color: white;
    }
    
    .importance-normal {
        background-color: #6c757d;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# ===== íƒ€ì… ì •ì˜ =====
class ChunkDict(TypedDict):
    """ì²­í¬ ë°ì´í„°ì˜ íƒ€ì… ì •ì˜
    
    ì²­í¬ëŠ” ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆˆ ê²ƒì…ë‹ˆë‹¤.
    ê° ì²­í¬ëŠ” ê²€ìƒ‰ê³¼ ì°¸ì¡°ê°€ ê°€ëŠ¥í•œ ë…ë¦½ì ì¸ ì •ë³´ ë‹¨ìœ„ì…ë‹ˆë‹¤.
    """
    chunk_id: str
    content: str
    source: str
    page: int
    chunk_type: str
    metadata: str

class AnalysisResult(TypedDict):
    """GPT ë¶„ì„ ê²°ê³¼ì˜ íƒ€ì… ì •ì˜"""
    query_analysis: dict
    legal_concepts: list
    search_strategy: dict
    answer_requirements: dict

# ===== ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜ ë„ì„œê´€ì—ì„œ ì°¾ì€ ì±…ì˜ ì •ë³´ ì¹´ë“œì™€ ê°™ìŠµë‹ˆë‹¤.
    ì–´ë–¤ ë‚´ìš©ì´ ì–´ë””ì— ìˆëŠ”ì§€, ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    """
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')
    
    @property
    def is_latest(self) -> bool:
        """ìµœì‹  ìë£Œ ì—¬ë¶€ í™•ì¸"""
        return self.metadata.get('is_latest', False)

class QueryComplexity(Enum):
    """ì§ˆë¬¸ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"

class QueryImportance(Enum):
    """ì§ˆë¬¸ ì¤‘ìš”ë„ ë ˆë²¨ - ë²•ì  ë¦¬ìŠ¤í¬ì™€ ê¸ˆì•¡ì„ ê³ ë ¤"""
    CRITICAL = "critical"  # ë²•ì  ë¦¬ìŠ¤í¬ê°€ ë†’ê±°ë‚˜ ëŒ€ê·œëª¨ ê¸ˆì•¡
    HIGH = "high"         # ì¤‘ìš”í•œ ë²•ì  íŒë‹¨ì´ í•„ìš”
    NORMAL = "normal"     # ì¼ë°˜ì ì¸ ì§ˆë¬¸

# ===== LRU ìºì‹œ êµ¬í˜„ =====
class LRUCache:
    """ì‹œê°„ ê¸°ë°˜ ë§Œë£Œë¥¼ ì§€ì›í•˜ëŠ” LRU ìºì‹œ êµ¬í˜„
    
    LRU(Least Recently Used) ìºì‹œëŠ” ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©í•˜ì§€ ì•Šì€ í•­ëª©ì„
    ìë™ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë˜‘ë˜‘í•œ ë³´ê´€í•¨ì…ë‹ˆë‹¤. ìì£¼ ì‚¬ìš©í•˜ëŠ” ì •ë³´ëŠ”
    ë¹ ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë©”ëª¨ë¦¬ì— ë³´ê´€í•©ë‹ˆë‹¤.
    """
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl  # Time To Live (ì´ˆ ë‹¨ìœ„)
        
    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ë§Œë£Œëœ í•­ëª©ì€ ì œê±°"""
        if key not in self.cache:
            return None
            
        value, timestamp = self.cache[key]
        # ë§Œë£Œ ì‹œê°„ í™•ì¸
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
            
        # ìµœê·¼ ì‚¬ìš©ëœ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™ (LRU êµ¬í˜„)
        self.cache.move_to_end(key)
        return value
        
    def put(self, key: str, value):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if key in self.cache:
            del self.cache[key]
            
        # ìºì‹œê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(self.cache) >= self.max_size:
            self.cache.popitem(last=False)
            
        self.cache[key] = (value, time.time())
        
    def clear_expired(self):
        """ë§Œë£Œëœ ëª¨ë“  í•­ëª© ì œê±° - ì£¼ê¸°ì ì¸ ì •ë¦¬ë¥¼ ìœ„í•´"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]

# ===== ë¹„ë™ê¸° ì‹¤í–‰ í—¬í¼ =====
def run_async_in_streamlit(coro):
    """Streamlit í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
    
    Streamlitì€ ê¸°ë³¸ì ìœ¼ë¡œ ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ë¯€ë¡œ,
    ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ê°€ ê·¸ ë‹¤ë¦¬ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    try:
        loop = asyncio.get_running_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, coro)
            return future.result()
    except RuntimeError:
        return asyncio.run(coro)

# ===== ë¬¸ì„œ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ =====
class DocumentVersionManager:
    """ë¬¸ì„œì˜ ë²„ì „ê³¼ ìµœì‹ ì„±ì„ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ë²•ë¥ ì€ ì‹œê°„ì— ë”°ë¼ ë³€ê²½ë˜ë¯€ë¡œ, ì–´ë–¤ ì •ë³´ê°€ ìµœì‹ ì¸ì§€
    íŒŒì•…í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜
    ë„ì„œê´€ì˜ ê°œì •íŒ ê´€ë¦¬ ì‹œìŠ¤í…œê³¼ ê°™ì€ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ì£¼ìš” ê·œì • ë³€ê²½ ì´ë ¥
        self.regulation_changes = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜_ê¸ˆì•¡ê¸°ì¤€': [
                {
                    'date': '2023-01-01', 
                    'old_value': '50ì–µì›', 
                    'new_value': '100ì–µì›',
                    'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 100ì–µì› ì´ìƒ'
                },
                {
                    'date': '2020-01-01', 
                    'old_value': '30ì–µì›', 
                    'new_value': '50ì–µì›',
                    'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 50ì–µì› ì´ìƒ'
                }
            ],
            'ê³µì‹œ_ê¸°í•œ': [
                {
                    'date': '2022-07-01', 
                    'old_value': '7ì¼', 
                    'new_value': '5ì¼',
                    'description': 'ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œ ë‹¨ì¶•'
                }
            ]
        }
        
        # ì¤‘ìš” ì •ë³´ íŒ¨í„´ ì •ì˜
        self.critical_patterns = {
            'ê¸ˆì•¡': r'(\d+)ì–µ\s*ì›',
            'ë¹„ìœ¨': r'(\d+(?:\.\d+)?)\s*%',
            'ê¸°í•œ': r'(\d+)\s*ì¼',
            'ë‚ ì§œ': r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        }
    
    def extract_document_date(self, chunk: Dict) -> Optional[str]:
        """ë¬¸ì„œì—ì„œ ì‘ì„±/ê°œì • ë‚ ì§œ ì¶”ì¶œ
        
        ë¬¸ì„œì˜ ë‚ ì§œëŠ” ê·¸ ë¬¸ì„œì˜ ìœ íš¨ì„±ì„ íŒë‹¨í•˜ëŠ” ì¤‘ìš”í•œ ê¸°ì¤€ì…ë‹ˆë‹¤.
        ì—¬ëŸ¬ íŒ¨í„´ì„ ì‚¬ìš©í•´ ë‚ ì§œë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        """
        content = chunk.get('content', '')
        metadata = json.loads(chunk.get('metadata', '{}'))
        
        # ë©”íƒ€ë°ì´í„°ì— ë‚ ì§œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if 'document_date' in metadata:
            return metadata['document_date']
        
        # ë‚´ìš©ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*ê°œì •',
            r'ì‹œí–‰ì¼\s*:\s*(\d{4})ë…„\s*(\d{1,2})ì›”',
            r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                return self._normalize_date(match.group(0))
        
        return None
    
    def _normalize_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹(YYYY-MM-DD)ìœ¼ë¡œ ë³€í™˜"""
        # ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ìë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€ê²½
        date_str = re.sub(r'[^\d]', '-', date_str)
        parts = date_str.split('-')
        if len(parts) >= 2:
            year = parts[0] if len(parts[0]) == 4 else '20' + parts[0]
            month = parts[1].zfill(2)
            day = parts[2].zfill(2) if len(parts) > 2 else '01'
            return f"{year}-{month}-{day}"
        return None
    
    def check_for_outdated_info(self, content: str, document_date: str = None) -> List[Dict]:
        """êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        ì´ëŠ” ë§ˆì¹˜ ì‹í’ˆì˜ ìœ í†µê¸°í•œì„ í™•ì¸í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
        ì˜¤ë˜ëœ ì •ë³´ëŠ” ì‚¬ìš©ìì—ê²Œ í•´ê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
        ì‹ ì¤‘í•˜ê²Œ í™•ì¸í•˜ê³  ê²½ê³ í•©ë‹ˆë‹¤.
        """
        warnings = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ í™•ì¸
        amount_match = re.search(r'(\d+)ì–µ\s*ì›.*ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', content)
        if amount_match:
            amount = int(amount_match.group(1))
            if amount == 50:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '50ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
            elif amount == 30:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '30ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
        
        # ê³µì‹œ ê¸°í•œ í™•ì¸
        deadline_match = re.search(r'ì´ì‚¬íšŒ.*ì˜ê²°.*(\d+)ì¼', content)
        if deadline_match:
            days = int(deadline_match.group(1))
            if days == 7:
                warnings.append({
                    'type': 'outdated_deadline',
                    'found': '7ì¼',
                    'current': '5ì¼',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œ',
                    'changed_date': '2022-07-01',
                    'severity': 'high'
                })
        
        return warnings

# ===== ì¶©ëŒ í•´ê²° ì‹œìŠ¤í…œ =====
class ConflictResolver:
    """ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì œê³µí•  ë•Œ,
    ì–´ë–¤ ê²ƒì´ ë§ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.
    ì´ í´ë˜ìŠ¤ëŠ” ë§ˆì¹˜ ì¬íŒê´€ê³¼ ê°™ì€ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, version_manager: DocumentVersionManager):
        self.version_manager = version_manager
    
    def resolve_conflicts(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ
        
        ì´ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤:
        1. ê° ê²°ê³¼ì˜ ë‚ ì§œì™€ ë‚´ìš©ì„ í™•ì¸
        2. êµ¬ë²„ì „ ì •ë³´ê°€ ìˆëŠ”ì§€ ê²€ì‚¬
        3. ì¶©ëŒí•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì•„ë‚´ê¸°
        4. ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì¬ì •ë ¬
        """
        
        # ê° ê²°ê³¼ì— ëŒ€í•´ êµ¬ë²„ì „ ì •ë³´ í™•ì¸
        for result in results:
            doc_date = result.document_date
            warnings = self.version_manager.check_for_outdated_info(result.content, doc_date)
            
            if warnings:
                result.metadata['warnings'] = warnings
                result.metadata['has_outdated_info'] = True
            else:
                result.metadata['has_outdated_info'] = False
        
        # ì¤‘ìš” ì •ë³´ ì¶”ì¶œ ë° ì¶©ëŒ í™•ì¸
        critical_info = self._extract_critical_info(results, query)
        if critical_info:
            conflicts = self._find_conflicts(critical_info)
            if conflicts:
                results = self._prioritize_latest_info(results, conflicts)
        
        # ìµœì¢… ì •ë ¬: ìµœì‹  ì •ë³´ì™€ ë†’ì€ ì ìˆ˜ë¥¼ ìš°ì„ ì‹œ
        results.sort(key=lambda r: (
            not r.metadata.get('has_outdated_info', False),  # ìµœì‹  ì •ë³´ ìš°ì„ 
            r.document_date or '1900-01-01',  # ìµœì‹  ë‚ ì§œ ìš°ì„ 
            r.score  # ê´€ë ¨ì„± ì ìˆ˜
        ), reverse=True)
        
        return results
    
    def _extract_critical_info(self, results: List[SearchResult], query: str) -> Dict:
        """ê²°ê³¼ì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
        
        ê¸ˆì•¡, ë¹„ìœ¨, ê¸°í•œ ë“± ë²•ì ìœ¼ë¡œ ì¤‘ìš”í•œ ìˆ˜ì¹˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        critical_info = defaultdict(list)
        
        for i, result in enumerate(results):
            # ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ
            amounts = re.findall(r'(\d+)ì–µ\s*ì›', result.content)
            for amount in amounts:
                critical_info['amounts'].append({
                    'value': amount + 'ì–µì›',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            # ë¹„ìœ¨ ì •ë³´ ì¶”ì¶œ
            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', result.content)
            for pct in percentages:
                critical_info['percentages'].append({
                    'value': pct + '%',
                    'result_index': i,
                    'context': result.content[:100]
                })
        
        return dict(critical_info)
    
    def _find_conflicts(self, critical_info: Dict) -> List[Dict]:
        """ì¤‘ìš” ì •ë³´ ê°„ ì¶©ëŒ ì°¾ê¸°"""
        conflicts = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ ì¶©ëŒ í™•ì¸
        if 'amounts' in critical_info:
            amount_values = set()
            for item in critical_info['amounts']:
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in item['context']:
                    amount_values.add(item['value'])
            
            # ì—¬ëŸ¬ ë‹¤ë¥¸ ê¸ˆì•¡ì´ ì–¸ê¸‰ë˜ê³ , ê·¸ ì¤‘ êµ¬ë²„ì „ì´ ìˆëŠ” ê²½ìš°
            if len(amount_values) > 1 and ('50ì–µì›' in amount_values or '30ì–µì›' in amount_values):
                conflicts.append({
                    'type': 'amount_conflict',
                    'values': list(amount_values),
                    'correct_value': '100ì–µì›'
                })
        
        return conflicts
    
    def _prioritize_latest_info(self, results: List[SearchResult], conflicts: List[Dict]) -> List[SearchResult]:
        """ì¶©ëŒì´ ìˆì„ ë•Œ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ
        
        êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ëœ ê²°ê³¼ì˜ ì ìˆ˜ë¥¼ ë‚®ì¶°ì„œ
        ìì—°ìŠ¤ëŸ½ê²Œ í•˜ìœ„ë¡œ ë°€ë ¤ë‚˜ê²Œ í•©ë‹ˆë‹¤.
        """
        for conflict in conflicts:
            if conflict['type'] == 'amount_conflict':
                for i, result in enumerate(results):
                    # êµ¬ë²„ì „ ê¸ˆì•¡ì´ í¬í•¨ëœ ê²½ìš° ì ìˆ˜ ê°ì†Œ
                    if any(old_val in result.content for old_val in ['50ì–µì›', '30ì–µì›']):
                        results[i].score *= 0.5  # ì ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ
                        results[i].metadata['score_reduced'] = True
                        results[i].metadata['reduction_reason'] = 'outdated_amount'
        
        return results

# ===== ì¤‘ìš”ë„ í‰ê°€ ì‹œìŠ¤í…œ =====
class ImportanceAssessor:
    """ì§ˆë¬¸ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ë²•ì  ë¦¬ìŠ¤í¬ì™€ ê¸ˆì•¡ì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì˜ ì¤‘ìš”ë„ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
    ì´ëŠ” ì‘ê¸‰ì‹¤ì—ì„œ í™˜ìì˜ ì¤‘ì¦ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì›ë¦¬ì…ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ë²•ì  ë¦¬ìŠ¤í¬ í‚¤ì›Œë“œ
        self.legal_risk_keywords = {
            'critical': ['ê³µì‹œì˜ë¬´', 'ì´ì‚¬íšŒì˜ê²°', 'ë²•ì ì±…ì„', 'ì œì¬', 'ê³¼íƒœë£Œ', 'ìœ„ë°˜', 'ì²˜ë²Œ'],
            'high': ['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 'ê³„ì—´ì‚¬ê±°ë˜', 'íŠ¹ìˆ˜ê´€ê³„ì¸', 'ê³µì •ê±°ë˜ë²•', 'ì‹ ê³ ì˜ë¬´'],
            'medium': ['ì ˆì°¨', 'ê¸°í•œ', 'ë³´ê³ ', 'ì œì¶œ', 'í†µì§€']
        }
        
        # ê¸ˆì•¡ ì„ê³„ê°’ (ì–µì›)
        self.amount_thresholds = {
            'critical': 100,  # 100ì–µì› ì´ìƒ
            'high': 50,       # 50ì–µì› ì´ìƒ
            'medium': 10      # 10ì–µì› ì´ìƒ
        }
    
    def assess_importance(self, query: str) -> Tuple[QueryImportance, Dict]:
        """ì§ˆë¬¸ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€
        
        ê¸ˆì•¡ê³¼ ë²•ì  ë¦¬ìŠ¤í¬ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬
        ì§ˆë¬¸ì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        """
        importance_score = 0
        factors = []
        
        query_lower = query.lower()
        
        # ê¸ˆì•¡ í™•ì¸
        amount_matches = re.findall(r'(\d+)ì–µ\s*ì›', query)
        max_amount = 0
        for amount_str in amount_matches:
            amount = int(amount_str)
            max_amount = max(max_amount, amount)
            
            if amount >= self.amount_thresholds['critical']:
                importance_score += 10
                factors.append(f"{amount}ì–µì› - ëŒ€ê·œëª¨ ê±°ë˜")
            elif amount >= self.amount_thresholds['high']:
                importance_score += 7
                factors.append(f"{amount}ì–µì› - ì¤‘ê·œëª¨ ê±°ë˜")
            elif amount >= self.amount_thresholds['medium']:
                importance_score += 4
                factors.append(f"{amount}ì–µì› ê±°ë˜")
        
        # ë²•ì  ë¦¬ìŠ¤í¬ í‚¤ì›Œë“œ í™•ì¸
        for level, keywords in self.legal_risk_keywords.items():
            for keyword in keywords:
                if keyword in query_lower:
                    if level == 'critical':
                        importance_score += 8
                        factors.append(f"í•µì‹¬ ë²•ì  ê°œë…: {keyword}")
                    elif level == 'high':
                        importance_score += 5
                        factors.append(f"ì¤‘ìš” ë²•ì  ê°œë…: {keyword}")
                    else:
                        importance_score += 2
                        factors.append(f"ë²•ì  ê°œë…: {keyword}")
        
        # ë³µí•© ê±°ë˜ ì—¬ë¶€
        if 'ë™ì‹œì—' in query_lower or ('ê·¸ë¦¬ê³ ' in query_lower and 'ê±°ë˜' in query_lower):
            importance_score += 3
            factors.append("ë³µí•© ê±°ë˜ ë¶„ì„ í•„ìš”")
        
        # ì¤‘ìš”ë„ ë ˆë²¨ ê²°ì •
        if importance_score >= 15 or max_amount >= self.amount_thresholds['critical']:
            importance = QueryImportance.CRITICAL
        elif importance_score >= 8 or max_amount >= self.amount_thresholds['high']:
            importance = QueryImportance.HIGH
        else:
            importance = QueryImportance.NORMAL
        
        return importance, {
            'score': importance_score,
            'factors': factors,
            'max_amount': max_amount
        }

# ===== ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ =====
class OptimizedModelSelector:
    """ì§ˆë¬¸ì˜ ì¤‘ìš”ë„ì™€ ë³µì¡ë„ë¥¼ ê³ ë ¤í•œ ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ
    
    ì´ ì‹œìŠ¤í…œì€ ë§ˆì¹˜ ë³‘ì›ì—ì„œ í™˜ìì˜ ìƒíƒœì— ë”°ë¼
    ì ì ˆí•œ ì˜ë£Œì§„ì„ ë°°ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    ì¤‘ìš”í•œ ì§ˆë¬¸ì—ëŠ” ë” ì •í™•í•œ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.model_profiles = {
            'gpt-4o-mini': {
                'accuracy_score': 0.65,
                'speed_score': 0.95,
                'best_for': ['simple_facts', 'basic_queries'],
                'description': 'ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸ì— ìµœì í™”ëœ ê²½ì œì  ëª¨ë¸'
            },
            'o4-mini': {
                'accuracy_score': 0.75,
                'speed_score': 0.93,
                'best_for': ['standard_queries', 'basic_analysis'],
                'description': 'í‘œì¤€ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë¹ ë¥¸ ì²˜ë¦¬'
            },
            'o3-mini': {
                'accuracy_score': 0.78,
                'speed_score': 0.90,
                'best_for': ['intermediate_queries', 'moderate_complexity'],
                'description': 'ì¤‘ê°„ ë³µì¡ë„ì˜ ë¶„ì„ì´ ê°€ëŠ¥í•œ ëª¨ë¸'
            },
            'gpt-4o': {
                'accuracy_score': 0.90,
                'speed_score': 0.80,
                'best_for': ['complex_analysis', 'legal_interpretation', 'critical_decisions'],
                'description': 'ë³µì¡í•œ ë²•ë¥  í•´ì„ê³¼ ì¤‘ìš”í•œ íŒë‹¨ì— ìµœì í™”ëœ ëª¨ë¸'
            }
        }
    
    def select_model(self, query: str, complexity: QueryComplexity, 
                    importance: QueryImportance, initial_analysis: Dict) -> Tuple[str, Dict]:
        """ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒ
        
        ì¤‘ìš”ë„ê°€ ë†’ì€ ì§ˆë¬¸ì€ ë³µì¡ë„ì™€ ê´€ê³„ì—†ì´ ìƒìœ„ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        ì´ëŠ” ìˆ˜ìˆ ì˜ ë‚œì´ë„ì™€ ê´€ê³„ì—†ì´ ì¤‘ìš”í•œ ìˆ˜ìˆ ì€
        ê²½í—˜ ë§ì€ ì˜ì‚¬ê°€ ì§‘ë„í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì›ë¦¬ì…ë‹ˆë‹¤.
        """
        
        # ì¤‘ìš”ë„ê°€ CRITICALì¸ ê²½ìš° ë¬´ì¡°ê±´ ìµœê³  ëª¨ë¸ ì‚¬ìš©
        if importance == QueryImportance.CRITICAL:
            selected_model = 'gpt-4o'
            reason = "ë²•ì  ë¦¬ìŠ¤í¬ê°€ ë†’ê±°ë‚˜ ëŒ€ê·œëª¨ ê¸ˆì•¡ì´ ê´€ë ¨ëœ ì¤‘ìš”í•œ ì§ˆë¬¸"
        
        # ì¤‘ìš”ë„ê°€ HIGHì¸ ê²½ìš° ìƒìœ„ ëª¨ë¸ ì‚¬ìš©
        elif importance == QueryImportance.HIGH:
            if complexity == QueryComplexity.COMPLEX:
                selected_model = 'gpt-4o'
                reason = "ì¤‘ìš”í•˜ê³  ë³µì¡í•œ ë²•ë¥  í•´ì„ì´ í•„ìš”í•œ ì§ˆë¬¸"
            else:
                selected_model = 'o3-mini'
                reason = "ì¤‘ìš”ë„ê°€ ë†’ì€ í‘œì¤€ì ì¸ ì§ˆë¬¸"
        
        # ì¼ë°˜ì ì¸ ê²½ìš° ë³µì¡ë„ì— ë”°ë¼ ì„ íƒ
        else:
            if complexity == QueryComplexity.SIMPLE:
                selected_model = 'gpt-4o-mini'
                reason = "ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸ ë˜ëŠ” ê¸°ë³¸ ì •ë³´ ìš”ì²­"
            elif complexity == QueryComplexity.MEDIUM:
                selected_model = 'o4-mini'
                reason = "í‘œì¤€ì ì¸ ì ˆì°¨ë‚˜ ê·œì •ì— ëŒ€í•œ ì§ˆë¬¸"
            else:
                selected_model = 'o3-mini'
                reason = "ë³µì¡í•˜ì§€ë§Œ ì¤‘ìš”ë„ê°€ ë‚®ì€ ì§ˆë¬¸"
        
        selection_info = {
            'model': selected_model,
            'reason': reason,
            'complexity': complexity.value,
            'importance': importance.value,
            'accuracy_score': self.model_profiles[selected_model]['accuracy_score']
        }
        
        return selected_model, selection_info

# ===== ìºì‹± ì „ëµ ì‹œìŠ¤í…œ =====
class SmartCacheStrategy:
    """ëª¨ë¸ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ìºì‹± ì „ëµ
    
    ìºì‹±ì€ ìì£¼ ì°¾ëŠ” ì±…ì„ ì±…ìƒ ìœ„ì— ì˜¬ë ¤ë†“ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    í•œ ë²ˆ ì°¾ì€ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´
    ì‹œê°„ê³¼ ë¹„ìš©ì„ í¬ê²Œ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.cache_configs = {
            'gpt-4o-mini': {
                'ttl': 7200,  # 2ì‹œê°„ - ì €ë ´í•˜ë¯€ë¡œ ì˜¤ë˜ ë³´ê´€
                'similarity_threshold': 0.85
            },
            'o4-mini': {
                'ttl': 5400,  # 1.5ì‹œê°„
                'similarity_threshold': 0.88
            },
            'o3-mini': {
                'ttl': 3600,  # 1ì‹œê°„
                'similarity_threshold': 0.90
            },
            'gpt-4o': {
                'ttl': 1800,  # 30ë¶„ - ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ìì£¼ ì—…ë°ì´íŠ¸
                'similarity_threshold': 0.95
            }
        }
        
        # ì§ˆë¬¸-ë‹µë³€ ìºì‹œ
        self.query_cache = LRUCache(max_size=200, ttl=7200)
        # ì„ë² ë”© ìºì‹œ
        self.embedding_cache = LRUCache(max_size=500, ttl=10800)
        # ë¶„ì„ ê²°ê³¼ ìºì‹œ
        self.analysis_cache = LRUCache(max_size=100, ttl=3600)
    
    def get_cached_result(self, query: str) -> Optional[Dict]:
        """ìºì‹œëœ ê²°ê³¼ ë°˜í™˜"""
        cache_key = self._generate_cache_key(query)
        return self.query_cache.get(cache_key)
    
    def store_result(self, query: str, result: Dict, model: str, importance: QueryImportance):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        
        ì¤‘ìš”í•œ ì§ˆë¬¸ì˜ ê²°ê³¼ëŠ” ìºì‹±í•˜ì§€ ì•Šê±°ë‚˜ ì§§ê²Œ ë³´ê´€í•©ë‹ˆë‹¤.
        ì´ëŠ” ì¤‘ìš”í•œ ì •ë³´ì¼ìˆ˜ë¡ ìµœì‹ ì„±ì´ ë” ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        """
        # CRITICAL ì¤‘ìš”ë„ì˜ ì§ˆë¬¸ì€ ìºì‹±í•˜ì§€ ì•ŠìŒ
        if importance == QueryImportance.CRITICAL:
            return
        
        cache_key = self._generate_cache_key(query)
        result['cached_at'] = time.time()
        result['model'] = model
        self.query_cache.put(cache_key, result)
    
    def _generate_cache_key(self, query: str) -> str:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ê³ ìœ í•œ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(query.encode()).hexdigest()

# ===== GPT ì§ˆë¬¸ ë¶„ì„ê¸° =====
class AdaptiveQueryAnalyzer:
    """ì§ˆë¬¸ì˜ ì¤‘ìš”ë„ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ë” ì •êµí•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ì´ëŠ” ì¤‘ìš”í•œ ì„œë¥˜ëŠ” ë” ê¼¼ê¼¼íˆ ê²€í† í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì›ë¦¬ì…ë‹ˆë‹¤.
    """
    
    def __init__(self, cache_strategy: SmartCacheStrategy, importance_assessor: ImportanceAssessor):
        self.cache_strategy = cache_strategy
        self.importance_assessor = importance_assessor
    
    def analyze_and_strategize(self, query: str, available_chunks_info: Dict, 
                              importance: QueryImportance) -> Dict:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
        
        ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ë” ì •êµí•œ ë¶„ì„ì„ ìœ„í•´ ìƒìœ„ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(f"analysis_{query}".encode()).hexdigest()
        cached = self.cache_strategy.analysis_cache.get(cache_key)
        if cached:
            logger.info(f"Analysis cache hit for query: {query[:50]}...")
            return cached
        
        # ì¤‘ìš”ë„ì— ë”°ë¼ ë¶„ì„ ëª¨ë¸ ì„ íƒ
        if importance == QueryImportance.CRITICAL:
            analysis_model = "gpt-4o"  # ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ìµœê³  ëª¨ë¸ë¡œ ë¶„ì„
        else:
            analysis_model = "gpt-4o-mini"  # ì¼ë°˜ ì§ˆë¬¸ì€ ê²½ì œì  ëª¨ë¸ ì‚¬ìš©
        
        prompt = f"""
        ë‹¹ì‹ ì€ ê³µì •ê±°ë˜ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ì •ë³´:
        - ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼: {available_chunks_info.get('ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 0)}ê°œ ì²­í¬
        - í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼: {available_chunks_info.get('í˜„í™©ê³µì‹œ', 0)}ê°œ ì²­í¬
        - ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼: {available_chunks_info.get('ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­', 0)}ê°œ ì²­í¬
        
        ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "query_analysis": {{
                "core_intent": "ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ (í•œ ë¬¸ì¥)",
                "actual_complexity": "simple/medium/complex",
                "complexity_reason": "ë³µì¡ë„ íŒë‹¨ ì´ìœ "
            }},
            "legal_concepts": [
                {{
                    "concept": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜/í˜„í™©ê³µì‹œ/ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­",
                    "relevance": "primary/secondary",
                    "specific_aspects": ["ê¸ˆì•¡ê¸°ì¤€", "ì ˆì°¨", "ê³µì‹œì˜ë¬´"]
                }}
            ],
            "search_strategy": {{
                "approach": "direct_lookup/focused_search/comprehensive_analysis",
                "primary_manual": "ì£¼ë¡œ ê²€ìƒ‰í•  ë§¤ë‰´ì–¼",
                "search_keywords": ["í•µì‹¬ ê²€ìƒ‰ì–´1", "í•µì‹¬ ê²€ìƒ‰ì–´2"],
                "expected_chunks_needed": 10,
                "rationale": "ì´ ì „ëµì„ ì„ íƒí•œ ì´ìœ "
            }},
            "answer_requirements": {{
                "needs_specific_numbers": true,
                "needs_process_steps": false,
                "needs_timeline": false,
                "needs_exceptions": false,
                "needs_multiple_perspectives": false
            }}
        }}
        """
        
        try:
            response = openai.chat.completions.create(
                model=analysis_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            analysis = json.loads(response.choices[0].message.content)
            
            # ìºì‹œ ì €ì¥ (ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ìºì‹œí•˜ì§€ ì•ŠìŒ)
            if importance != QueryImportance.CRITICAL:
                self.cache_strategy.analysis_cache.put(cache_key, analysis)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Query analysis error: {str(e)}")
            return self._get_fallback_strategy(query)
    
    def _get_fallback_strategy(self, query: str) -> Dict:
        """GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì „ëµ"""
        return {
            "query_analysis": {
                "core_intent": "ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨ - ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰",
                "actual_complexity": "medium",
                "complexity_reason": "ìë™ ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¤‘ê°„ ë³µì¡ë„ ê°€ì •"
            },
            "legal_concepts": [],
            "search_strategy": {
                "approach": "focused_search",
                "primary_manual": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜",
                "search_keywords": query.split()[:5],
                "expected_chunks_needed": 30,
                "rationale": "ê¸°ë³¸ ê²€ìƒ‰ ì „ëµ"
            },
            "answer_requirements": {
                "needs_specific_numbers": True,
                "needs_process_steps": True,
                "needs_timeline": True,
                "needs_exceptions": False,
                "needs_multiple_perspectives": False
            }
        }

# ===== ì§ˆë¬¸ ë¶„ë¥˜ê¸° =====
class QuestionClassifier:
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë§¤ë‰´ì–¼ì„ ìš°ì„  ê²€ìƒ‰í• ì§€ ê²°ì •
    
    ì´ëŠ” ë„ì„œê´€ì—ì„œ ì–´ëŠ ì„¹ì…˜ìœ¼ë¡œ ê°€ì•¼ í• ì§€
    ì•ˆë‚´í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì˜ í‚¤ì›Œë“œì™€ íŒ¨í„´ì„
    ë¶„ì„í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë§¤ë‰´ì–¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.categories = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': {
                'keywords': ['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 'ë‚´ë¶€ê±°ë˜', 'ì´ì‚¬íšŒ ì˜ê²°', 'ì´ì‚¬íšŒ', 'ì˜ê²°', 
                           'ê³„ì—´ì‚¬', 'ê³„ì—´íšŒì‚¬', 'íŠ¹ìˆ˜ê´€ê³„ì¸', 'ìê¸ˆ', 'ëŒ€ì—¬', 'ì°¨ì…', 'ë³´ì¦',
                           'ìê¸ˆê±°ë˜', 'ìœ ê°€ì¦ê¶Œ', 'ìì‚°ê±°ë˜', '50ì–µ', 'ê±°ë˜ê¸ˆì•¡', '100ì–µ'],
                'patterns': [r'ì´ì‚¬íšŒ.*ì˜ê²°', r'ê³„ì—´.*ê±°ë˜', r'ë‚´ë¶€.*ê±°ë˜'],
                'manual_pattern': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜.*ë§¤ë‰´ì–¼',
                'priority': 1
            },
            'í˜„í™©ê³µì‹œ': {
                'keywords': ['í˜„í™©ê³µì‹œ', 'ê¸°ì—…ì§‘ë‹¨', 'ì†Œì†íšŒì‚¬', 'ë™ì¼ì¸', 'ì¹œì¡±', 
                           'ì§€ë¶„ìœ¨', 'ì„ì›', 'ìˆœí™˜ì¶œì', 'ìƒí˜¸ì¶œì', 'ì§€ë°°êµ¬ì¡°',
                           'ê³„ì—´í¸ì…', 'ê³„ì—´ì œì™¸', 'ì£¼ì£¼í˜„í™©', 'ì„ì›í˜„í™©'],
                'patterns': [r'ê¸°ì—…ì§‘ë‹¨.*í˜„í™©', r'ì†Œì†.*íšŒì‚¬', r'ì§€ë¶„.*ë³€ë™'],
                'manual_pattern': 'ê¸°ì—…ì§‘ë‹¨í˜„í™©ê³µì‹œ.*ë§¤ë‰´ì–¼',
                'priority': 2
            },
            'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': {
                'keywords': ['ë¹„ìƒì¥', 'ì¤‘ìš”ì‚¬í•­', 'ì£¼ì‹', 'ì–‘ë„', 'ì–‘ìˆ˜', 'í•©ë³‘', 
                           'ë¶„í• ', 'ì˜ì—…ì–‘ë„', 'ì„ì›ë³€ê²½', 'ì¦ì', 'ê°ì',
                           'ì •ê´€ë³€ê²½', 'í•´ì‚°', 'ì²­ì‚°'],
                'patterns': [r'ë¹„ìƒì¥.*ê³µì‹œ', r'ì£¼ì‹.*ì–‘ë„', r'ì¤‘ìš”.*ì‚¬í•­'],
                'manual_pattern': 'ë¹„ìƒì¥ì‚¬.*ì¤‘ìš”ì‚¬í•­.*ë§¤ë‰´ì–¼',
                'priority': 3
            }
        }
    
    def classify(self, question: str) -> Tuple[str, float]:
        """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì‹ ë¢°ë„ë¥¼ ë°˜í™˜
        
        ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ ,
        ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        ì‹ ë¢°ë„ëŠ” ì–¼ë§ˆë‚˜ í™•ì‹¤í•œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        """
        question_lower = question.lower()
        scores = {}
        
        for category, info in self.categories.items():
            score = 0
            matched_keywords = []
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ì¤‘ìš”ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬)
            for i, keyword in enumerate(info['keywords']):
                if keyword in question_lower:
                    weight = 1.0 if i < 5 else 0.7  # ì•ìª½ í‚¤ì›Œë“œê°€ ë” ì¤‘ìš”
                    score += weight
                    matched_keywords.append(keyword)
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in info.get('patterns', []):
                if re.search(pattern, question_lower):
                    score += 1.5
            
            scores[category] = score
        
        if scores:
            best_category = max(scores, key=scores.get)
            max_possible_score = len(self.categories[best_category]['keywords']) + \
                               len(self.categories[best_category].get('patterns', [])) * 1.5
            confidence = min(scores[best_category] / max_possible_score, 1.0)
            
            # ë„ˆë¬´ ë‚®ì€ ì‹ ë¢°ë„ëŠ” ë¶„ë¥˜í•˜ì§€ ì•ŠìŒ
            if confidence < 0.15:
                return None, 0.0
                
            return best_category, confidence
        
        return None, 0.0

# ===== ë³µì¡ë„ í‰ê°€ê¸° =====
class ComplexityAssessor:
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •
    
    ì´ëŠ” ìš”ë¦¬ì˜ ë‚œì´ë„ë¥¼ í‰ê°€í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    ì¬ë£Œì˜ ìˆ˜, ì¡°ë¦¬ ë‹¨ê³„, í•„ìš”í•œ ê¸°ìˆ  ë“±ì„
    ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ë‚œì´ë„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ë³µì¡ë„ ì§€í‘œë“¤
        self.simple_indicators = [
            r'ì–¸ì œ', r'ë©°ì¹ ', r'ê¸°í•œ', r'ë‚ ì§œ', r'ê¸ˆì•¡', r'%', r'ì–¼ë§ˆ',
            r'ì •ì˜[ê°€ëŠ”]?', r'ë¬´ì—‡', r'ëœ»[ì´ì€]?', r'ì˜ë¯¸[ê°€ëŠ”]?'
        ]
        
        self.complex_indicators = [
            r'ë™ì‹œì—', r'ì—¬ëŸ¬', r'ë³µí•©', r'ì—°ê´€', r'ì˜í–¥',
            r'ë§Œ[ì•½ì¼].*ê²½ìš°', r'[AB].*ë™ì‹œ.*[CD]', r'ê±°ë˜.*ì—¬ëŸ¬',
            r'ì „ì²´ì ', r'ì¢…í•©ì ', r'ë¶„ì„', r'ê²€í† ', r'í‰ê°€',
            r'ë¦¬ìŠ¤í¬', r'ìœ„í—˜', r'ëŒ€ì‘', r'ì „ëµ'
        ]
        
        self.medium_indicators = [
            r'ì–´ë–»ê²Œ', r'ë°©ë²•', r'ì ˆì°¨', r'ê³¼ì •',
            r'ì£¼ì˜', r'ì˜ˆì™¸', r'íŠ¹ë³„', r'ê³ ë ¤'
        ]
    
    def assess(self, query: str) -> Tuple[QueryComplexity, float, Dict]:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ê³  ê´€ë ¨ ì •ë³´ ë°˜í™˜"""
        query_lower = query.lower()
        
        # ê° ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°
        simple_score = sum(1 for pattern in self.simple_indicators 
                         if re.search(pattern, query_lower))
        complex_score = sum(2 for pattern in self.complex_indicators 
                          if re.search(pattern, query_lower))
        medium_score = sum(1.5 for pattern in self.medium_indicators 
                         if re.search(pattern, query_lower))
        
        # ê¸¸ì´ì— ë”°ë¥¸ ì¶”ê°€ ì ìˆ˜
        if len(query) > 150:
            complex_score += 2
        elif len(query) > 100:
            complex_score += 1
        elif len(query) < 30:
            simple_score += 0.5
            
        # íŠ¹ìˆ˜ íŒ¨í„´ í™•ì¸
        if re.search(r'[AB]íšŒì‚¬.*[CD]íšŒì‚¬', query_lower):
            complex_score += 2
        if '?' in query and query.count('?') > 1:
            complex_score += 1
            
        total_score = simple_score + medium_score + complex_score
        
        # ë³µì¡ë„ ë ˆë²¨ ê²°ì •
        if total_score == 0:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.5
        elif complex_score > simple_score * 3:
            complexity = QueryComplexity.COMPLEX
            confidence = min(complex_score / (total_score + 1), 0.9)
        elif simple_score > complex_score * 2:
            complexity = QueryComplexity.SIMPLE
            confidence = min(simple_score / (total_score + 1), 0.9)
        else:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.6
            
        # ì •ê·œí™”ëœ ì ìˆ˜ (0-1)
        normalized_score = min(total_score / 10, 1.0)
        
        analysis = {
            'simple_score': simple_score,
            'medium_score': medium_score,
            'complex_score': complex_score,
            'total_score': total_score,
            'normalized_score': normalized_score,
            'query_length': len(query)
        }
        
        return complexity, confidence, analysis

# ===== í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ =====
class OptimizedHybridRAGPipeline:
    """ì¤‘ìš”ë„ ê¸°ë°˜ ì ì‘ì  ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸
    
    ì´ í´ë˜ìŠ¤ëŠ” ì „ì²´ ì‹œìŠ¤í…œì˜ í•µì‹¬ì…ë‹ˆë‹¤.
    ì§ˆë¬¸ì˜ ì¤‘ìš”ë„ì— ë”°ë¼ ë‹¤ë¥¸ ìˆ˜ì¤€ì˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬
    ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤.
    """
    
    def __init__(self, embedding_model, reranker_model, index, chunks):
        # ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ ê²€ì¦
        if not chunks:
            raise ValueError("No chunks provided to HybridRAGPipeline")
        
        if index.ntotal == 0:
            raise ValueError("FAISS index is empty")
        
        # ì„ë² ë”© ì°¨ì› ê²€ì¦
        test_embedding = embedding_model.encode(["test"])
        if len(test_embedding[0]) != index.d:
            raise ValueError(f"Embedding dimension {len(test_embedding[0])} doesn't match index dimension {index.d}")
        
        # ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunks = chunks
        
        # ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œë“¤
        self.cache_strategy = SmartCacheStrategy()
        self.version_manager = DocumentVersionManager()
        self.conflict_resolver = ConflictResolver(self.version_manager)
        self.importance_assessor = ImportanceAssessor()
        
        self.classifier = QuestionClassifier()
        self.complexity_assessor = ComplexityAssessor()
        self.query_analyzer = AdaptiveQueryAnalyzer(self.cache_strategy, self.importance_assessor)
        self.model_selector = OptimizedModelSelector()
        
        # ë§¤ë‰´ì–¼ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.manual_indices = self._build_manual_indices()
        
        # ì²­í¬ ì •ë³´
        self.chunks_info = {
            category: len(indices) 
            for category, indices in self.manual_indices.items()
        }
        
        # ëª¨ë“  ì²­í¬ì˜ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        self._extract_chunk_dates()
        
        logger.info(f"HybridRAGPipeline initialized with {len(chunks)} chunks")
        logger.info(f"Manual distribution: {self.chunks_info}")
    
    def _extract_chunk_dates(self):
        """ëª¨ë“  ì²­í¬ì˜ ë‚ ì§œ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ"""
        for chunk in self.chunks:
            doc_date = self.version_manager.extract_document_date(chunk)
            if doc_date:
                metadata = json.loads(chunk.get('metadata', '{}'))
                metadata['document_date'] = doc_date
                chunk['metadata'] = json.dumps(metadata)
    
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx, chunk in enumerate(self.chunks):
            source = chunk.get('source', '').lower()
            
            if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜'].append(idx)
            elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                indices['í˜„í™©ê³µì‹œ'].append(idx)
            elif 'ë¹„ìƒì¥' in source:
                indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­'].append(idx)
            else:
                indices['ê¸°íƒ€'].append(idx)
        
        return dict(indices)
    
    def _get_query_embedding(self, query: str) -> np.ndarray:
        """ì¿¼ë¦¬ ì„ë² ë”©ì„ ìºì‹œì™€ í•¨ê»˜ ê°€ì ¸ì˜¤ê¸°"""
        # ìºì‹œ í™•ì¸
        cached_embedding = self.cache_strategy.embedding_cache.get(query)
        if cached_embedding is not None:
            return cached_embedding
            
        # ìƒˆë¡œ ìƒì„±
        embedding = self.embedding_model.encode([query])
        embedding = np.array(embedding, dtype=np.float32)
        
        # ìºì‹œ ì €ì¥
        self.cache_strategy.embedding_cache.put(query, embedding)
        
        return embedding
    
    async def process_query(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        
        ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ë” ì‹ ì¤‘í•˜ê²Œ, ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        start_time = time.time()
        
        # 1. ì¤‘ìš”ë„ í‰ê°€
        importance, importance_info = self.importance_assessor.assess_importance(query)
        logger.info(f"Query importance: {importance.value} - {importance_info['factors']}")
        
        # 2. ìºì‹œ í™•ì¸ (ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ìºì‹œ ì‚¬ìš© ì•ˆ í•¨)
        if importance != QueryImportance.CRITICAL:
            cached_result = self.cache_strategy.get_cached_result(query)
            if cached_result:
                logger.info(f"Cache hit for query: {query[:50]}...")
                return cached_result['results'], cached_result['stats']
        
        # 3. ì§ˆë¬¸ ë¶„ë¥˜ ë° ë³µì¡ë„ í‰ê°€
        category, cat_confidence = self.classifier.classify(query)
        complexity, comp_confidence, complexity_analysis = self.complexity_assessor.assess(query)
        
        # 4. ì ì‘ì  ì§ˆë¬¸ ë¶„ì„ (ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ë” ì •êµí•˜ê²Œ ë¶„ì„)
        analysis_start = time.time()
        try:
            gpt_analysis = self.query_analyzer.analyze_and_strategize(
                query, self.chunks_info, importance
            )
            analysis_time = time.time() - analysis_start
        except Exception as e:
            logger.error(f"Query analysis failed: {str(e)}, falling back to rule-based")
            gpt_analysis = self._get_fallback_analysis(query, category)
            analysis_time = time.time() - analysis_start
        
        # 5. ëª¨ë¸ ì„ íƒ (ì¤‘ìš”ë„ ìš°ì„  ê³ ë ¤)
        selected_model, selection_info = self.model_selector.select_model(
            query, complexity, importance, gpt_analysis
        )
        
        # 6. ì¤‘ìš”ë„ì— ë”°ë¥¸ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
        if importance == QueryImportance.CRITICAL:
            # ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ë” ê´‘ë²”ìœ„í•œ ê²€ìƒ‰
            results, search_stats = await self._comprehensive_search_for_critical(
                query, gpt_analysis, top_k
            )
        else:
            # ì¼ë°˜ì ì¸ ê²€ìƒ‰ ì „ëµ
            search_approach = gpt_analysis['search_strategy']['approach']
            
            if search_approach == 'direct_lookup':
                results, search_stats = await self._gpt_guided_direct_search(
                    query, gpt_analysis, top_k
                )
            elif search_approach == 'focused_search':
                results, search_stats = await self._gpt_guided_focused_search(
                    query, gpt_analysis, top_k
                )
            else:  # comprehensive_analysis
                results, search_stats = await self._gpt_guided_comprehensive_search(
                    query, gpt_analysis, top_k
                )
        
        # 7. ì¶©ëŒ í•´ê²° ë° ìµœì‹  ì •ë³´ ìš°ì„ ì‹œ
        results = self.conflict_resolver.resolve_conflicts(results, query)
        
        # 8. êµ¬ë²„ì „ ê²½ê³  ìˆ˜ì§‘
        outdated_warnings = []
        for result in results:
            if result.metadata.get('has_outdated_info'):
                outdated_warnings.extend(result.metadata.get('warnings', []))
        
        # 9. í†µê³„ ì •ë³´ êµ¬ì„±
        stats = {
            'total_time': time.time() - start_time,
            'analysis_time': analysis_time,
            'gpt_analysis': gpt_analysis,
            'category': category,
            'category_confidence': cat_confidence,
            'complexity': complexity.value,
            'complexity_confidence': comp_confidence,
            'complexity_analysis': complexity_analysis,
            'importance': importance.value,
            'importance_info': importance_info,
            'selected_model': selected_model,
            'selection_info': selection_info,
            'search_approach': search_stats.get('search_method', 'unknown'),
            'outdated_warnings': outdated_warnings,
            'has_version_conflicts': len(outdated_warnings) > 0,
            **search_stats
        }
        
        # 10. ìºì‹œ ì €ì¥ (ì¤‘ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ë§Œ)
        if importance != QueryImportance.CRITICAL and not outdated_warnings:
            self.cache_strategy.store_result(query, {
                'results': results,
                'stats': stats
            }, selected_model, importance)
        
        return results, stats
    
    async def _comprehensive_search_for_critical(self, query: str, gpt_analysis: Dict, 
                                               top_k: int) -> Tuple[List[SearchResult], Dict]:
        """ì¤‘ìš”í•œ ì§ˆë¬¸ì— ëŒ€í•œ í¬ê´„ì  ê²€ìƒ‰
        
        ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
        ì—¬ëŸ¬ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        start_time = time.time()
        all_results = []
        
        # ëª¨ë“  ê´€ë ¨ ë§¤ë‰´ì–¼ì—ì„œ ê´‘ë²”ìœ„í•˜ê²Œ ê²€ìƒ‰
        for manual in self.manual_indices.keys():
            if manual != 'ê¸°íƒ€':
                partial_results = await self._search_in_manual(
                    query, manual, [], top_k * 2  # ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘
                )
                all_results.extend(partial_results)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        seen_chunks = set()
        unique_results = []
        for result in sorted(all_results, key=lambda x: x.score, reverse=True):
            if result.chunk_id not in seen_chunks:
                seen_chunks.add(result.chunk_id)
                unique_results.append(result)
                if len(unique_results) >= top_k * 3:  # ë” ë§ì€ ê²°ê³¼ ë³´ê´€
                    break
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        final_results = unique_results[:top_k]
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': sum(len(indices) for indices in self.manual_indices.values()),
            'search_method': 'comprehensive_critical',
            'total_results_found': len(unique_results),
            'results_returned': len(final_results)
        }
        
        return final_results, stats
    
    async def _gpt_guided_direct_search(self, query: str, gpt_analysis: Dict, 
                                       top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§ì ‘ ê²€ìƒ‰"""
        start_time = time.time()
        
        primary_manual = gpt_analysis['search_strategy']['primary_manual']
        search_keywords = gpt_analysis['search_strategy']['search_keywords']
        
        # ëŒ€ìƒ ì¸ë±ìŠ¤ ì„ íƒ
        target_indices = self.manual_indices.get(primary_manual, [])[:100]
        
        if not target_indices:
            logger.warning(f"No indices for manual '{primary_manual}', using all chunks")
            target_indices = list(range(min(len(self.chunks), 50)))
        
        # ê²€ìƒ‰ì–´ í™•ì¥
        enhanced_query = f"{query} {' '.join(search_keywords)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        # FAISS ê²€ìƒ‰
        k_search = min(len(target_indices), max(1, top_k * 3))
        
        try:
            scores, indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error: {str(e)}")
            return [], {'search_time': time.time() - start_time, 'error': str(e)}
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        target_set = set(target_indices)
        
        for idx, score in zip(indices[0], scores[0]):
            if idx in target_set and idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
                if len(results) >= top_k:
                    break
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': len(target_indices),
            'search_method': 'direct_vector'
        }
        
        return results, stats
    
    async def _gpt_guided_focused_search(self, query: str, gpt_analysis: Dict, 
                                        top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§‘ì¤‘ ê²€ìƒ‰"""
        start_time = time.time()
        
        primary_manual = gpt_analysis['search_strategy'].get('primary_manual', '')
        search_keywords = gpt_analysis['search_strategy'].get('search_keywords', [])
        expected_chunks = gpt_analysis['search_strategy'].get('expected_chunks_needed', 10)
        
        # ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •
        search_limit = min(expected_chunks * 2, 200)
        target_indices = self.manual_indices.get(primary_manual, [])[:search_limit]
        
        if not target_indices:
            target_indices = list(range(min(len(self.chunks), 100)))
        
        # ë‹µë³€ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ í•„í„°ë§
        requirements = gpt_analysis.get('answer_requirements', {})
        if requirements.get('needs_specific_numbers'):
            filtered_indices = [
                idx for idx in target_indices 
                if idx < len(self.chunks) and
                re.search(r'\d+ì–µ|\d+%', self.chunks[idx].get('content', ''))
            ]
            if filtered_indices:
                target_indices = filtered_indices
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        enhanced_query = f"{query} {' '.join(search_keywords)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        k_search = min(len(target_indices), max(1, top_k * 5))
        
        try:
            scores, indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error: {str(e)}")
            return [], {'search_time': time.time() - start_time, 'error': str(e)}
        
        # ê²°ê³¼ êµ¬ì„± ë° ê´€ë ¨ì„± ë¶€ìŠ¤íŒ…
        results = []
        target_set = set(target_indices)
        
        for idx, score in zip(indices[0], scores[0]):
            if idx < 0 or idx >= len(self.chunks):
                continue
                
            if idx not in target_set:
                continue
                
            chunk = self.chunks[idx]
            
            # GPT ë¶„ì„ê³¼ì˜ ê´€ë ¨ì„± ê³„ì‚°
            relevance_boost = self._calculate_gpt_relevance(
                chunk['content'], gpt_analysis
            )
            
            result = SearchResult(
                chunk_id=chunk.get('chunk_id', str(idx)),
                content=chunk.get('content', ''),
                score=float(score) * (1 + relevance_boost),
                source=chunk.get('source', 'Unknown'),
                page=chunk.get('page', 0),
                chunk_type=chunk.get('chunk_type', 'unknown'),
                metadata=json.loads(chunk.get('metadata', '{}'))
            )
            
            results.append(result)
            
            if len(results) >= top_k * 2:
                break
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ ì„ íƒ
        results.sort(key=lambda x: x.score, reverse=True)
        results = results[:top_k]
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': len(target_indices),
            'search_method': 'focused_vector',
            'results_count': len(results)
        }
        
        return results, stats
    
    async def _gpt_guided_comprehensive_search(self, query: str, gpt_analysis: Dict, 
                                              top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¢…í•© ê²€ìƒ‰"""
        start_time = time.time()
        
        all_results = []
        
        # ëª¨ë“  ê´€ë ¨ ë²•ì  ê°œë…ì— ëŒ€í•´ ê²€ìƒ‰
        for concept in gpt_analysis['legal_concepts']:
            if concept['relevance'] in ['primary', 'secondary']:
                manual = concept['concept']
                if manual in self.manual_indices:
                    partial_results = await self._search_in_manual(
                        query, manual, concept['specific_aspects'], top_k // 2
                    )
                    all_results.extend(partial_results)
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ ê²°ê³¼ ì„ íƒ
        seen_chunks = set()
        unique_results = []
        for result in sorted(all_results, key=lambda x: x.score, reverse=True):
            if result.chunk_id not in seen_chunks:
                seen_chunks.add(result.chunk_id)
                unique_results.append(result)
                if len(unique_results) >= top_k:
                    break
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': sum(len(self.manual_indices.get(c['concept'], [])) 
                                 for c in gpt_analysis['legal_concepts'] 
                                 if c['relevance'] in ['primary', 'secondary']),
            'search_method': 'comprehensive_multi_manual'
        }
        
        return unique_results, stats
    
    def _calculate_gpt_relevance(self, content: str, gpt_analysis: Dict) -> float:
        """GPT ë¶„ì„ ê²°ê³¼ì™€ ì²­í¬ ë‚´ìš©ì˜ ê´€ë ¨ì„± ê³„ì‚°"""
        relevance_boost = 0.0
        content_lower = content.lower()
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in gpt_analysis['search_strategy']['search_keywords']:
            if keyword.lower() in content_lower:
                relevance_boost += 0.1
        
        # ë‹µë³€ ìš”êµ¬ì‚¬í•­ê³¼ì˜ ë§¤ì¹­
        requirements = gpt_analysis['answer_requirements']
        if requirements.get('needs_specific_numbers') and re.search(r'\d+ì–µ|\d+%', content):
            relevance_boost += 0.2
        if requirements.get('needs_timeline') and re.search(r'\d+ì¼|ê¸°í•œ', content):
            relevance_boost += 0.2
        if requirements.get('needs_process_steps') and re.search(r'ì ˆì°¨|ë‹¨ê³„|ìˆœì„œ', content):
            relevance_boost += 0.15
        
        return min(relevance_boost, 0.5)
    
    async def _search_in_manual(self, query: str, manual: str, aspects: List[str], 
                               limit: int) -> List[SearchResult]:
        """íŠ¹ì • ë§¤ë‰´ì–¼ ë‚´ì—ì„œ ê²€ìƒ‰"""
        indices = self.manual_indices.get(manual, [])[:100]
        
        if not indices:
            return []
        
        enhanced_query = f"{query} {' '.join(aspects)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        k_search = min(len(indices), max(1, limit * 3))
        
        try:
            scores, search_indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error in manual search: {str(e)}")
            return []
        
        results = []
        indices_set = set(indices)
        
        for idx, score in zip(search_indices[0], scores[0]):
            if idx in indices_set and idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
                if len(results) >= limit:
                    break
        
        return results
    
    def _get_fallback_analysis(self, query: str, category: str = None) -> Dict:
        """GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ í´ë°± ë¶„ì„"""
        primary_manual = category or "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜"
        
        return {
            'query_analysis': {
                'core_intent': query,
                'actual_complexity': 'medium',
                'complexity_reason': 'GPT ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©'
            },
            'legal_concepts': [{
                'concept': primary_manual,
                'relevance': 'primary',
                'specific_aspects': []
            }],
            'search_strategy': {
                'approach': 'focused_search',
                'primary_manual': primary_manual,
                'search_keywords': query.split()[:5],
                'expected_chunks_needed': 20,
                'rationale': 'ê¸°ë³¸ ê²€ìƒ‰ ì „ëµ'
            },
            'answer_requirements': {
                'needs_specific_numbers': True,
                'needs_process_steps': True,
                'needs_timeline': False,
                'needs_exceptions': False,
                'needs_multiple_perspectives': False
            }
        }

# ===== ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ =====
class AnswerQualityValidator:
    """ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ì‹œìŠ¤í…œ
    
    ì´ëŠ” ì œí’ˆ ì¶œí•˜ ì „ í’ˆì§ˆ ê²€ì‚¬ì™€ ê°™ìŠµë‹ˆë‹¤.
    ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    
    def validate_answer(self, query: str, answer: str, importance: QueryImportance) -> Dict:
        """ë‹µë³€ì˜ í’ˆì§ˆì„ ê²€ì¦
        
        ì¤‘ìš”í•œ ì§ˆë¬¸ì˜ ë‹µë³€ì€ ë” ì—„ê²©í•˜ê²Œ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        issues = []
        quality_score = 1.0
        
        # ë¶ˆí™•ì‹¤í•œ í‘œí˜„ ì²´í¬
        uncertain_phrases = ['í™•ì‹¤í•˜ì§€ ì•Š', 'ì•„ë§ˆë„', 'ì¶”ì •', '~ê²ƒ ê°™ìŠµë‹ˆë‹¤', '~ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤']
        for phrase in uncertain_phrases:
            if phrase in answer:
                issues.append(f"ë¶ˆí™•ì‹¤í•œ í‘œí˜„ ì‚¬ìš©: {phrase}")
                quality_score -= 0.1
        
        # ë²•ì  ê·¼ê±° í™•ì¸
        legal_references = re.findall(r'(ì œ\d+ì¡°|ì‹œí–‰ë ¹|ê·œì •|ë§¤ë‰´ì–¼)', answer)
        if len(legal_references) == 0:
            issues.append("ë²•ì  ê·¼ê±° ë¶€ì¡±")
            quality_score -= 0.2
        
        # ë‹µë³€ ê¸¸ì´ í™•ì¸
        if len(answer) < 100:
            issues.append("ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ")
            quality_score -= 0.15
        
        # ì¤‘ìš”í•œ ì§ˆë¬¸ì˜ ê²½ìš° ì¶”ê°€ ê²€ì¦
        if importance == QueryImportance.CRITICAL:
            # êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ í™•ì¸
            numbers = re.findall(r'\d+', answer)
            if len(numbers) == 0:
                issues.append("êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì •ë³´ ë¶€ì¡±")
                quality_score -= 0.1
            
            # ì£¼ì˜ì‚¬í•­ í¬í•¨ ì—¬ë¶€
            if not any(word in answer for word in ['ì£¼ì˜', 'ìœ ì˜', 'ì˜ˆì™¸', 'ë‹¨ì„œ']):
                issues.append("ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì˜ˆì™¸ì‚¬í•­ ë¯¸í¬í•¨")
                quality_score -= 0.1
        
        return {
            'quality_score': max(quality_score, 0),
            'issues': issues,
            'passed': quality_score >= 0.7,
            'needs_revision': quality_score < 0.7 and importance == QueryImportance.CRITICAL
        }

# ===== ë‹µë³€ ìƒì„± í•¨ìˆ˜ =====
def determine_temperature(query: str, complexity: str, model: str, importance: str) -> float:
    """ì§ˆë¬¸ ìœ í˜•, ë³µì¡ë„, ëª¨ë¸, ì¤‘ìš”ë„ì— ë”°ë¼ ìµœì ì˜ temperature ê²°ì •
    
    TemperatureëŠ” AIì˜ ì°½ì˜ì„± ìˆ˜ì¤€ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
    ì¤‘ìš”í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë‚®ì€ temperatureë¥¼ ì‚¬ìš©í•˜ì—¬
    ì¼ê´€ë˜ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    query_lower = query.lower()
    
    # ì¤‘ìš”ë„ë³„ ê¸°ë³¸ temperature
    if importance == 'critical':
        base_temp = 0.1  # ë§¤ìš° ë‚®ê²Œ ì„¤ì •
    elif importance == 'high':
        base_temp = 0.2
    else:
        base_temp = 0.3
    
    # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë¯¸ì„¸ ì¡°ì •
    if any(keyword in query_lower for keyword in ['ì–¸ì œ', 'ë©°ì¹ ', 'ê¸°í•œ', 'ë‚ ì§œ', 'ê¸ˆì•¡', '%']):
        base_temp = min(base_temp, 0.1)  # ì‚¬ì‹¤ í™•ì¸ì€ ì •í™•ì„±ì´ ì¤‘ìš”
    elif any(keyword in query_lower for keyword in ['ì „ëµ', 'ëŒ€ì‘', 'ë¦¬ìŠ¤í¬', 'ì£¼ì˜', 'ê¶Œì¥']):
        base_temp = min(base_temp + 0.1, 0.5)  # ì „ëµì  ì¡°ì–¸ë„ ì‹ ì¤‘í•˜ê²Œ
    
    return base_temp

async def generate_answer(query: str, results: List[SearchResult], stats: Dict) -> Tuple[str, Dict]:
    """ì„ íƒëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±
    
    ì¤‘ìš”í•œ ì§ˆë¬¸ì€ ë” ì‹ ì¤‘í•˜ê²Œ ë‹µë³€ì„ ìƒì„±í•˜ê³ ,
    í•„ìš”ì‹œ í’ˆì§ˆ ê²€ì¦ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # ì„ íƒëœ ëª¨ë¸ ë° ì¤‘ìš”ë„ í™•ì¸
    model = stats.get('selected_model', 'gpt-4o-mini')
    importance = stats.get('importance', 'normal')
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    
    # êµ¬ë²„ì „ ê²½ê³ ê°€ ìˆìœ¼ë©´ ë¨¼ì € í‘œì‹œ
    if stats.get('has_version_conflicts'):
        context_parts.append("âš ï¸ ì£¼ì˜: ì¼ë¶€ ì°¸ê³  ìë£Œì— êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
    
    # ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
    for i, result in enumerate(results[:5]):
        # êµ¬ë²„ì „ ì •ë³´ í‘œì‹œ
        warning_marker = ""
        if result.metadata.get('has_outdated_info'):
            warnings = result.metadata.get('warnings', [])
            if warnings:
                warning_marker = " âš ï¸ [êµ¬ë²„ì „ ì •ë³´ í¬í•¨]"
        
        context_parts.append(f"""
[ì°¸ê³  {i+1}] {result.source} (í˜ì´ì§€ {result.page}){warning_marker}
{result.content}
""")
    
    context = "\n---\n".join(context_parts)
    
    # ë³µì¡ë„ ì •ë³´ í™œìš©
    complexity = stats.get('complexity', 'medium')
    temperature = determine_temperature(query, complexity, model, importance)
    
    # ì¤‘ìš”í•œ ì§ˆë¬¸ì— ëŒ€í•œ íŠ¹ë³„ ì§€ì‹œì‚¬í•­
    if importance == 'critical':
        extra_instructions = """
íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:
1. ì •í™•í•œ ë²•ì  ê·¼ê±° (ì¡°í•­ ëª…ì‹œ)
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê¸°í•œ
3. ì£¼ì˜ì‚¬í•­ ë° ì˜ˆì™¸ì‚¬í•­
4. ì‹¤ë¬´ ì ìš© ì‹œ í™•ì¸í•´ì•¼ í•  ì²´í¬ë¦¬ìŠ¤íŠ¸"""
    else:
        extra_instructions = ""
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥)
2. ìƒì„¸ ì„¤ëª… (ê·¼ê±° ì¡°í•­ í¬í•¨)
3. ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì˜ˆì™¸ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
4. ì‹¤ë¬´ ì ìš© ì‹œ ê¶Œì¥ì‚¬í•­ (í•„ìš”í•œ ê²½ìš°)

{extra_instructions}

ì¤‘ìš”: êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°, ë°˜ë“œì‹œ ìµœì‹  ê¸°ì¤€ì„ ëª…ì‹œí•˜ì„¸ìš”."""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

{"ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ" if importance == 'critical' else "ëª…í™•í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ"} ë‹µë³€í•´ì£¼ì„¸ìš”."""}
    ]
    
    # API í˜¸ì¶œ ì‹œì‘
    api_start = time.time()
    
    try:
        response = openai.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=2000 if importance == 'critical' else 1500
        )
        
        answer = response.choices[0].message.content
        
        # ì¤‘ìš”í•œ ì§ˆë¬¸ì˜ ê²½ìš° í’ˆì§ˆ ê²€ì¦
        if importance == 'critical':
            validator = AnswerQualityValidator()
            validation_result = validator.validate_answer(query, answer, QueryImportance.CRITICAL)
            
            if validation_result['needs_revision']:
                logger.warning(f"Answer quality issues detected: {validation_result['issues']}")
                # í•„ìš”ì‹œ ì¬ìƒì„± ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
        
        generation_stats = {
            'generation_time': time.time() - api_start,
            'model': model,
            'temperature': temperature
        }
        
        return answer, generation_stats
        
    except Exception as e:
        logger.error(f"Answer generation error: {str(e)}")
        
        # í´ë°± ë‹µë³€
        fallback_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì˜¤ë¥˜ ë‚´ìš©: {str(e)}

ë‹¤ìŒ ì°¸ê³  ìë£Œë¥¼ ì§ì ‘ í™•ì¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:
"""
        for i, result in enumerate(results[:3]):
            fallback_answer += f"\n{i+1}. {result.source} (í˜ì´ì§€ {result.page})"
        
        return fallback_answer, {
            'generation_time': time.time() - api_start,
            'error': str(e),
            'model': model
        }

# ===== ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© =====
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    try:
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["manuals_vector_db.index", "all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            st.info("ğŸ’¡ prepare_pdfs_ftc.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
            return None, None, None, None
        
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)"):
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            index = faiss.read_index("manuals_vector_db.index")
            logger.info(f"Loaded FAISS index with {index.ntotal} vectors")
            
            # ì²­í¬ ë°ì´í„° ë¡œë“œ
            with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
                chunks = json.load(f)
            logger.info(f"Loaded {len(chunks)} chunks")
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            try:
                embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
                logger.info("Loaded Korean embedding model")
            except Exception as e:
                st.warning("í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                logger.warning(f"Using fallback embedding model: {str(e)}")
            
            # ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            try:
                reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
                logger.info("Loaded Korean reranker model")
            except:
                reranker_model = None
                logger.warning("Reranker model not loaded")
        
        return embedding_model, reranker_model, index, chunks
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(f"Initialization error: {str(e)}")
        logger.debug(traceback.format_exc())
        return None, None, None, None

# ===== ë©”ì¸ UI =====
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜"""
    
    # í—¤ë” í‘œì‹œ
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ í†µí•© AI Q&A ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    models = load_models_and_data()
    if not all(models):
        st.stop()
    
    embedding_model, reranker_model, index, chunks = models
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    if 'rag_pipeline' not in st.session_state:
        with st.spinner("ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘..."):
            st.session_state.rag_pipeline = OptimizedHybridRAGPipeline(
                embedding_model, reranker_model, index, chunks
            )
    
    rag = st.session_state.rag_pipeline
    
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ì»¨í…Œì´ë„ˆ
    chat_container = st.container()
    
    with chat_container:
        # ì´ì „ ëŒ€í™” í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "user":
                    st.write(message["content"])
                else:
                    if isinstance(message["content"], dict):
                        # ë‹µë³€ í‘œì‹œ
                        st.write(message["content"]["answer"])
                        
                        # êµ¬ë²„ì „ ê²½ê³  í‘œì‹œ
                        if message["content"].get("has_version_conflicts"):
                            st.markdown("""
                            <div class="version-warning">
                            âš ï¸ <strong>ì£¼ì˜:</strong> ì¼ë¶€ ì°¸ê³  ìë£Œì— êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                            ìµœì‹  ê·œì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.
                            </div>
                            """, unsafe_allow_html=True)
                        
                        # ë©”íƒ€ ì •ë³´ í‘œì‹œ
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            model_used = message["content"].get("model_used", "unknown")
                            model_emoji = {
                                'gpt-4o-mini': 'ğŸŸ¢',
                                'o4-mini': 'ğŸŸ¡',
                                'o3-mini': 'ğŸŸ ',
                                'gpt-4o': 'ğŸ”µ'
                            }.get(model_used, 'âšª')
                            st.caption(f"{model_emoji} {model_used}")
                        
                        with col2:
                            importance = message["content"].get("importance", "normal")
                            importance_class = f"importance-{importance}"
                            st.markdown(f'<span class="importance-indicator {importance_class}">{importance.upper()}</span>', 
                                      unsafe_allow_html=True)
                        
                        with col3:
                            complexity = message["content"].get("complexity", "unknown")
                            complexity_class = f"complexity-{complexity}"
                            st.markdown(f'<span class="complexity-indicator {complexity_class}">{complexity.upper()}</span>', 
                                      unsafe_allow_html=True)
                        
                        with col4:
                            total_time = message["content"].get("total_time", 0)
                            st.caption(f"â±ï¸ {total_time:.1f}ì´ˆ")
                    else:
                        st.write(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            with st.chat_message("user"):
                st.write(prompt)
            
            # AI ì‘ë‹µ
            with st.chat_message("assistant"):
                total_start_time = time.time()
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                search_start_time = time.time()
                with st.spinner("ğŸ” ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ì¤‘..."):
                    results, search_stats = run_async_in_streamlit(
                        rag.process_query(prompt, top_k=5)
                    )
                search_time = time.time() - search_start_time
                
                # ë‹µë³€ ìƒì„±
                generation_start_time = time.time()
                with st.spinner(f"ğŸ’­ {search_stats.get('selected_model', 'AI')}ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer, generation_stats = run_async_in_streamlit(
                        generate_answer(prompt, results, search_stats)
                    )
                generation_time = time.time() - generation_start_time
                
                # ë‹µë³€ í‘œì‹œ
                st.write(answer)
                
                # êµ¬ë²„ì „ ê²½ê³  í‘œì‹œ
                if search_stats.get('has_version_conflicts'):
                    st.markdown("""
                    <div class="version-warning">
                    âš ï¸ <strong>ì£¼ì˜:</strong> ì¼ë¶€ ì°¸ê³  ìë£Œì— êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                    ìµœì‹  ê·œì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # êµ¬ì²´ì ì¸ ê²½ê³  ë‚´ìš©
                    for warning in search_stats.get('outdated_warnings', []):
                        st.warning(f"êµ¬ë²„ì „: {warning['found']} â†’ í˜„ì¬: {warning['current']} ({warning['regulation']})")
                
                # í†µê³„ ì •ë³´
                total_time = time.time() - total_start_time
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    model_used = search_stats.get('selected_model', 'unknown')
                    model_emoji = {
                        'gpt-4o-mini': 'ğŸŸ¢',
                        'o4-mini': 'ğŸŸ¡',
                        'o3-mini': 'ğŸŸ ',
                        'gpt-4o': 'ğŸ”µ'
                    }.get(model_used, 'âšª')
                    st.metric("ëª¨ë¸", f"{model_emoji} {model_used}")
                
                with col2:
                    st.metric("ê²€ìƒ‰", f"{search_time:.1f}ì´ˆ")
                
                with col3:
                    st.metric("ìƒì„±", f"{generation_time:.1f}ì´ˆ")
                
                with col4:
                    st.metric("ì „ì²´", f"{total_time:.1f}ì´ˆ")
                
                # ì¤‘ìš”ë„ì™€ ë³µì¡ë„ í‘œì‹œ
                importance = search_stats.get('importance', 'normal')
                complexity = search_stats.get('complexity', 'unknown')
                
                col1, col2 = st.columns(2)
                with col1:
                    importance_html = f'<span class="importance-indicator importance-{importance}">ì¤‘ìš”ë„: {importance.upper()}</span>'
                    st.markdown(importance_html, unsafe_allow_html=True)
                
                with col2:
                    complexity_html = f'<span class="complexity-indicator complexity-{complexity}">ë³µì¡ë„: {complexity.upper()}</span>'
                    st.markdown(complexity_html, unsafe_allow_html=True)
                
                # ìƒì„¸ ì •ë³´ (ì ‘ì„ ìˆ˜ ìˆìŒ)
                with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                    # íƒ­ êµ¬ì„±
                    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ë¶„ì„ ê³¼ì •", "ğŸ“š ì°¸ê³  ìë£Œ", "ğŸ¤– AI ë¶„ì„"])
                    
                    with tab1:
                        # ì¤‘ìš”ë„ ì •ë³´
                        importance_info = search_stats.get('importance_info', {})
                        if importance_info:
                            st.subheader("ğŸ¯ ì¤‘ìš”ë„ í‰ê°€")
                            st.write(f"**ì¤‘ìš”ë„ ì ìˆ˜**: {importance_info.get('score', 0)}")
                            st.write("**í‰ê°€ ìš”ì¸**:")
                            for factor in importance_info.get('factors', []):
                                st.write(f"  â€¢ {factor}")
                        
                        # ëª¨ë¸ ì„ íƒ ì´ìœ 
                        selection_info = search_stats.get('selection_info', {})
                        if selection_info:
                            st.info(f"**ëª¨ë¸ ì„ íƒ ì´ìœ **: {selection_info.get('reason', 'N/A')}")
                        
                        # ê²€ìƒ‰ ì „ëµ
                        st.markdown("**ê²€ìƒ‰ ì „ëµ**")
                        st.json({
                            "ì ‘ê·¼ ë°©ì‹": search_stats.get('search_approach', 'N/A'),
                            "ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜": search_stats.get('searched_chunks', 0)
                        })
                    
                    with tab2:
                        for i, result in enumerate(results[:5]):
                            with st.container():
                                # ì¶œì²˜ ì •ë³´
                                st.markdown(f"**[{i+1}] {result.source}** - í˜ì´ì§€ {result.page}")
                                
                                # ë¬¸ì„œ ë‚ ì§œ ë° ê²½ê³ 
                                if result.document_date:
                                    st.caption(f"ğŸ“… ë¬¸ì„œ ë‚ ì§œ: {result.document_date}")
                                
                                if result.metadata.get('has_outdated_info'):
                                    st.error("âš ï¸ ì´ ë¬¸ì„œì—ëŠ” êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                                
                                # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                                content_preview = result.content[:300] + "..." if len(result.content) > 300 else result.content
                                st.text(content_preview)
                                
                                # ê´€ë ¨ë„ ì ìˆ˜
                                st.caption(f"ê´€ë ¨ë„ ì ìˆ˜: {result.score:.2f}")
                                st.divider()
                    
                    with tab3:
                        # GPT ë¶„ì„ ê²°ê³¼
                        gpt_analysis = search_stats.get('gpt_analysis', {})
                        
                        st.markdown("**ì§ˆë¬¸ ë¶„ì„**")
                        st.json(gpt_analysis.get('query_analysis', {}))
                        
                        st.markdown("**ë²•ì  ê°œë…**")
                        st.json(gpt_analysis.get('legal_concepts', []))
                        
                        st.markdown("**ë‹µë³€ ìš”êµ¬ì‚¬í•­**")
                        requirements = gpt_analysis.get('answer_requirements', {})
                        req_text = []
                        if requirements.get('needs_specific_numbers'):
                            req_text.append("âœ“ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ í•„ìš”")
                        if requirements.get('needs_process_steps'):
                            req_text.append("âœ“ ë‹¨ê³„ë³„ ì ˆì°¨ í•„ìš”")
                        if requirements.get('needs_timeline'):
                            req_text.append("âœ“ ì‹œê°„ ìˆœì„œ í•„ìš”")
                        st.write("\n".join(req_text) if req_text else "íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­ ì—†ìŒ")
                
                # ì‘ë‹µ ë°ì´í„° ì €ì¥
                response_data = {
                    "answer": answer,
                    "model_used": model_used,
                    "total_time": total_time,
                    "complexity": complexity,
                    "importance": importance,
                    "has_version_conflicts": search_stats.get('has_version_conflicts', False),
                    "search_stats": search_stats,
                    "generation_stats": generation_stats
                }
                st.session_state.messages.append({"role": "assistant", "content": response_data})
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        
        st.subheader("ğŸŸ¢ ê°„ë‹¨í•œ ì§ˆë¬¸")
        example_simple = [
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?",
            "ì´ì‚¬íšŒ ì˜ê²° ê¸ˆì•¡ ê¸°ì¤€ì€?",
            "í˜„í™©ê³µì‹œëŠ” ì–¸ì œ í•´ì•¼ í•˜ë‚˜ìš”?"
        ]
        for example in example_simple:
            if st.button(example, key=f"simple_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.subheader("ğŸŸ¡ í‘œì¤€ ì§ˆë¬¸")
        example_standard = [
            "ê³„ì—´ì‚¬ì™€ ìê¸ˆê±°ë˜ ì‹œ ì ˆì°¨ëŠ”?",
            "ë¹„ìƒì¥ì‚¬ ì£¼ì‹ ì–‘ë„ ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ”?",
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë©´ì œ ì¡°ê±´ì€?"
        ]
        for example in example_standard:
            if st.button(example, key=f"standard_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.subheader("ğŸ”µ ì¤‘ìš”í•œ ì§ˆë¬¸")
        example_important = [
            "ê³„ì—´íšŒì‚¬ Aê°€ Bì—ê²Œ ì±„ê¶Œ 500ì–µì› ê·œëª¨ë¥¼ ë§¤ê°í•˜ì˜€ìŠµë‹ˆë‹¤. ë§¤ê°ì´ìµì€ 200ì–µì› ì…ë‹ˆë‹¤. ê³µì‹œëŒ€ìƒì€?",
            "AíšŒì‚¬ê°€ Bê³„ì—´ì‚¬ì— 100ì–µì›ì„ ëŒ€ì—¬í•˜ë©´ì„œ ë™ì‹œì— Cê³„ì—´ì‚¬ì˜ ì§€ë¶„ì„ ì·¨ë“í•˜ëŠ” ê²½ìš° ì ìš©ë˜ëŠ” ê·œì œëŠ”?",
            "ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜í•  ë•Œ ê²€í† í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        ]
        for example in example_important:
            if st.button(example, key=f"important_{example}"):
                st.session_state.new_question = example
                st.rerun()
        
        st.divider()
        
        # ì‹œìŠ¤í…œ ì •ë³´
        with st.expander("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´"):
            st.info("""
            **ì¤‘ìš”ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ**
            - ğŸ”´ Critical: ëŒ€ê·œëª¨ ê¸ˆì•¡, ë²•ì  ë¦¬ìŠ¤í¬ â†’ gpt-4o
            - ğŸŸ  High: ì¤‘ìš”í•œ ë²•ì  íŒë‹¨ â†’ o3-mini/gpt-4o
            - ğŸŸ¢ Normal: ì¼ë°˜ ì§ˆë¬¸ â†’ gpt-4o-mini/o4-mini
            
            **ì§ˆë¬¸ ë¶„ì„ í”„ë¡œì„¸ìŠ¤**
            1. ì¤‘ìš”ë„ í‰ê°€ (ê¸ˆì•¡, ë²•ì  ë¦¬ìŠ¤í¬)
            2. ë³µì¡ë„ ë¶„ì„
            3. ìµœì  ëª¨ë¸ ì„ íƒ
            4. í’ˆì§ˆ ë³´ì¦
            """)
        
        # ë¦¬ì…‹ ë²„íŠ¼
        if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", type="secondary"):
            st.session_state.messages = []
            st.rerun()
    
    # í˜ì´ì§€ í•˜ë‹¨
    st.divider()
    st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ìƒì„±í•œ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬í•­ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    st.caption("ğŸ“… ì‹œìŠ¤í…œ ê¸°ì¤€ì¼: 2025ë…„ 1ì›” (ìµœì‹  ê·œì • ë°˜ì˜)")
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
