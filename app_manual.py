# íŒŒì¼ ì´ë¦„: app.py (Reranker ë° CoT í”„ë¡¬í”„íŠ¸ ì ìš© ìµœì¢… ë²„ì „)

import streamlit as st
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder # CrossEncoder ì¶”ê°€
import json
import openai
import os

# --- í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰) ---
st.set_page_config(page_title="ì „ëµê¸°íšë¶€ AI ë‹µë³€ ì±—ë´‡", page_icon="ğŸ¢", layout="centered")

# --- API í‚¤ ì„¤ì • ---
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except Exception as e:
    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ Secretsì— í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.")

# --- ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© (ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”) ---
@st.cache_resource
def load_models_and_data():
    """ì‚¬ì „ì— ì¤€ë¹„ëœ ë°ì´í„° íŒŒì¼ê³¼, ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # 1ë‹¨ê³„ ê²€ìƒ‰ì„ ìœ„í•œ Bi-Encoder ëª¨ë¸
        embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        # 2ë‹¨ê³„ ì¬ì •ë ¬ì„ ìœ„í•œ Cross-Encoder ëª¨ë¸
        reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
        
        index = faiss.read_index("manuals_vector_db.index")
        with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
            chunks_with_metadata = json.load(f)
        return embedding_model, reranker_model, index, chunks_with_metadata
    except FileNotFoundError:
        return None, None, None, None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def get_relevant_manual_chunks(user_question, k=7, top_n=3):
    """2ë‹¨ê³„ ê²€ìƒ‰(Retrieval & Reranking)ì„ í†µí•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    # Stage 1: Bi-Encoderë¡œ 1ì°¨ í›„ë³´êµ° ê²€ìƒ‰ (ë„‰ë„‰í•˜ê²Œ 7ê°œ)
    question_vector = model.encode([user_question])
    distances, indices = index.search(np.array(question_vector, dtype=np.float32), k)
    initial_chunks = [chunks_with_metadata[i] for i in indices[0]]
    
    # Stage 2: Cross-Encoder(Reranker)ë¡œ ìµœì¢… TOP 3 ì„ ì •
    pairs = [[user_question, chunk['content']] for chunk in initial_chunks]
    scores = reranker.predict(pairs)
    
    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ top_nê°œë§Œ ì„ íƒ
    ranked_chunks = [chunk for _, chunk in sorted(zip(scores, initial_chunks), key=lambda x: x[0], reverse=True)]
    return ranked_chunks[:top_n]

def generate_answer_with_llm(user_question, relevant_chunks):
    """ê²€ìƒ‰ëœ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ê·¼ê±°ë¡œ LLM(GPT)ì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    context_str = "\n\n".join(
        [f"ì¶œì²˜ íŒŒì¼ëª…: {chunk['source']} (Page: {chunk.get('page', 'N/A')})\në‚´ìš©: {chunk['content']}" for chunk in relevant_chunks]
    )
    
    # === í–¥ìƒëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Chain of Thought í¬í•¨) ===
    system_prompt = """
    # Role: ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ë²•ì„ ì „ë¬¸ìœ¼ë¡œ ë‹¤ë£¨ëŠ” 'ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì'ì…ë‹ˆë‹¤.

    # Instructions:
    1.  **ì‚¬ê³  ê³¼ì •(Chain of Thought):** ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì „ì— ë‹¤ìŒì˜ ì‚¬ê³  ê³¼ì •ì„ ë°˜ë“œì‹œ ê±°ì³ì•¼ í•©ë‹ˆë‹¤.
        (1) ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í•µì‹¬ì ì¸ ë²•ë¥ ì  ìŸì ì„ ì •í™•íˆ íŒŒì•…í•œë‹¤.
        (2) ì£¼ì–´ì§„ [ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´]ì—ì„œ í•´ë‹¹ ìŸì ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ëª¨ë“  ì¡°í•­ê³¼ ì‚¬ì‹¤ë“¤ì„ ë¹ ì§ì—†ì´ ì¶”ì¶œí•œë‹¤.
        (3) ì¶”ì¶œëœ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ ë…¼ë¦¬ì ì¸ ê²°ë¡ ì„ ë„ì¶œí•œë‹¤.
        (4) ì´ ê²°ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ 'ë‹µë³€ êµ¬ì¡°'ì— ë§ì¶° ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•œë‹¤.

    2.  **ë‹µë³€ êµ¬ì¡° ì¤€ìˆ˜:** ëª¨ë“  ë‹µë³€ì€ ì•„ë˜ì˜ ëª©ì°¨ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. ê° ëª©ì°¨ì˜ ì œëª©ì€ Markdown H3(###) í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        - ### 1. ê°œìš”
        - ### 2. ì£¼ìš” ë‚´ìš©
        - ### 3. ì¶œì²˜
        - ### 4. ê²°ë¡ 
        - ### 5. ì¶”ê°€ ì§ˆë¬¸ ì œì•ˆ

    3.  **ë‹µë³€ ìƒì„± ê·œì¹™:**
        - ë°˜ë“œì‹œ ì œê³µëœ [ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì‚¬ì „ ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        - ë§Œì•½ [ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´]ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë‹¤ë¥¸ ëª©ì°¨ ì—†ì´ "ì£„ì†¡í•©ë‹ˆë‹¤, ì œê³µëœ ìë£Œ ë‚´ì—ì„œëŠ” ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        - ëª¨ë“  ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"[ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´]\n{context_str}\n---\n[ì§ˆë¬¸]\n{user_question}"}
    ]
    
    try:
        response = openai.chat.completions.create(model="gpt-4o", messages=messages, temperature=0.1)
        return response.choices[0].message.content
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# --- ë©”ì¸ UI êµ¬ì„± ---
st.title("ğŸ¢ ì „ëµê¸°íšë¶€ AI ë‹µë³€ ì±—ë´‡")
model, reranker, index, chunks_with_metadata = load_models_and_data()

if model is None or reranker is None:
    st.error("ì±—ë´‡ ë°ì´í„° íŒŒì¼ ë˜ëŠ” ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GitHub ì €ì¥ì†Œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    st.success("AI ë¹„ì„œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âœ…")
    
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)

    if prompt := st.chat_input("ê³µì •ê±°ë˜ë²• ê³µì‹œ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AIê°€ ë§¤ë‰´ì–¼ì„ ê²€í† í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                relevant_chunks = get_relevant_manual_chunks(prompt)
                answer = generate_answer_with_llm(prompt, relevant_chunks)
                warning_message = "\n\n---\nâ—†ë³¸ ë‹µë³€ì€ ì „ëµê¸°íšë¶€ê°€ í•™ìŠµì‹œí‚¨ ChatGPTë¥¼ í†µí•´ ì œê³µí•˜ëŠ” ë‹µë³€ìœ¼ë¡œ, ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                full_answer = answer + warning_message
                
                st.markdown(full_answer, unsafe_allow_html=True)

                with st.expander("AIê°€ ì°¸ê³ í•œ ìµœì¢… ë§¤ë‰´ì–¼ ë‚´ìš© ë³´ê¸° (ì¬ì •ë ¬ í›„)"):
                    for chunk in relevant_chunks:
                        st.info(f"**ì¶œì²˜:** {chunk['source']}, **Page:** {chunk.get('page', 'N/A')}")
                        st.text(chunk['content'])
                        st.divider()

                st.session_state.messages.append({"role": "assistant", "content": full_answer})
