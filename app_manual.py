# íŒŒì¼ ì´ë¦„: app.py (ì •í™•ë„ ê·¹ëŒ€í™” SOTA ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np

# --- 1. ì´ˆê¸° ì„¤ì •: í˜ì´ì§€, API í‚¤ ---
st.set_page_config(page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", page_icon="âš–ï¸", layout="wide")
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("OpenAI API í‚¤ë¥¼ Streamlit Secretsì— ë“±ë¡í•´ì£¼ì„¸ìš”.")

# --- 2. ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ---
@st.cache_resource
def load_models_and_data():
    try:
        embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
        index = faiss.read_index("manuals_vector_db.index")
        with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
            chunks_with_metadata = json.load(f)
        return embedding_model, reranker_model, index, chunks_with_metadata
    except FileNotFoundError:
        return None, None, None, None

# --- 3. Advanced RAG í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ ---
def transform_query(user_question):
    """LLMì„ ì‚¬ìš©í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°œì˜ êµ¬ì²´ì ì¸ ê²€ìƒ‰ìš© ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜"""
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that generates 3 varied versions of a user question to improve document retrieval. The queries should be specific and cover different aspects of the original question. Output a numbered list of questions."},
            {"role": "user", "content": f"Original question: {user_question}"}
        ],
        temperature=0.3,
    )
    # ë³€í™˜ëœ ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
    transformed_queries = [user_question] + [q.strip() for q in response.choices[0].message.content.split('\n') if q.strip() and q[0].isdigit()]
    return list(set(transformed_queries)) # ì¤‘ë³µ ì œê±°

def get_relevant_manual_chunks(queries, k_initial=10, k_final=5):
    """ì¿¼ë¦¬ ë³€í™˜, ê²€ìƒ‰, ì¬ì •ë ¬ì„ í†µí•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    all_retrieved_chunks = []
    for query in queries:
        question_vector = model.encode([query])
        distances, indices = index.search(np.array(question_vector, dtype=np.float32), k_initial)
        all_retrieved_chunks.extend([chunks_with_metadata[i] for i in indices[0]])
    
    # ì¤‘ë³µëœ chunk ì œê±°
    unique_chunks = [dict(t) for t in {tuple(d.items()) for d in all_retrieved_chunks}]
    
    # Rerankerë¡œ ìµœì¢… ìˆœìœ„ ê²°ì •
    pairs = [[queries[0], chunk['content']] for chunk in unique_chunks]
    scores = reranker.predict(pairs)
    ranked_chunks = [chunk for _, chunk in sorted(zip(scores, unique_chunks), key=lambda x: x[0], reverse=True)]
    return ranked_chunks[:k_final]

def generate_answer(user_question, relevant_chunks):
    """ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë‹µë³€ê³¼ ê²€ì¦ ê²°ê³¼ë¥¼ ìƒì„±"""
    context_str = "\n\n".join([f"ì°¸ê³  ë¬¸ì„œ {i+1} (ì¶œì²˜: {chunk['source']}, Page: {chunk.get('page', 'N/A')}):\n{chunk['content']}" for i, chunk in enumerate(relevant_chunks)])
    
    # ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì´ì „ê³¼ ë™ì¼)
    generation_prompt = """
    # Role: ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ë²•ì„ ì „ë¬¸ìœ¼ë¡œ ë‹¤ë£¨ëŠ” 'ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì'ì…ë‹ˆë‹¤... (ì´ì „ ìµœì¢… í”„ë¡¬í”„íŠ¸ ë‚´ìš©)
    """
    
    # ë‹µë³€ ê²€ì¦ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    verification_prompt_template = """
    # Role: ë‹¹ì‹ ì€ AI ë‹µë³€ì„ ê²€ì¦í•˜ëŠ” ì—„ê²©í•œ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤.
    # Task: ì£¼ì–´ì§„ [ìƒì„±ëœ ë‹µë³€]ì´, ì˜¤ì§ [ì°¸ê³  ìë£Œ]ì—ë§Œ 100% ê·¼ê±°í•˜ì—¬ ì‘ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.
    # Instructions:
    1. ë‹µë³€ì˜ ëª¨ë“  ë¬¸ì¥ì„ í•˜ë‚˜ì”© ì½ìœ¼ë©°, ê° ë¬¸ì¥ì´ [ì°¸ê³  ìë£Œ]ì˜ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    2. ë§Œì•½ ë‹µë³€ì— [ì°¸ê³  ìë£Œ]ì— ì—†ëŠ” ë‚´ìš©, ì¶”ì¸¡, ë˜ëŠ” ê³¼ì¥ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ "ë¶€ì •í™•í•¨"ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
    3. ëª¨ë“  ë‚´ìš©ì´ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•  ê²½ìš°ì—ë§Œ "ì •í™•í•¨"ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
    4. ìµœì¢… íŒë‹¨ì„ "íŒë‹¨: [ì •í™•í•¨/ë¶€ì •í™•í•¨]" í˜•ì‹ìœ¼ë¡œ ê°€ì¥ ì²« ì¤„ì— ì¶œë ¥í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ë‹¤ìŒ ì¤„ì— ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
    """
    
    # 1. ë‹µë³€ ìƒì„±
    messages_for_generation = [
        {"role": "system", "content": generation_prompt},
        {"role": "user", "content": f"[ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´]\n{context_str}\n---\n[ì§ˆë¬¸]\n{user_question}"}
    ]
    response = openai.chat.completions.create(model="gpt-4o", messages=messages_for_generation, temperature=0.1)
    generated_answer = response.choices[0].message.content

    # 2. ìƒì„±ëœ ë‹µë³€ ê²€ì¦
    messages_for_verification = [
        {"role": "system", "content": verification_prompt_template},
        {"role": "user", "content": f"[ìƒì„±ëœ ë‹µë³€]\n{generated_answer}\n---\n[ì°¸ê³  ìë£Œ]\n{context_str}"}
    ]
    verification_response = openai.chat.completions.create(model="gpt-4o", messages=messages_for_verification, temperature=0.0)
    verification_result = verification_response.choices[0].message.content
    
    return generated_answer, verification_result, relevant_chunks

# --- 4. ë©”ì¸ UI êµ¬ì„± ---
st.title("âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì› (SOTA Ver.)")
model, reranker, index, chunks_with_metadata = load_models_and_data()

if model is None:
    st.error("ì±—ë´‡ í•µì‹¬ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.success("AI ë²•ë¥  ë³´ì¡°ì›ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ê³µì •ê±°ë˜ë²• ê³µì‹œ ì˜ë¬´ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if isinstance(message["content"], str):
                st.markdown(message["content"], unsafe_allow_html=True)
            else:
                st.write(message["content"])

    if prompt := st.chat_input("RCPS ì „í™˜ê¶Œ í–‰ì‚¬ì— ë”°ë¥¸ ì‹ ì£¼ ì·¨ë“ ì‹œ ê³µì‹œ ì˜ë¬´ê°€ ìˆë‚˜ìš”?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ì§ˆë¬¸ ë¶„ì„ ë° ë³€í™˜ ì¤‘..."):
                transformed_queries = transform_query(prompt)
            
            with st.spinner("ê´€ë ¨ ìë£Œ ê²€ìƒ‰ ë° ì¬ì •ë ¬ ì¤‘..."):
                relevant_chunks = get_relevant_manual_chunks(transformed_queries)

            with st.spinner("ë‹µë³€ ìƒì„± ë° ìì²´ ê²€ì¦ ìˆ˜í–‰ ì¤‘..."):
                answer, verification, sources = generate_answer(prompt, relevant_chunks)
            
            # ìµœì¢… ë‹µë³€ ê°ì²´ ìƒì„±
            final_response = {
                "answer": answer,
                "verification": verification,
            }
            
            # ë‹µë³€ ì¶œë ¥
            is_verified = "ì •í™•í•¨" in verification.split('\n')[0]
            st.markdown(f"#### ğŸ’¬ AI ë²•ë¥  ë³´ì¡°ì› ë‹µë³€ {'âœ…' if is_verified else 'âš ï¸'}")
            st.markdown(answer, unsafe_allow_html=True)
            
            # ê²€ì¦ ê²°ê³¼ ë° ì°¸ê³ ìë£Œ expander
            with st.expander("AI ë‹µë³€ ê²€ì¦ ê²°ê³¼ ë° í•µì‹¬ ì°¸ê³ ìë£Œ ë³´ê¸°"):
                if is_verified:
                    st.success(verification)
                else:
                    st.warning(verification)
                st.divider()
                st.markdown("**í•µì‹¬ ì°¸ê³ ìë£Œ (ì¬ì •ë ¬ í›„)**")
                for chunk in sources:
                    st.info(f"**ì¶œì²˜:** {chunk['source']}, **Page:** {chunk.get('page', 'N/A')}")
                    st.text(chunk['content'])
            
            warning_message = "\n\n---\nâ—†ë³¸ ë‹µë³€ì€ ì „ëµê¸°íšë¶€ê°€ í•™ìŠµì‹œí‚¨ ChatGPTë¥¼ í†µí•´ ì œê³µí•˜ëŠ” ë‹µë³€ìœ¼ë¡œ, ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            st.markdown(warning_message)

            st.session_state.messages.append({"role": "assistant", "content": final_response})
