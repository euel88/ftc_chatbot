# íŒŒì¼ ì´ë¦„: app_manual.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - ì™„ì „ ìˆ˜ì • ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set, TypedDict, Protocol, Iterator, Generator, OrderedDict, Any, Union
import re
from collections import defaultdict, Counter
import time
from dataclasses import dataclass, field
import os
import hashlib
from enum import Enum
import asyncio
import logging
from contextlib import contextmanager
import traceback
import gc
import concurrent.futures
from functools import lru_cache

# ===== ë¡œê¹… ì„¤ì • =====
def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    logger = logging.getLogger('ftc_chatbot')
    logger.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler('ftc_chatbot_debug.log')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# ===== ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ í´ë˜ìŠ¤ =====
class RAGPipelineError(Exception):
    """RAG íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

class IndexError(RAGPipelineError):
    """ì¸ë±ìŠ¤ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class EmbeddingError(RAGPipelineError):
    """ì„ë² ë”© ìƒì„± ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class GPTAnalysisError(RAGPipelineError):
    """GPT ë¶„ì„ ì‹¤íŒ¨ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

# ===== ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € =====
@contextmanager
def error_context(operation_name: str, fallback_value=None):
    """ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    try:
        logger.debug(f"Starting operation: {operation_name}")
        yield
    except Exception as e:
        logger.error(f"Error in {operation_name}: {str(e)}")
        logger.debug(f"Full traceback:\n{traceback.format_exc()}")
        
        if 'st' in globals():
            st.error(f"âš ï¸ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {operation_name}")
            with st.expander("ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                st.code(traceback.format_exc())
        
        if fallback_value is not None:
            return fallback_value
        raise

# ===== í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ =====
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display: none;}
    
    /* ë©”ì¸ í—¤ë” ìŠ¤íƒ€ì¼ */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 900px;
        margin: 0 auto;
    }
    
    /* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ ê°œì„  */
    [data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* ë³µì¡ë„ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .complexity-indicator {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 16px;
        font-size: 0.85rem;
        font-weight: 500;
        margin-left: 8px;
    }
    
    .complexity-simple {
        background-color: #d4edda;
        color: #155724;
    }
    
    .complexity-medium {
        background-color: #fff3cd;
        color: #856404;
    }
    
    .complexity-complex {
        background-color: #f8d7da;
        color: #721c24;
    }
</style>
""", unsafe_allow_html=True)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# ===== íƒ€ì… ì •ì˜ =====
class ChunkDict(TypedDict):
    """ì²­í¬ ë°ì´í„°ì˜ íƒ€ì… ì •ì˜"""
    chunk_id: str
    content: str
    source: str
    page: int
    chunk_type: str
    metadata: str

class AnalysisResult(TypedDict):
    """GPT ë¶„ì„ ê²°ê³¼ì˜ íƒ€ì… ì •ì˜"""
    query_analysis: dict
    legal_concepts: list
    search_strategy: dict
    answer_requirements: dict

# ===== ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')
    
    @property
    def is_latest(self) -> bool:
        """ìµœì‹  ìë£Œ ì—¬ë¶€ í™•ì¸"""
        return self.metadata.get('is_latest', False)

class QueryComplexity(Enum):
    """ì§ˆë¬¸ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"

# ===== LRU ìºì‹œ êµ¬í˜„ =====
class LRUCache:
    """ì‹œê°„ ê¸°ë°˜ ë§Œë£Œë¥¼ ì§€ì›í•˜ëŠ” LRU ìºì‹œ êµ¬í˜„"""
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl
        
    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ë§Œë£Œëœ í•­ëª©ì€ ì œê±°"""
        if key not in self.cache:
            return None
            
        value, timestamp = self.cache[key]
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
            
        self.cache.move_to_end(key)
        return value
        
    def put(self, key: str, value):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if key in self.cache:
            del self.cache[key]
            
        if len(self.cache) >= self.max_size:
            self.cache.popitem(last=False)
            
        self.cache[key] = (value, time.time())
        
    def clear_expired(self):
        """ë§Œë£Œëœ ëª¨ë“  í•­ëª© ì œê±°"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]

# ===== ë¹„ë™ê¸° ì‹¤í–‰ í—¬í¼ =====
def run_async_in_streamlit(coro):
    """Streamlit í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰"""
    try:
        loop = asyncio.get_running_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, coro)
            return future.result()
    except RuntimeError:
        return asyncio.run(coro)

# ===== ë¬¸ì„œ ë²„ì „ ê´€ë¦¬ =====
class DocumentVersionManager:
    """ë¬¸ì„œì˜ ë²„ì „ê³¼ ìµœì‹ ì„±ì„ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.regulation_changes = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜_ê¸ˆì•¡ê¸°ì¤€': [
                {'date': '2023-01-01', 'old_value': '50ì–µì›', 'new_value': '100ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 100ì–µì› ì´ìƒ'},
                {'date': '2020-01-01', 'old_value': '30ì–µì›', 'new_value': '50ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 50ì–µì› ì´ìƒ'}
            ],
            'ê³µì‹œ_ê¸°í•œ': [
                {'date': '2022-07-01', 'old_value': '7ì¼', 'new_value': '5ì¼',
                 'description': 'ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œ ë‹¨ì¶•'}
            ]
        }
        
        self.critical_patterns = {
            'ê¸ˆì•¡': r'(\d+)ì–µ\s*ì›',
            'ë¹„ìœ¨': r'(\d+(?:\.\d+)?)\s*%',
            'ê¸°í•œ': r'(\d+)\s*ì¼',
            'ë‚ ì§œ': r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        }
    
    def extract_document_date(self, chunk: Dict) -> Optional[str]:
        """ë¬¸ì„œì—ì„œ ì‘ì„±/ê°œì • ë‚ ì§œ ì¶”ì¶œ"""
        content = chunk.get('content', '')
        metadata = json.loads(chunk.get('metadata', '{}'))
        
        if 'document_date' in metadata:
            return metadata['document_date']
        
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*ê°œì •',
            r'ì‹œí–‰ì¼\s*:\s*(\d{4})ë…„\s*(\d{1,2})ì›”',
            r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                return self._normalize_date(match.group(0))
        
        return None
    
    def _normalize_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        date_str = re.sub(r'[^\d]', '-', date_str)
        parts = date_str.split('-')
        if len(parts) >= 2:
            year = parts[0] if len(parts[0]) == 4 else '20' + parts[0]
            month = parts[1].zfill(2)
            day = parts[2].zfill(2) if len(parts) > 2 else '01'
            return f"{year}-{month}-{day}"
        return None
    
    def check_for_outdated_info(self, content: str, document_date: str = None) -> List[Dict]:
        """êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        warnings = []
        
        amount_match = re.search(r'(\d+)ì–µ\s*ì›.*ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', content)
        if amount_match:
            amount = int(amount_match.group(1))
            if amount == 50:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '50ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
            elif amount == 30:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '30ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
        
        return warnings

# ===== ì¶©ëŒ í•´ê²° ì‹œìŠ¤í…œ =====
class ConflictResolver:
    """ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, version_manager: DocumentVersionManager):
        self.version_manager = version_manager
    
    def resolve_conflicts(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        
        for result in results:
            doc_date = result.document_date
            warnings = self.version_manager.check_for_outdated_info(result.content, doc_date)
            
            if warnings:
                result.metadata['warnings'] = warnings
                result.metadata['has_outdated_info'] = True
            else:
                result.metadata['has_outdated_info'] = False
        
        critical_info = self._extract_critical_info(results, query)
        if critical_info:
            conflicts = self._find_conflicts(critical_info)
            if conflicts:
                results = self._prioritize_latest_info(results, conflicts)
        
        results.sort(key=lambda r: (
            not r.metadata.get('has_outdated_info', False),
            r.document_date or '1900-01-01',
            r.score
        ), reverse=True)
        
        return results
    
    def _extract_critical_info(self, results: List[SearchResult], query: str) -> Dict:
        """ê²°ê³¼ì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ"""
        critical_info = defaultdict(list)
        
        for i, result in enumerate(results):
            amounts = re.findall(r'(\d+)ì–µ\s*ì›', result.content)
            for amount in amounts:
                critical_info['amounts'].append({
                    'value': amount + 'ì–µì›',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', result.content)
            for pct in percentages:
                critical_info['percentages'].append({
                    'value': pct + '%',
                    'result_index': i,
                    'context': result.content[:100]
                })
        
        return dict(critical_info)
    
    def _find_conflicts(self, critical_info: Dict) -> List[Dict]:
        """ì¤‘ìš” ì •ë³´ ê°„ ì¶©ëŒ ì°¾ê¸°"""
        conflicts = []
        
        if 'amounts' in critical_info:
            amount_values = set()
            for item in critical_info['amounts']:
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in item['context']:
                    amount_values.add(item['value'])
            
            if len(amount_values) > 1 and ('50ì–µì›' in amount_values or '30ì–µì›' in amount_values):
                conflicts.append({
                    'type': 'amount_conflict',
                    'values': list(amount_values),
                    'correct_value': '100ì–µì›'
                })
        
        return conflicts
    
    def _prioritize_latest_info(self, results: List[SearchResult], conflicts: List[Dict]) -> List[SearchResult]:
        """ì¶©ëŒì´ ìˆì„ ë•Œ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        for conflict in conflicts:
            if conflict['type'] == 'amount_conflict':
                for i, result in enumerate(results):
                    if any(old_val in result.content for old_val in ['50ì–µì›', '30ì–µì›']):
                        results[i].score *= 0.5
                        results[i].metadata['score_reduced'] = True
                        results[i].metadata['reduction_reason'] = 'outdated_amount'
        
        return results

# ===== GPT-4o ì§ˆë¬¸ ë¶„ì„ê¸° =====
class GPT4oQueryAnalyzer:
    """GPT-4oë¥¼ í™œìš©í•œ í†µí•© ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½"""
    
    def __init__(self):
        self.analysis_cache = LRUCache(max_size=100, ttl=3600)
    
    def analyze_and_strategize(self, query: str, available_chunks_info: Dict) -> Dict:
        """GPT-4oë¡œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½"""
        
        cache_data = f"{query}_{json.dumps(available_chunks_info, sort_keys=True)}"
        cache_key = hashlib.md5(cache_data.encode()).hexdigest()
        
        cached_analysis = self.analysis_cache.get(cache_key)
        if cached_analysis:
            logger.info(f"Cache hit for query analysis: {query[:50]}...")
            return cached_analysis
        
        prompt = f"""
        ë‹¹ì‹ ì€ ê³µì •ê±°ë˜ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ì •ë³´:
        - ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼: {available_chunks_info.get('ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 0)}ê°œ ì²­í¬
        - í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼: {available_chunks_info.get('í˜„í™©ê³µì‹œ', 0)}ê°œ ì²­í¬
        - ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼: {available_chunks_info.get('ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­', 0)}ê°œ ì²­í¬
        
        ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "query_analysis": {{
                "core_intent": "ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ (í•œ ë¬¸ì¥)",
                "actual_complexity": "simple/medium/complex",
                "complexity_reason": "ë³µì¡ë„ íŒë‹¨ ì´ìœ "
            }},
            "legal_concepts": [
                {{
                    "concept": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜/í˜„í™©ê³µì‹œ/ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­",
                    "relevance": "primary/secondary",
                    "specific_aspects": ["ê¸ˆì•¡ê¸°ì¤€", "ì ˆì°¨", "ê³µì‹œì˜ë¬´"]
                }}
            ],
            "search_strategy": {{
                "approach": "direct_lookup/focused_search/comprehensive_analysis",
                "primary_manual": "ì£¼ë¡œ ê²€ìƒ‰í•  ë§¤ë‰´ì–¼",
                "search_keywords": ["í•µì‹¬ ê²€ìƒ‰ì–´1", "í•µì‹¬ ê²€ìƒ‰ì–´2"],
                "expected_chunks_needed": 10,
                "rationale": "ì´ ì „ëµì„ ì„ íƒí•œ ì´ìœ "
            }},
            "answer_requirements": {{
                "needs_specific_numbers": true,
                "needs_process_steps": false,
                "needs_timeline": false,
                "needs_exceptions": false,
                "needs_multiple_perspectives": false
            }}
        }}
        """
        
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            analysis = json.loads(response.choices[0].message.content)
            
            self.analysis_cache.put(cache_key, analysis)
            
            if len(self.analysis_cache.cache) % 10 == 0:
                self.analysis_cache.clear_expired()
            
            return analysis
            
        except Exception as e:
            logger.error(f"GPT-4o analysis error: {str(e)}")
            return self._get_fallback_strategy(query)
    
    def _get_fallback_strategy(self, query: str) -> Dict:
        """GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì „ëµ"""
        return {
            "query_analysis": {
                "core_intent": "ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨ - ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰",
                "actual_complexity": "medium",
                "complexity_reason": "ìë™ ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¤‘ê°„ ë³µì¡ë„ ê°€ì •"
            },
            "legal_concepts": [],
            "search_strategy": {
                "approach": "focused_search",
                "primary_manual": "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜",
                "search_keywords": query.split()[:5],
                "expected_chunks_needed": 30,
                "rationale": "ê¸°ë³¸ ê²€ìƒ‰ ì „ëµ"
            },
            "answer_requirements": {
                "needs_specific_numbers": True,
                "needs_process_steps": True,
                "needs_timeline": True,
                "needs_exceptions": False,
                "needs_multiple_perspectives": False
            }
        }

# ===== ì§ˆë¬¸ ë¶„ë¥˜ê¸° =====
class QuestionClassifier:
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë§¤ë‰´ì–¼ì„ ìš°ì„  ê²€ìƒ‰í• ì§€ ê²°ì •"""
    
    def __init__(self):
        self.categories = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': {
                'keywords': ['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 'ë‚´ë¶€ê±°ë˜', 'ì´ì‚¬íšŒ ì˜ê²°', 'ì´ì‚¬íšŒ', 'ì˜ê²°', 
                           'ê³„ì—´ì‚¬', 'ê³„ì—´íšŒì‚¬', 'íŠ¹ìˆ˜ê´€ê³„ì¸', 'ìê¸ˆ', 'ëŒ€ì—¬', 'ì°¨ì…', 'ë³´ì¦',
                           'ìê¸ˆê±°ë˜', 'ìœ ê°€ì¦ê¶Œ', 'ìì‚°ê±°ë˜', '50ì–µ', 'ê±°ë˜ê¸ˆì•¡'],
                'patterns': [r'ì´ì‚¬íšŒ.*ì˜ê²°', r'ê³„ì—´.*ê±°ë˜', r'ë‚´ë¶€.*ê±°ë˜'],
                'manual_pattern': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜.*ë§¤ë‰´ì–¼',
                'priority': 1
            },
            'í˜„í™©ê³µì‹œ': {
                'keywords': ['í˜„í™©ê³µì‹œ', 'ê¸°ì—…ì§‘ë‹¨', 'ì†Œì†íšŒì‚¬', 'ë™ì¼ì¸', 'ì¹œì¡±', 
                           'ì§€ë¶„ìœ¨', 'ì„ì›', 'ìˆœí™˜ì¶œì', 'ìƒí˜¸ì¶œì', 'ì§€ë°°êµ¬ì¡°',
                           'ê³„ì—´í¸ì…', 'ê³„ì—´ì œì™¸', 'ì£¼ì£¼í˜„í™©', 'ì„ì›í˜„í™©'],
                'patterns': [r'ê¸°ì—…ì§‘ë‹¨.*í˜„í™©', r'ì†Œì†.*íšŒì‚¬', r'ì§€ë¶„.*ë³€ë™'],
                'manual_pattern': 'ê¸°ì—…ì§‘ë‹¨í˜„í™©ê³µì‹œ.*ë§¤ë‰´ì–¼',
                'priority': 2
            },
            'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': {
                'keywords': ['ë¹„ìƒì¥', 'ì¤‘ìš”ì‚¬í•­', 'ì£¼ì‹', 'ì–‘ë„', 'ì–‘ìˆ˜', 'í•©ë³‘', 
                           'ë¶„í• ', 'ì˜ì—…ì–‘ë„', 'ì„ì›ë³€ê²½', 'ì¦ì', 'ê°ì',
                           'ì •ê´€ë³€ê²½', 'í•´ì‚°', 'ì²­ì‚°'],
                'patterns': [r'ë¹„ìƒì¥.*ê³µì‹œ', r'ì£¼ì‹.*ì–‘ë„', r'ì¤‘ìš”.*ì‚¬í•­'],
                'manual_pattern': 'ë¹„ìƒì¥ì‚¬.*ì¤‘ìš”ì‚¬í•­.*ë§¤ë‰´ì–¼',
                'priority': 3
            }
        }
    
    def classify(self, question: str) -> Tuple[str, float]:
        """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì‹ ë¢°ë„ë¥¼ ë°˜í™˜"""
        question_lower = question.lower()
        scores = {}
        
        for category, info in self.categories.items():
            score = 0
            matched_keywords = []
            
            for i, keyword in enumerate(info['keywords']):
                if keyword in question_lower:
                    weight = 1.0 if i < 5 else 0.7
                    score += weight
                    matched_keywords.append(keyword)
            
            for pattern in info.get('patterns', []):
                if re.search(pattern, question_lower):
                    score += 1.5
            
            scores[category] = score
        
        if scores:
            best_category = max(scores, key=scores.get)
            max_possible_score = len(self.categories[best_category]['keywords']) + \
                               len(self.categories[best_category].get('patterns', [])) * 1.5
            confidence = min(scores[best_category] / max_possible_score, 1.0)
            
            if confidence < 0.15:
                return None, 0.0
                
            return best_category, confidence
        
        return None, 0.0

# ===== ë³µì¡ë„ í‰ê°€ê¸° =====
class ComplexityAssessor:
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •"""
    
    def __init__(self):
        self.simple_indicators = [
            r'ì–¸ì œ', r'ë©°ì¹ ', r'ê¸°í•œ', r'ë‚ ì§œ', r'ê¸ˆì•¡', r'%', r'ì–¼ë§ˆ',
            r'ì •ì˜[ê°€ëŠ”]?', r'ë¬´ì—‡', r'ëœ»[ì´ì€]?', r'ì˜ë¯¸[ê°€ëŠ”]?'
        ]
        
        self.complex_indicators = [
            r'ë™ì‹œì—', r'ì—¬ëŸ¬', r'ë³µí•©', r'ì—°ê´€', r'ì˜í–¥',
            r'ë§Œ[ì•½ì¼].*ê²½ìš°', r'[AB].*ë™ì‹œ.*[CD]', r'ê±°ë˜.*ì—¬ëŸ¬',
            r'ì „ì²´ì ', r'ì¢…í•©ì ', r'ë¶„ì„', r'ê²€í† ', r'í‰ê°€',
            r'ë¦¬ìŠ¤í¬', r'ìœ„í—˜', r'ëŒ€ì‘', r'ì „ëµ'
        ]
        
        self.medium_indicators = [
            r'ì–´ë–»ê²Œ', r'ë°©ë²•', r'ì ˆì°¨', r'ê³¼ì •',
            r'ì£¼ì˜', r'ì˜ˆì™¸', r'íŠ¹ë³„', r'ê³ ë ¤'
        ]
    
    def assess(self, query: str) -> Tuple[QueryComplexity, float, Dict]:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ê³  ê´€ë ¨ ì •ë³´ ë°˜í™˜"""
        query_lower = query.lower()
        
        simple_score = sum(1 for pattern in self.simple_indicators 
                         if re.search(pattern, query_lower))
        complex_score = sum(2 for pattern in self.complex_indicators 
                          if re.search(pattern, query_lower))
        medium_score = sum(1.5 for pattern in self.medium_indicators 
                         if re.search(pattern, query_lower))
        
        if len(query) > 150:
            complex_score += 2
        elif len(query) > 100:
            complex_score += 1
        elif len(query) < 30:
            simple_score += 0.5
            
        if re.search(r'[AB]íšŒì‚¬.*[CD]íšŒì‚¬', query_lower):
            complex_score += 2
        if '?' in query and query.count('?') > 1:
            complex_score += 1
            
        total_score = simple_score + medium_score + complex_score
        
        if total_score == 0:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.5
        elif complex_score > simple_score * 3:
            complexity = QueryComplexity.COMPLEX
            confidence = min(complex_score / (total_score + 1), 0.9)
        elif simple_score > complex_score * 2:
            complexity = QueryComplexity.SIMPLE
            confidence = min(simple_score / (total_score + 1), 0.9)
        else:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.6
            
        analysis = {
            'simple_score': simple_score,
            'medium_score': medium_score,
            'complex_score': complex_score,
            'query_length': len(query),
            'estimated_cost_multiplier': self._estimate_cost_multiplier(complexity)
        }
        
        return complexity, confidence, analysis
    
    def _estimate_cost_multiplier(self, complexity: QueryComplexity) -> float:
        """ë³µì¡ë„ì— ë”°ë¥¸ ì˜ˆìƒ ë¹„ìš© ë°°ìˆ˜"""
        multipliers = {
            QueryComplexity.SIMPLE: 1.0,
            QueryComplexity.MEDIUM: 3.0,
            QueryComplexity.COMPLEX: 10.0
        }
        return multipliers[complexity]

# ===== í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ =====
class HybridRAGPipeline:
    """GPT-4o ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, embedding_model, reranker_model, index, chunks):
        if not chunks:
            raise ValueError("No chunks provided to HybridRAGPipeline")
        
        if index.ntotal == 0:
            raise ValueError("FAISS index is empty")
        
        # ì„ë² ë”© ì°¨ì› ê²€ì¦
        test_embedding = embedding_model.encode(["test"])
        if len(test_embedding[0]) != index.d:
            raise ValueError(f"Embedding dimension {len(test_embedding[0])} doesn't match index dimension {index.d}")
        
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunks = chunks
        
        self.classifier = QuestionClassifier()
        self.complexity_assessor = ComplexityAssessor()
        self.gpt4o_analyzer = GPT4oQueryAnalyzer()
        
        self.version_manager = DocumentVersionManager()
        self.conflict_resolver = ConflictResolver(self.version_manager)
        
        self.manual_indices = self._build_manual_indices()
        
        self.chunks_info = {
            category: len(indices) 
            for category, indices in self.manual_indices.items()
        }
        
        self.search_cache = LRUCache(max_size=50, ttl=1800)
        self.embedding_cache = LRUCache(max_size=100, ttl=3600)
        
        self._extract_chunk_dates()
        
        logger.info(f"HybridRAGPipeline initialized with {len(chunks)} chunks")
    
    def _extract_chunk_dates(self):
        """ëª¨ë“  ì²­í¬ì˜ ë‚ ì§œ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ"""
        for chunk in self.chunks:
            doc_date = self.version_manager.extract_document_date(chunk)
            if doc_date:
                metadata = json.loads(chunk.get('metadata', '{}'))
                metadata['document_date'] = doc_date
                chunk['metadata'] = json.dumps(metadata)
    
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx, chunk in enumerate(self.chunks):
            source = chunk.get('source', '').lower()
            
            if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜'].append(idx)
            elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                indices['í˜„í™©ê³µì‹œ'].append(idx)
            elif 'ë¹„ìƒì¥' in source:
                indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­'].append(idx)
            else:
                indices['ê¸°íƒ€'].append(idx)
        
        return dict(indices)
    
    def _get_query_embedding(self, query: str) -> np.ndarray:
        """ì¿¼ë¦¬ ì„ë² ë”©ì„ ìºì‹œì™€ í•¨ê»˜ ê°€ì ¸ì˜¤ê¸°"""
        cached_embedding = self.embedding_cache.get(query)
        if cached_embedding is not None:
            return cached_embedding
            
        embedding = self.embedding_model.encode([query])
        embedding = np.array(embedding, dtype=np.float32)
        
        self.embedding_cache.put(query, embedding)
        
        return embedding
    
    async def process_query(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """GPT-4oê°€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì²˜ë¦¬ ë°©ì‹ì„ ì„ íƒ"""
        start_time = time.time()
        
        analysis_start = time.time()
        try:
            gpt_analysis = self.gpt4o_analyzer.analyze_and_strategize(
                query, self.chunks_info
            )
            analysis_time = time.time() - analysis_start
        except Exception as e:
            logger.error(f"GPT-4o analysis failed: {str(e)}, falling back to rule-based")
            return self._fallback_process_query(query, top_k)
        
        actual_complexity = gpt_analysis['query_analysis']['actual_complexity']
        search_approach = gpt_analysis['search_strategy']['approach']
        
        stats = {
            'gpt_analysis': gpt_analysis,
            'analysis_time': analysis_time,
            'actual_complexity': actual_complexity,
            'search_approach': search_approach
        }
        
        if search_approach == 'direct_lookup':
            results, search_stats = await self._gpt_guided_direct_search(
                query, gpt_analysis, top_k
            )
            stats['processing_mode'] = 'gpt_guided_direct'
            
        elif search_approach == 'focused_search':
            results, search_stats = await self._gpt_guided_focused_search(
                query, gpt_analysis, top_k
            )
            stats['processing_mode'] = 'gpt_guided_focused'
            
        else:  # comprehensive_analysis
            results, search_stats = await self._gpt_guided_comprehensive_search(
                query, gpt_analysis, top_k
            )
            stats['processing_mode'] = 'gpt_guided_comprehensive'
        
        results = self.conflict_resolver.resolve_conflicts(results, query)
        
        outdated_warnings = []
        for result in results:
            if result.metadata.get('has_outdated_info'):
                outdated_warnings.extend(result.metadata.get('warnings', []))
        
        stats.update(search_stats)
        stats['total_time'] = time.time() - start_time
        stats['outdated_warnings'] = outdated_warnings
        stats['has_version_conflicts'] = len(outdated_warnings) > 0
        
        return results, stats
    
    async def _gpt_guided_direct_search(self, query: str, gpt_analysis: Dict, 
                                       top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§ì ‘ ê²€ìƒ‰"""
        start_time = time.time()
        
        primary_manual = gpt_analysis['search_strategy']['primary_manual']
        search_keywords = gpt_analysis['search_strategy']['search_keywords']
        
        target_indices = self.manual_indices.get(primary_manual, [])[:100]
        
        if not target_indices:
            logger.warning(f"No indices for manual '{primary_manual}', using all chunks")
            target_indices = list(range(min(len(self.chunks), 50)))
            if not target_indices:
                return [], {
                    'search_time': time.time() - start_time,
                    'searched_chunks': 0,
                    'search_method': 'direct_vector',
                    'warning': 'No chunks available'
                }
        
        enhanced_query = f"{query} {' '.join(search_keywords)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        k_search = min(len(target_indices), max(1, top_k * 3))
        
        try:
            scores, indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error in direct search: {str(e)}")
            return [], {
                'search_time': time.time() - start_time,
                'searched_chunks': len(target_indices),
                'search_method': 'direct_vector',
                'error': str(e)
            }
        
        results = []
        target_set = set(target_indices)
        
        for idx, score in zip(indices[0], scores[0]):
            if idx in target_set and idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
                if len(results) >= top_k:
                    break
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': len(target_indices),
            'search_method': 'direct_vector'
        }
        
        return results, stats
    
    async def _gpt_guided_focused_search(self, query: str, gpt_analysis: Dict, 
                                        top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§‘ì¤‘ ê²€ìƒ‰ (ì™„ì „íˆ ìˆ˜ì •ëœ ë²„ì „)"""
        start_time = time.time()
        stats = {
            'search_time': 0,
            'searched_chunks': 0,
            'search_method': 'focused_vector',
            'errors': []
        }
        
        try:
            if not query or not isinstance(query, str):
                raise ValueError(f"Invalid query: {query}")
            if not isinstance(top_k, int) or top_k <= 0:
                raise ValueError(f"Invalid top_k value: {top_k}")
                
            if not gpt_analysis or 'search_strategy' not in gpt_analysis:
                logger.warning("Invalid or missing GPT analysis, using defaults")
                gpt_analysis = self._get_default_analysis(query)
                
            primary_manual = gpt_analysis['search_strategy'].get('primary_manual', '')
            search_keywords = gpt_analysis['search_strategy'].get('search_keywords', [])
            expected_chunks = gpt_analysis['search_strategy'].get('expected_chunks_needed', 10)
            
            logger.info(f"Search strategy - Manual: {primary_manual}, Keywords: {search_keywords}")
            
            search_limit = min(expected_chunks * 2, 200)
            target_indices = self.manual_indices.get(primary_manual, [])[:search_limit]
            
            if not target_indices:
                logger.warning(f"No target indices found for manual '{primary_manual}'")
                stats['errors'].append({
                    'type': 'missing_indices',
                    'manual': primary_manual,
                    'action': 'fallback_to_all_chunks'
                })
                
                target_indices = list(range(min(len(self.chunks), 100)))
                if not target_indices:
                    logger.error("No chunks available for search")
                    return [], stats
            
            requirements = gpt_analysis.get('answer_requirements', {})
            if requirements.get('needs_specific_numbers'):
                try:
                    filtered_indices = [
                        idx for idx in target_indices 
                        if idx < len(self.chunks) and
                        re.search(r'\d+ì–µ|\d+%', self.chunks[idx].get('content', ''))
                    ]
                    if filtered_indices:
                        target_indices = filtered_indices
                        logger.debug(f"Filtered to {len(filtered_indices)} chunks with numbers")
                except Exception as e:
                    logger.warning(f"Error during number filtering: {str(e)}")
                    stats['errors'].append({
                        'type': 'filter_error',
                        'error': str(e)
                    })
            
            try:
                enhanced_query = f"{query} {' '.join(search_keywords)}"
                
                with error_context("Creating query embedding"):
                    query_vector = self._get_query_embedding(enhanced_query)
                    
                k_search = min(len(target_indices), max(1, top_k * 5))
                logger.debug(f"Performing FAISS search with k={k_search}")
                
                scores, indices = self.index.search(query_vector, k_search)
                
            except Exception as e:
                logger.error(f"FAISS search error: {str(e)}")
                stats['errors'].append({
                    'type': 'faiss_error',
                    'error': str(e),
                    'k_search': k_search,
                    'target_indices_count': len(target_indices)
                })
                return [], stats
            
            results = []
            target_set = set(target_indices)
            invalid_results_count = 0
            
            for idx, score in zip(indices[0], scores[0]):
                try:
                    if idx < 0 or idx >= len(self.chunks):
                        invalid_results_count += 1
                        continue
                        
                    if idx not in target_set:
                        continue
                        
                    chunk = self.chunks[idx]
                    
                    if not chunk or 'content' not in chunk:
                        logger.warning(f"Invalid chunk at index {idx}")
                        invalid_results_count += 1
                        continue
                    
                    relevance_boost = self._calculate_gpt_relevance(
                        chunk['content'], gpt_analysis
                    )
                    
                    result = SearchResult(
                        chunk_id=chunk.get('chunk_id', str(idx)),
                        content=chunk.get('content', ''),
                        score=float(score) * (1 + relevance_boost),
                        source=chunk.get('source', 'Unknown'),
                        page=chunk.get('page', 0),
                        chunk_type=chunk.get('chunk_type', 'unknown'),
                        metadata=json.loads(chunk.get('metadata', '{}'))
                    )
                    
                    results.append(result)
                    
                    if len(results) >= top_k * 2:
                        break
                        
                except Exception as e:
                    logger.error(f"Error processing result at index {idx}: {str(e)}")
                    invalid_results_count += 1
                    continue
            
            if invalid_results_count > 0:
                logger.warning(f"Found {invalid_results_count} invalid results during processing")
                stats['errors'].append({
                    'type': 'invalid_results',
                    'count': invalid_results_count
                })
            
            results.sort(key=lambda x: x.score, reverse=True)
            results = results[:top_k]
            
            stats.update({
                'search_time': time.time() - start_time,
                'searched_chunks': len(target_indices),
                'results_count': len(results),
                'has_errors': len(stats['errors']) > 0
            })
            
            logger.info(f"Search completed in {stats['search_time']:.2f}s, found {len(results)} results")
            
            return results, stats
            
        except Exception as e:
            logger.error(f"Unexpected error in focused search: {str(e)}")
            logger.debug(f"Full traceback:\n{traceback.format_exc()}")
            
            stats['errors'].append({
                'type': 'unexpected_error',
                'error': str(e),
                'traceback': traceback.format_exc()
            })
            stats['search_time'] = time.time() - start_time
            
            return [], stats
    
    async def _gpt_guided_comprehensive_search(self, query: str, gpt_analysis: Dict, 
                                              top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¢…í•© ê²€ìƒ‰"""
        start_time = time.time()
        
        all_results = []
        
        for concept in gpt_analysis['legal_concepts']:
            if concept['relevance'] in ['primary', 'secondary']:
                manual = concept['concept']
                if manual in self.manual_indices:
                    partial_results = await self._search_in_manual(
                        query, manual, concept['specific_aspects'], top_k // 2
                    )
                    all_results.extend(partial_results)
        
        seen_chunks = set()
        unique_results = []
        for result in sorted(all_results, key=lambda x: x.score, reverse=True):
            if result.chunk_id not in seen_chunks:
                seen_chunks.add(result.chunk_id)
                unique_results.append(result)
                if len(unique_results) >= top_k:
                    break
        
        stats = {
            'search_time': time.time() - start_time,
            'searched_chunks': sum(len(self.manual_indices.get(c['concept'], [])) 
                                 for c in gpt_analysis['legal_concepts'] 
                                 if c['relevance'] in ['primary', 'secondary']),
            'search_method': 'comprehensive_multi_manual'
        }
        
        return unique_results, stats
    
    def _calculate_gpt_relevance(self, content: str, gpt_analysis: Dict) -> float:
        """GPT ë¶„ì„ ê²°ê³¼ì™€ ì²­í¬ ë‚´ìš©ì˜ ê´€ë ¨ì„± ê³„ì‚°"""
        relevance_boost = 0.0
        content_lower = content.lower()
        
        for keyword in gpt_analysis['search_strategy']['search_keywords']:
            if keyword.lower() in content_lower:
                relevance_boost += 0.1
        
        requirements = gpt_analysis['answer_requirements']
        if requirements.get('needs_specific_numbers') and re.search(r'\d+ì–µ|\d+%', content):
            relevance_boost += 0.2
        if requirements.get('needs_timeline') and re.search(r'\d+ì¼|ê¸°í•œ', content):
            relevance_boost += 0.2
        if requirements.get('needs_process_steps') and re.search(r'ì ˆì°¨|ë‹¨ê³„|ìˆœì„œ', content):
            relevance_boost += 0.15
        
        return min(relevance_boost, 0.5)
    
    async def _search_in_manual(self, query: str, manual: str, aspects: List[str], 
                               limit: int) -> List[SearchResult]:
        """íŠ¹ì • ë§¤ë‰´ì–¼ ë‚´ì—ì„œ ê²€ìƒ‰"""
        indices = self.manual_indices.get(manual, [])[:100]
        
        if not indices:
            logger.warning(f"No indices found for manual '{manual}'")
            return []
        
        enhanced_query = f"{query} {' '.join(aspects)}"
        query_vector = self._get_query_embedding(enhanced_query)
        
        k_search = min(len(indices), max(1, limit * 3))
        
        try:
            scores, search_indices = self.index.search(query_vector, k_search)
        except Exception as e:
            logger.error(f"FAISS search error in manual search: {str(e)}")
            return []
        
        results = []
        indices_set = set(indices)
        
        for idx, score in zip(search_indices[0], scores[0]):
            if idx in indices_set and idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(idx)),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
                if len(results) >= limit:
                    break
        
        return results
    
    def _fallback_process_query(self, query: str, top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±"""
        results, stats = self._fast_traditional_search(query, top_k)
        stats['processing_mode'] = 'fallback_traditional'
        stats['gpt_failure'] = True
        return results, stats
    
    def _fast_traditional_search(self, query: str, top_k: int) -> Tuple[List[SearchResult], Dict]:
        """ê¸°ì¡´ì˜ ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰ ë°©ì‹"""
        start_time = time.time()
        
        cache_key = hashlib.md5(f"{query}_{top_k}_traditional".encode()).hexdigest()
        cached = self.search_cache.get(cache_key)
        if cached:
            stats = cached['stats'].copy()
            stats['cache_hit'] = True
            return cached['results'], stats
        
        category, cat_confidence = self.classifier.classify(query)
        
        if category and cat_confidence > 0.3:
            primary_indices = self.manual_indices.get(category, [])
            if len(primary_indices) > 200:
                primary_indices = primary_indices[:200]
            secondary_indices = []
        else:
            primary_indices = list(range(min(len(self.chunks), 300)))
            secondary_indices = []
        
        query_vector = self._get_query_embedding(query)
        
        k_search = min(len(primary_indices), max(1, top_k * 5))
        scores, indices = self.index.search(query_vector, k_search)
        
        results = []
        seen_chunks = set()
        
        if primary_indices:
            primary_set = set(primary_indices)
            for idx, score in zip(indices[0], scores[0]):
                if idx in primary_set and idx not in seen_chunks and idx < len(self.chunks):
                    seen_chunks.add(idx)
                    chunk = self.chunks[idx]
                    result = SearchResult(
                        chunk_id=chunk.get('chunk_id', str(idx)),
                        content=chunk['content'],
                        score=float(score),
                        source=chunk['source'],
                        page=chunk['page'],
                        chunk_type=chunk.get('chunk_type', 'unknown'),
                        metadata=json.loads(chunk.get('metadata', '{}'))
                    )
                    results.append(result)
                    if len(results) >= top_k:
                        break
        
        stats = {
            'search_time': time.time() - start_time,
            'category': category,
            'category_confidence': cat_confidence,
            'cache_hit': False,
            'searched_chunks': len(primary_indices)
        }
        
        if stats['search_time'] < 0.5 and len(self.search_cache.cache) < self.search_cache.max_size:
            self.search_cache.put(cache_key, {
                'results': results,
                'stats': stats,
                'timestamp': time.time()
            })
        
        return results, stats
    
    def _get_default_analysis(self, query: str) -> Dict:
        """GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ë¶„ì„ ìƒì„±"""
        return {
            'query_analysis': {
                'core_intent': query,
                'actual_complexity': 'medium',
                'complexity_reason': 'Default analysis due to GPT failure'
            },
            'search_strategy': {
                'primary_manual': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜',
                'search_keywords': query.split()[:5],
                'expected_chunks_needed': 20,
                'approach': 'focused_search'
            },
            'answer_requirements': {
                'needs_specific_numbers': True,
                'needs_process_steps': True
            }
        }

# ===== ë‹µë³€ ìƒì„± í•¨ìˆ˜ =====
def determine_temperature(query: str, complexity: str) -> float:
    """ì§ˆë¬¸ ìœ í˜•ê³¼ ë³µì¡ë„ì— ë”°ë¼ ìµœì ì˜ temperature ê²°ì •"""
    query_lower = query.lower()
    
    base_temps = {
        'simple': 0.1,
        'medium': 0.3,
        'complex': 0.5
    }
    
    temp = base_temps.get(complexity, 0.3)
    
    if any(keyword in query_lower for keyword in ['ì–¸ì œ', 'ë©°ì¹ ', 'ê¸°í•œ', 'ë‚ ì§œ', 'ê¸ˆì•¡', '%']):
        temp = min(temp, 0.1)
    elif any(keyword in query_lower for keyword in ['ì „ëµ', 'ëŒ€ì‘', 'ë¦¬ìŠ¤í¬', 'ì£¼ì˜', 'ê¶Œì¥']):
        temp = max(temp, 0.7)
    
    return temp

def generate_answer(query: str, results: List[SearchResult], stats: Dict) -> str:
    """GPT-4oë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±"""
    
    has_outdated = stats.get('has_version_conflicts', False)
    outdated_warnings = stats.get('outdated_warnings', [])
    
    context_parts = []
    latest_info_parts = []
    outdated_info_parts = []
    
    for i, result in enumerate(results[:5]):
        context_str = f"""
[ì°¸ê³  {i+1}] {result.source} (í˜ì´ì§€ {result.page})
{result.content}
"""
        if result.metadata.get('has_outdated_info'):
            outdated_info_parts.append(context_str)
        else:
            latest_info_parts.append(context_str)
    
    context_parts = latest_info_parts + outdated_info_parts
    context = "\n---\n".join(context_parts)
    
    critical_updates = ""
    if has_outdated:
        critical_updates = "\n\n[ì¤‘ìš” ë²•ê·œ ë³€ê²½ì‚¬í•­]"
        for warning in outdated_warnings:
            if warning['severity'] == 'critical':
                critical_updates += f"\n- {warning['regulation']}: {warning['found']} â†’ {warning['current']} (ë³€ê²½ì¼: {warning['changed_date']})"
    
    gpt_analysis = stats.get('gpt_analysis', {})
    complexity = gpt_analysis.get('query_analysis', {}).get('actual_complexity', 'medium')
    temperature = determine_temperature(query, complexity)
    
    mode_instructions = {
        'gpt_guided_direct': "GPT-4oê°€ ì„ íƒí•œ ì§ì ‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        'gpt_guided_focused': "GPT-4oê°€ ë¶„ì„í•œ í•µì‹¬ ì£¼ì œì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        'gpt_guided_comprehensive': "GPT-4oê°€ íŒŒì•…í•œ ì—¬ëŸ¬ ê´€ë ¨ ì£¼ì œë¥¼ ì¢…í•©í•˜ì—¬ í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        'fallback_traditional': "ì œê³µëœ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    }
    
    mode = stats.get('processing_mode', 'fallback_traditional')
    extra_instruction = mode_instructions.get(mode, "")
    
    category = stats.get('category')
    if not category and gpt_analysis:
        primary_manual = gpt_analysis.get('search_strategy', {}).get('primary_manual')
        category = primary_manual
    
    if category:
        category_instructions = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': "íŠ¹íˆ ì´ì‚¬íšŒ ì˜ê²° ìš”ê±´, ê³µì‹œ ê¸°í•œ, ë©´ì œ ì¡°ê±´ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”. ê¸ˆì•¡ ê¸°ì¤€ì€ ë°˜ë“œì‹œ ìµœì‹  ê¸°ì¤€(100ì–µì› ì´ìƒ ë˜ëŠ” ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ)ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
            'í˜„í™©ê³µì‹œ': "ê³µì‹œ ì£¼ì²´, ì‹œê¸°, ì œì¶œ ì„œë¥˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.",
            'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': "ê³µì‹œ ëŒ€ìƒ ê±°ë˜, ê¸°í•œ, ì œì¶œ ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”."
        }
        extra_instruction += f"\n{category_instructions.get(category, '')}"
    
    system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë§Œì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì§ˆë¬¸ ë³µì¡ë„: {complexity}
ì²˜ë¦¬ ë°©ì‹: {mode}

ì¤‘ìš”: ë²•ê·œê°€ ë³€ê²½ëœ ê²½ìš° ë°˜ë“œì‹œ ìµœì‹  ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. 
íŠ¹íˆ ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ì€ 2023ë…„ë¶€í„° 100ì–µì› ì´ìƒìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥) - ìµœì‹  ë²•ê·œ ê¸°ì¤€
2. ìƒì„¸ ì„¤ëª… (ê·¼ê±° ì¡°í•­ í¬í•¨)
3. ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì˜ˆì™¸ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
4. ë²•ê·œ ë³€ê²½ì‚¬í•­ (ì¤‘ìš”í•œ ë³€ê²½ì´ ìˆì—ˆë˜ ê²½ìš°)

{extra_instruction}"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
{critical_updates}

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

{"ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ" if complexity == 'simple' else "ìƒì„¸í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ"} ë‹µë³€í•´ì£¼ì„¸ìš”.
êµ¬ë²„ì „ ì •ë³´ì™€ ìµœì‹  ì •ë³´ê°€ ìƒì¶©í•˜ëŠ” ê²½ìš°, ë°˜ë“œì‹œ ìµœì‹  ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""}
    ]
    
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=temperature,
        max_tokens=1500
    )
    
    return response.choices[0].message.content

# ===== ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© =====
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    try:
        required_files = ["manuals_vector_db.index", "all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            st.info("ğŸ’¡ prepare_pdfs_ftc.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
            return None, None, None, None
        
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)"):
            index = faiss.read_index("manuals_vector_db.index")
            with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
                chunks = json.load(f)
            
            try:
                embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            except Exception as e:
                st.warning("í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            
            try:
                reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
            except:
                reranker_model = None
        
        return embedding_model, reranker_model, index, chunks
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None, None, None, None

# ===== ë©”ì¸ UI =====
def main():
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ GPT-4o í†µí•© Q&A ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)
    
    models = load_models_and_data()
    if not all(models):
        st.stop()
    
    embedding_model, reranker_model, index, chunks = models
    
    rag = HybridRAGPipeline(embedding_model, reranker_model, index, chunks)
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    chat_container = st.container()
    
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "user":
                    st.write(message["content"])
                else:
                    if isinstance(message["content"], dict):
                        st.write(message["content"]["answer"])
                        
                        complexity = message["content"].get("complexity", "unknown")
                        complexity_html = f'<span class="complexity-indicator complexity-{complexity}">{complexity.upper()}</span>'
                        st.markdown(f"ì²˜ë¦¬ ë³µì¡ë„: {complexity_html}", unsafe_allow_html=True)
                        
                        if "total_time" in message["content"]:
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ğŸ” ê²€ìƒ‰", f"{message['content']['search_time']:.1f}ì´ˆ")
                            with col2:
                                st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{message['content']['generation_time']:.1f}ì´ˆ")
                            with col3:
                                st.metric("â±ï¸ ì „ì²´", f"{message['content']['total_time']:.1f}ì´ˆ")
                    else:
                        st.write(message["content"])
        
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?)"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.write(prompt)
            
            with st.chat_message("assistant"):
                total_start_time = time.time()
                
                search_start_time = time.time()
                with st.spinner("ğŸ” GPT-4oê°€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ì¤‘..."):
                    results, stats = run_async_in_streamlit(rag.process_query(prompt, top_k=5))
                search_time = time.time() - search_start_time
                
                generation_start_time = time.time()
                with st.spinner("ğŸ’­ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer = generate_answer(prompt, results, stats)
                generation_time = time.time() - generation_start_time
                
                total_time = time.time() - total_start_time
                
                st.write(answer)
                
                gpt_analysis = stats.get('gpt_analysis', {})
                complexity = gpt_analysis.get('query_analysis', {}).get('actual_complexity', 'unknown')
                mode = stats.get('processing_mode', 'unknown')
                complexity_html = f'<span class="complexity-indicator complexity-{complexity}">{complexity.upper()}</span>'
                st.markdown(f"ì§ˆë¬¸ ë³µì¡ë„: {complexity_html} | ì²˜ë¦¬ ë°©ì‹: **{mode}**", unsafe_allow_html=True)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ” ê²€ìƒ‰", f"{search_time:.1f}ì´ˆ")
                with col2:
                    st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{generation_time:.1f}ì´ˆ")
                with col3:
                    st.metric("â±ï¸ ì „ì²´", f"{total_time:.1f}ì´ˆ")
                
                with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                    if stats.get('has_version_conflicts'):
                        st.error("âš ï¸ **ì¤‘ìš”: ë²•ê·œ ë³€ê²½ì‚¬í•­ ë°œê²¬**")
                        for warning in stats.get('outdated_warnings', []):
                            if warning['severity'] == 'critical':
                                st.warning(f"""
                                ğŸ“Œ **{warning['regulation']}** ë³€ê²½
                                - ì´ì „: {warning['found']}
                                - í˜„ì¬: **{warning['current']}** âœ…
                                - ë³€ê²½ì¼: {warning['changed_date']}
                                """)
                        st.info("ğŸ’¡ ë³¸ ì‹œìŠ¤í…œì€ ìµœì‹  ë²•ê·œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                    
                    if gpt_analysis:
                        st.subheader("ğŸ¤– GPT-4o ì§ˆë¬¸ ë¶„ì„")
                        st.json({
                            "í•µì‹¬ ì˜ë„": gpt_analysis.get('query_analysis', {}).get('core_intent', ''),
                            "ì‹¤ì œ ë³µì¡ë„": gpt_analysis.get('query_analysis', {}).get('actual_complexity', ''),
                            "ê²€ìƒ‰ ì „ëµ": gpt_analysis.get('search_strategy', {}).get('approach', ''),
                            "ì£¼ìš” ë§¤ë‰´ì–¼": gpt_analysis.get('search_strategy', {}).get('primary_manual', '')
                        })
                    
                    mode_descriptions = {
                        'gpt_guided_direct': "GPT-4oê°€ ë‹¨ìˆœí•œ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ì§ì ‘ ê²€ìƒ‰ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                        'gpt_guided_focused': "GPT-4oê°€ íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ì§‘ì¤‘ ê²€ìƒ‰ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                        'gpt_guided_comprehensive': "GPT-4oê°€ ì—¬ëŸ¬ ì£¼ì œì— ê±¸ì¹œ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                        'fallback_traditional': "GPT-4o ë¶„ì„ì´ ì‹¤íŒ¨í•˜ì—¬ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤."
                    }
                    st.info(f"ğŸ¯ **ì²˜ë¦¬ ë°©ì‹**: {mode_descriptions.get(mode, 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                    
                    if stats.get('searched_chunks'):
                        st.info(f"ğŸ” {stats['searched_chunks']}ê°œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
                    
                    st.subheader("ğŸ“š ì°¸ê³  ìë£Œ")
                    for i, result in enumerate(results[:3]):
                        version_indicator = ""
                        if result.metadata.get('has_outdated_info'):
                            version_indicator = " âš ï¸ **[êµ¬ë²„ì „ ì •ë³´ í¬í•¨]**"
                        
                        st.caption(f"**{result.source}** - í˜ì´ì§€ {result.page} (ê´€ë ¨ë„: {result.score:.2f}){version_indicator}")
                        
                        if result.document_date:
                            st.caption(f"ğŸ“… ë¬¸ì„œ ë‚ ì§œ: {result.document_date}")
                        
                        with st.container():
                            content = result.content[:300] + "..." if len(result.content) > 300 else result.content
                            
                            if '50ì–µì›' in content or '30ì–µì›' in content:
                                content = re.sub(r'(50ì–µì›|30ì–µì›)', r'~~\1~~ â†’ **100ì–µì›**', content)
                            
                            st.text(content)
                    
                    if total_time < 5:
                        st.success("âš¡ ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ ì†ë„!")
                    elif total_time < 10:
                        st.info("âœ… ì ì ˆí•œ ì‘ë‹µ ì†ë„")
                    else:
                        st.warning("â° ì‘ë‹µ ì‹œê°„ì´ ë‹¤ì†Œ ê¸¸ì—ˆìŠµë‹ˆë‹¤ (ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¸í•œ ì •ìƒì ì¸ ì²˜ë¦¬)")
                
                response_data = {
                    "answer": answer,
                    "search_time": search_time,
                    "generation_time": generation_time,
                    "total_time": total_time,
                    "complexity": complexity,
                    "processing_mode": mode
                }
                st.session_state.messages.append({"role": "assistant", "content": response_data})
    
    st.divider()
    st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ìƒì„±í•œ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬í•­ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    
    with st.sidebar:
        st.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        
        st.subheader("ğŸŸ¢ ë‹¨ìˆœ ì§ˆë¬¸")
        if st.button("ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œì€ ë©°ì¹ ì¸ê°€ìš”?"
            st.rerun()
        if st.button("ì´ì‚¬íšŒ ì˜ê²° ê¸ˆì•¡ ê¸°ì¤€ì€?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ì—ì„œ ì´ì‚¬íšŒ ì˜ê²°ì´ í•„ìš”í•œ ê±°ë˜ ê¸ˆì•¡ì€?"
            st.rerun()
            
        st.subheader("ğŸŸ¡ ì¤‘ê°„ ë³µì¡ë„")
        if st.button("ê³„ì—´ì‚¬ ê±°ë˜ ì‹œ ì£¼ì˜ì‚¬í•­ì€?"):
            st.session_state.new_question = "ê³„ì—´ì‚¬ì™€ ìê¸ˆê±°ë˜ë¥¼ í•  ë•Œ ì–´ë–¤ ì ˆì°¨ë¥¼ ê±°ì³ì•¼ í•˜ê³  ì£¼ì˜í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            st.rerun()
        if st.button("ë¹„ìƒì¥ì‚¬ ì£¼ì‹ ì–‘ë„ ì ˆì°¨ëŠ”?"):
            st.session_state.new_question = "ë¹„ìƒì¥íšŒì‚¬ê°€ ì£¼ì‹ì„ ì–‘ë„í•  ë•Œ í•„ìš”í•œ ì ˆì°¨ì™€ ê³µì‹œ ì˜ë¬´ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
            st.rerun()
            
        st.subheader("ğŸ”´ ë³µì¡í•œ ì§ˆë¬¸")
        if st.button("ë³µí•© ê±°ë˜ ë¶„ì„"):
            st.session_state.new_question = "AíšŒì‚¬ê°€ Bê³„ì—´ì‚¬ì— ìê¸ˆì„ ëŒ€ì—¬í•˜ë©´ì„œ ë™ì‹œì— Cê³„ì—´ì‚¬ì˜ ì£¼ì‹ì„ ì·¨ë“í•˜ëŠ” ê²½ìš°, ê°ê° ì–´ë–¤ ê·œì œê°€ ì ìš©ë˜ê³  ê³µì‹œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
            st.rerun()
        if st.button("ì¢…í•©ì  ë¦¬ìŠ¤í¬ ê²€í† "):
            st.session_state.new_question = "ìš°ë¦¬ íšŒì‚¬ê°€ ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜ë¥¼ ì§„í–‰í•  ë•Œ ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê·œì œì™€ ê´€ë ¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•´ì•¼ í•  ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ì „ëµì€?"
            st.rerun()
        
        st.divider()
        st.caption("ğŸ’¡ GPT-4oê°€ ëª¨ë“  ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
    
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
