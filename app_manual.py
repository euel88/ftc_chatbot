# íŒŒì¼ ì´ë¦„: app_hybrid_gpt.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì› - GPT í•˜ì´ë¸Œë¦¬ë“œ í†µí•© ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
import re
from collections import defaultdict, Counter
import time
from dataclasses import dataclass
import os
import hashlib
from enum import Enum

# ===== 1. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ =====
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ê¹”ë”í•œ UIë¥¼ ìœ„í•œ CSS (ê¸°ìˆ ì  ì •ë³´ ìˆ¨ê¹€)
st.markdown("""
<style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display: none;}
    
    /* ë©”ì¸ í—¤ë” ìŠ¤íƒ€ì¼ */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        max-width: 900px;
        margin: 0 auto;
    }
    
    /* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ ê°œì„  */
    [data-testid="metric-container"] {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* ë‹µë³€ ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .stChatMessage {
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    
    /* ë³µì¡ë„ í‘œì‹œ ìŠ¤íƒ€ì¼ */
    .complexity-indicator {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 16px;
        font-size: 0.85rem;
        font-weight: 500;
        margin-left: 8px;
    }
    
    .complexity-simple {
        background-color: #d4edda;
        color: #155724;
    }
    
    .complexity-medium {
        background-color: #fff3cd;
        color: #856404;
    }
    
    .complexity-complex {
        background-color: #f8d7da;
        color: #721c24;
    }
</style>
""", unsafe_allow_html=True)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# ===== 2. ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')
    
    @property
    def is_latest(self) -> bool:
        """ìµœì‹  ìë£Œ ì—¬ë¶€ í™•ì¸"""
        return self.metadata.get('is_latest', False)

class QueryComplexity(Enum):
    """ì§ˆë¬¸ ë³µì¡ë„ ë ˆë²¨"""
    SIMPLE = "simple"      # ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸
    MEDIUM = "medium"      # ì¤‘ê°„ ë³µì¡ë„
    COMPLEX = "complex"    # ë³µì¡í•œ ë¶„ì„ í•„ìš”

# ===== 3. ë¬¸ì„œ ë²„ì „ ê´€ë¦¬ ë° ìµœì‹ ì„± ê²€ì¦ ì‹œìŠ¤í…œ (ìƒˆë¡œìš´ ê¸°ëŠ¥) =====
class DocumentVersionManager:
    """ë¬¸ì„œì˜ ë²„ì „ê³¼ ìµœì‹ ì„±ì„ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ì¤‘ìš” ë²•ê·œ ë³€ê²½ì‚¬í•­ ë°ì´í„°ë² ì´ìŠ¤
        self.regulation_changes = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜_ê¸ˆì•¡ê¸°ì¤€': [
                {'date': '2023-01-01', 'old_value': '50ì–µì›', 'new_value': '100ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 100ì–µì› ì´ìƒ'},
                {'date': '2020-01-01', 'old_value': '30ì–µì›', 'new_value': '50ì–µì›',
                 'description': 'ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ ë˜ëŠ” 50ì–µì› ì´ìƒ'}
            ],
            'ê³µì‹œ_ê¸°í•œ': [
                {'date': '2022-07-01', 'old_value': '7ì¼', 'new_value': '5ì¼',
                 'description': 'ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œ ë‹¨ì¶•'}
            ]
        }
        
        # í•µì‹¬ ìˆ˜ì¹˜ íŒ¨í„´ (ì •ê·œí‘œí˜„ì‹)
        self.critical_patterns = {
            'ê¸ˆì•¡': r'(\d+)ì–µ\s*ì›',
            'ë¹„ìœ¨': r'(\d+(?:\.\d+)?)\s*%',
            'ê¸°í•œ': r'(\d+)\s*ì¼',
            'ë‚ ì§œ': r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        }
    
    def extract_document_date(self, chunk: Dict) -> Optional[str]:
        """ë¬¸ì„œì—ì„œ ì‘ì„±/ê°œì • ë‚ ì§œ ì¶”ì¶œ"""
        content = chunk.get('content', '')
        metadata = json.loads(chunk.get('metadata', '{}'))
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚ ì§œ í™•ì¸
        if 'document_date' in metadata:
            return metadata['document_date']
        
        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*ê°œì •',
            r'ì‹œí–‰ì¼\s*:\s*(\d{4})ë…„\s*(\d{1,2})ì›”',
            r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                return self._normalize_date(match.group(0))
        
        return None
    
    def _normalize_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # ê°„ë‹¨í•œ ì •ê·œí™” (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì²˜ë¦¬ í•„ìš”)
        date_str = re.sub(r'[^\d]', '-', date_str)
        parts = date_str.split('-')
        if len(parts) >= 2:
            year = parts[0] if len(parts[0]) == 4 else '20' + parts[0]
            month = parts[1].zfill(2)
            day = parts[2].zfill(2) if len(parts) > 2 else '01'
            return f"{year}-{month}-{day}"
        return None
    
    def check_for_outdated_info(self, content: str, document_date: str = None) -> List[Dict]:
        """êµ¬ë²„ì „ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        warnings = []
        
        # ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ í™•ì¸
        amount_match = re.search(r'(\d+)ì–µ\s*ì›.*ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', content)
        if amount_match:
            amount = int(amount_match.group(1))
            if amount == 50:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '50ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
            elif amount == 30:
                warnings.append({
                    'type': 'outdated_amount',
                    'found': '30ì–µì›',
                    'current': '100ì–µì›',
                    'regulation': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€',
                    'changed_date': '2023-01-01',
                    'severity': 'critical'
                })
        
        return warnings

class ConflictResolver:
    """ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, version_manager: DocumentVersionManager):
        self.version_manager = version_manager
    
    def resolve_conflicts(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ìƒì¶©í•˜ëŠ” ì •ë³´ë¥¼ í•´ê²°í•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        
        # 1. ê° ê²°ê³¼ì˜ ë‚ ì§œì™€ êµ¬ë²„ì „ ì •ë³´ í™•ì¸
        for result in results:
            doc_date = result.document_date
            warnings = self.version_manager.check_for_outdated_info(result.content, doc_date)
            
            # ë©”íƒ€ë°ì´í„°ì— ê²½ê³  ì¶”ê°€
            if warnings:
                result.metadata['warnings'] = warnings
                result.metadata['has_outdated_info'] = True
            else:
                result.metadata['has_outdated_info'] = False
        
        # 2. ì¤‘ìš” ìˆ˜ì¹˜ì— ëŒ€í•œ ì¶©ëŒ ê²€ì‚¬
        critical_info = self._extract_critical_info(results, query)
        if critical_info:
            conflicts = self._find_conflicts(critical_info)
            if conflicts:
                results = self._prioritize_latest_info(results, conflicts)
        
        # 3. ìµœì‹  ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ë¥¼ ìƒìœ„ë¡œ ì¬ì •ë ¬
        results.sort(key=lambda r: (
            not r.metadata.get('has_outdated_info', False),  # êµ¬ë²„ì „ ì •ë³´ê°€ ì—†ëŠ” ê²ƒ ìš°ì„ 
            r.document_date or '1900-01-01',  # ìµœì‹  ë¬¸ì„œ ìš°ì„ 
            r.score  # ì›ë˜ ì ìˆ˜
        ), reverse=True)
        
        return results
    
    def _extract_critical_info(self, results: List[SearchResult], query: str) -> Dict:
        """ê²°ê³¼ì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ"""
        critical_info = defaultdict(list)
        
        for i, result in enumerate(results):
            # ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ
            amounts = re.findall(r'(\d+)ì–µ\s*ì›', result.content)
            for amount in amounts:
                critical_info['amounts'].append({
                    'value': amount + 'ì–µì›',
                    'result_index': i,
                    'context': result.content[:100]
                })
            
            # ë¹„ìœ¨ ì •ë³´ ì¶”ì¶œ
            percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', result.content)
            for pct in percentages:
                critical_info['percentages'].append({
                    'value': pct + '%',
                    'result_index': i,
                    'context': result.content[:100]
                })
        
        return dict(critical_info)
    
    def _find_conflicts(self, critical_info: Dict) -> List[Dict]:
        """ì¤‘ìš” ì •ë³´ ê°„ ì¶©ëŒ ì°¾ê¸°"""
        conflicts = []
        
        # ê¸ˆì•¡ ì¶©ëŒ í™•ì¸ (ì˜ˆ: 50ì–µ vs 100ì–µ)
        if 'amounts' in critical_info:
            amount_values = set()
            for item in critical_info['amounts']:
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in item['context']:
                    amount_values.add(item['value'])
            
            if len(amount_values) > 1 and ('50ì–µì›' in amount_values or '30ì–µì›' in amount_values):
                conflicts.append({
                    'type': 'amount_conflict',
                    'values': list(amount_values),
                    'correct_value': '100ì–µì›'
                })
        
        return conflicts
    
    def _prioritize_latest_info(self, results: List[SearchResult], conflicts: List[Dict]) -> List[SearchResult]:
        """ì¶©ëŒì´ ìˆì„ ë•Œ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ"""
        # êµ¬ë²„ì „ ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ì˜ ì ìˆ˜ë¥¼ ë‚®ì¶¤
        for conflict in conflicts:
            if conflict['type'] == 'amount_conflict':
                for i, result in enumerate(results):
                    if any(old_val in result.content for old_val in ['50ì–µì›', '30ì–µì›']):
                        # êµ¬ë²„ì „ ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ì˜ ì ìˆ˜ë¥¼ 50% ê°ì†Œ
                        results[i].score *= 0.5
                        results[i].metadata['score_reduced'] = True
                        results[i].metadata['reduction_reason'] = 'outdated_amount'
        
        return results

# ===== 3-1. ì§ˆë¬¸ ë³µì¡ë„ í‰ê°€ê¸° (ê¸°ì¡´ ì½”ë“œ) =====
class ComplexityAssessor:
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •"""
    
    def __init__(self):
        # ë³µì¡ë„ íŒë‹¨ ê¸°ì¤€
        self.simple_indicators = [
            # ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸
            r'ì–¸ì œ', r'ë©°ì¹ ', r'ê¸°í•œ', r'ë‚ ì§œ', r'ê¸ˆì•¡', r'%', r'ì–¼ë§ˆ',
            r'ì •ì˜[ê°€ëŠ”]?', r'ë¬´ì—‡', r'ëœ»[ì´ì€]?', r'ì˜ë¯¸[ê°€ëŠ”]?'
        ]
        
        self.complex_indicators = [
            # ë³µì¡í•œ ë¶„ì„ í•„ìš”
            r'ë™ì‹œì—', r'ì—¬ëŸ¬', r'ë³µí•©', r'ì—°ê´€', r'ì˜í–¥',
            r'ë§Œ[ì•½ì¼].*ê²½ìš°', r'[AB].*ë™ì‹œ.*[CD]', r'ê±°ë˜.*ì—¬ëŸ¬',
            r'ì „ì²´ì ', r'ì¢…í•©ì ', r'ë¶„ì„', r'ê²€í† ', r'í‰ê°€',
            r'ë¦¬ìŠ¤í¬', r'ìœ„í—˜', r'ëŒ€ì‘', r'ì „ëµ'
        ]
        
        self.medium_indicators = [
            # ì¤‘ê°„ ë³µì¡ë„
            r'ì–´ë–»ê²Œ', r'ë°©ë²•', r'ì ˆì°¨', r'ê³¼ì •',
            r'ì£¼ì˜', r'ì˜ˆì™¸', r'íŠ¹ë³„', r'ê³ ë ¤'
        ]
        
    def assess(self, query: str) -> Tuple[QueryComplexity, float, Dict]:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ê³  ê´€ë ¨ ì •ë³´ ë°˜í™˜"""
        query_lower = query.lower()
        
        # ì ìˆ˜ ê³„ì‚°
        simple_score = sum(1 for pattern in self.simple_indicators 
                         if re.search(pattern, query_lower))
        complex_score = sum(2 for pattern in self.complex_indicators 
                          if re.search(pattern, query_lower))
        medium_score = sum(1.5 for pattern in self.medium_indicators 
                         if re.search(pattern, query_lower))
        
        # ì¶”ê°€ ë³µì¡ë„ ìš”ì¸
        # 1. ì§ˆë¬¸ ê¸¸ì´
        if len(query) > 100:
            complex_score += 1
        elif len(query) < 30:
            simple_score += 0.5
            
        # 2. íŠ¹ìˆ˜ íŒ¨í„´
        if re.search(r'[AB]íšŒì‚¬.*[CD]íšŒì‚¬', query_lower):
            complex_score += 2  # ì—¬ëŸ¬ íšŒì‚¬ ê´€ë ¨
        if '?' in query and query.count('?') > 1:
            complex_score += 1  # ì—¬ëŸ¬ ì§ˆë¬¸
            
        # ìµœì¢… ë³µì¡ë„ ê²°ì •
        total_score = simple_score + medium_score + complex_score
        
        if total_score == 0:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.5
        elif complex_score > simple_score * 2:
            complexity = QueryComplexity.COMPLEX
            confidence = min(complex_score / (total_score + 1), 0.9)
        elif simple_score > complex_score * 2:
            complexity = QueryComplexity.SIMPLE
            confidence = min(simple_score / (total_score + 1), 0.9)
        else:
            complexity = QueryComplexity.MEDIUM
            confidence = 0.6
            
        # ë¶„ì„ ì •ë³´
        analysis = {
            'simple_score': simple_score,
            'medium_score': medium_score,
            'complex_score': complex_score,
            'query_length': len(query),
            'estimated_cost_multiplier': self._estimate_cost_multiplier(complexity)
        }
        
        return complexity, confidence, analysis
    
    def _estimate_cost_multiplier(self, complexity: QueryComplexity) -> float:
        """ë³µì¡ë„ì— ë”°ë¥¸ ì˜ˆìƒ ë¹„ìš© ë°°ìˆ˜"""
        multipliers = {
            QueryComplexity.SIMPLE: 1.0,
            QueryComplexity.MEDIUM: 3.0,
            QueryComplexity.COMPLEX: 10.0
        }
        return multipliers[complexity]

# ===== 4. ì§ˆë¬¸ ë¶„ë¥˜ê¸° (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) =====
class QuestionClassifier:
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë§¤ë‰´ì–¼ì„ ìš°ì„  ê²€ìƒ‰í• ì§€ ê²°ì •"""
    
    def __init__(self):
        # ê° ì¹´í…Œê³ ë¦¬ë³„ í•µì‹¬ í‚¤ì›Œë“œì™€ íŒ¨í„´
        self.categories = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': {
                'keywords': ['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 'ë‚´ë¶€ê±°ë˜', 'ì´ì‚¬íšŒ ì˜ê²°', 'ì´ì‚¬íšŒ', 'ì˜ê²°', 
                           'ê³„ì—´ì‚¬', 'ê³„ì—´íšŒì‚¬', 'íŠ¹ìˆ˜ê´€ê³„ì¸', 'ìê¸ˆ', 'ëŒ€ì—¬', 'ì°¨ì…', 'ë³´ì¦',
                           'ìê¸ˆê±°ë˜', 'ìœ ê°€ì¦ê¶Œ', 'ìì‚°ê±°ë˜', '50ì–µ', 'ê±°ë˜ê¸ˆì•¡'],
                'patterns': [r'ì´ì‚¬íšŒ.*ì˜ê²°', r'ê³„ì—´.*ê±°ë˜', r'ë‚´ë¶€.*ê±°ë˜'],
                'manual_pattern': 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜.*ë§¤ë‰´ì–¼',
                'priority': 1
            },
            'í˜„í™©ê³µì‹œ': {
                'keywords': ['í˜„í™©ê³µì‹œ', 'ê¸°ì—…ì§‘ë‹¨', 'ì†Œì†íšŒì‚¬', 'ë™ì¼ì¸', 'ì¹œì¡±', 
                           'ì§€ë¶„ìœ¨', 'ì„ì›', 'ìˆœí™˜ì¶œì', 'ìƒí˜¸ì¶œì', 'ì§€ë°°êµ¬ì¡°',
                           'ê³„ì—´í¸ì…', 'ê³„ì—´ì œì™¸', 'ì£¼ì£¼í˜„í™©', 'ì„ì›í˜„í™©'],
                'patterns': [r'ê¸°ì—…ì§‘ë‹¨.*í˜„í™©', r'ì†Œì†.*íšŒì‚¬', r'ì§€ë¶„.*ë³€ë™'],
                'manual_pattern': 'ê¸°ì—…ì§‘ë‹¨í˜„í™©ê³µì‹œ.*ë§¤ë‰´ì–¼',
                'priority': 2
            },
            'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': {
                'keywords': ['ë¹„ìƒì¥', 'ì¤‘ìš”ì‚¬í•­', 'ì£¼ì‹', 'ì–‘ë„', 'ì–‘ìˆ˜', 'í•©ë³‘', 
                           'ë¶„í• ', 'ì˜ì—…ì–‘ë„', 'ì„ì›ë³€ê²½', 'ì¦ì', 'ê°ì',
                           'ì •ê´€ë³€ê²½', 'í•´ì‚°', 'ì²­ì‚°'],
                'patterns': [r'ë¹„ìƒì¥.*ê³µì‹œ', r'ì£¼ì‹.*ì–‘ë„', r'ì¤‘ìš”.*ì‚¬í•­'],
                'manual_pattern': 'ë¹„ìƒì¥ì‚¬.*ì¤‘ìš”ì‚¬í•­.*ë§¤ë‰´ì–¼',
                'priority': 3
            }
        }
    
    def classify(self, question: str) -> Tuple[str, float]:
        """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ì‹ ë¢°ë„ë¥¼ ë°˜í™˜"""
        question_lower = question.lower()
        scores = {}
        
        for category, info in self.categories.items():
            score = 0
            matched_keywords = []
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ì ìš©)
            for i, keyword in enumerate(info['keywords']):
                if keyword in question_lower:
                    # ì•ìª½ í‚¤ì›Œë“œì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
                    weight = 1.0 if i < 5 else 0.7
                    score += weight
                    matched_keywords.append(keyword)
            
            # íŒ¨í„´ ë§¤ì¹­ (ì •ê·œí‘œí˜„ì‹)
            for pattern in info.get('patterns', []):
                if re.search(pattern, question_lower):
                    score += 1.5
            
            scores[category] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ì„ íƒ
        if scores:
            best_category = max(scores, key=scores.get)
            max_possible_score = len(self.categories[best_category]['keywords']) + \
                               len(self.categories[best_category].get('patterns', [])) * 1.5
            confidence = min(scores[best_category] / max_possible_score, 1.0)
            
            # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ None ë°˜í™˜
            if confidence < 0.15:
                return None, 0.0
                
            return best_category, confidence
        
        return None, 0.0

# ===== 5. GPT í†µí•© ê²€ìƒ‰ í´ë˜ìŠ¤ (ìƒˆë¡œìš´ ê¸°ëŠ¥) =====
class GPTIntegratedSearch:
    """GPTê°€ ê²€ìƒ‰ê³¼ ë¶„ì„ì„ ëª¨ë‘ ë‹´ë‹¹í•˜ëŠ” í†µí•© ê²€ìƒ‰"""
    
    def __init__(self, chunks: List[Dict]):
        self.chunks = chunks
        self.max_chunks_per_call = 50  # GPT í† í° ì œí•œ ê³ ë ¤
        
    def search_and_analyze(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """GPTê°€ ê²€ìƒ‰ê³¼ ë¶„ì„ì„ í†µí•©ì ìœ¼ë¡œ ìˆ˜í–‰"""
        start_time = time.time()
        
        # 1ë‹¨ê³„: GPTê°€ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
        search_strategy = self._develop_search_strategy(query)
        
        # 2ë‹¨ê³„: ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ GPT í‰ê°€
        all_evaluations = []
        for i in range(0, len(self.chunks), self.max_chunks_per_call):
            batch = self.chunks[i:i + self.max_chunks_per_call]
            evaluations = self._evaluate_chunks_batch(query, batch, search_strategy)
            all_evaluations.extend(evaluations)
        
        # 3ë‹¨ê³„: ìƒìœ„ ê²°ê³¼ ì„ íƒ ë° ì¬ì •ë ¬
        all_evaluations.sort(key=lambda x: x['relevance_score'], reverse=True)
        top_results = all_evaluations[:top_k * 2]  # ì—¬ìœ ìˆê²Œ ì„ íƒ
        
        # 4ë‹¨ê³„: GPTê°€ ìµœì¢… ìˆœìœ„ ê²°ì •
        final_results = self._finalize_ranking(query, top_results, top_k)
        
        # í†µê³„ ìƒì„±
        stats = {
            'method': 'gpt_integrated',
            'search_time': time.time() - start_time,
            'chunks_evaluated': len(self.chunks),
            'strategy': search_strategy,
            'estimated_cost': self._estimate_cost(len(self.chunks))
        }
        
        return final_results, stats
    
    def _develop_search_strategy(self, query: str) -> Dict:
        """GPTê°€ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½"""
        prompt = f"""
        ë‹¤ìŒ ë²•ë¥  ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”:
        
        ì§ˆë¬¸: {query}
        
        ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "key_concepts": ["í•µì‹¬ ê°œë…1", "í•µì‹¬ ê°œë…2"],
            "related_concepts": ["ê´€ë ¨ ê°œë…1", "ê´€ë ¨ ê°œë…2"],
            "legal_areas": ["ê´€ë ¨ ë²•ë¥  ì˜ì—­1", "ê´€ë ¨ ë²•ë¥  ì˜ì—­2"],
            "search_focus": "ê²€ìƒ‰ ì´ˆì  ì„¤ëª…"
        }}
        """
        
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            strategy = json.loads(response.choices[0].message.content)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ["key_concepts", "related_concepts", "legal_areas", "search_focus"]
            for field in required_fields:
                if field not in strategy:
                    strategy[field] = [] if field != "search_focus" else "ì¼ë°˜ ê²€ìƒ‰"
            
            return strategy
            
        except Exception as e:
            print(f"Error in _develop_search_strategy: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì „ëµ ë°˜í™˜
            return {
                "key_concepts": [query],
                "related_concepts": [],
                "legal_areas": ["ê³µì •ê±°ë˜ë²•"],
                "search_focus": "ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰"
            }
    
    def _evaluate_chunks_batch(self, query: str, chunks: List[Dict], strategy: Dict) -> List[Dict]:
        """GPTê°€ ì²­í¬ ë°°ì¹˜ì˜ ê´€ë ¨ì„±ì„ í‰ê°€"""
        # ì²­í¬ ìš”ì•½ ìƒì„±
        chunks_summary = "\n".join([
            f"[ì²­í¬ {i}] ({chunk['source']}, p.{chunk['page']}): {chunk['content'][:150]}..."
            for i, chunk in enumerate(chunks)
        ])
        
        prompt = f"""
        ì§ˆë¬¸: {query}
        ê²€ìƒ‰ ì „ëµ: {json.dumps(strategy, ensure_ascii=False)}
        
        ë‹¤ìŒ ë¬¸ì„œ ì²­í¬ë“¤ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì„¸ìš”.
        ê° ì²­í¬ì— ëŒ€í•´ 0-10ì ì˜ ê´€ë ¨ì„± ì ìˆ˜ì™€ ì´ìœ ë¥¼ ì œê³µí•˜ì„¸ìš”.
        
        {chunks_summary}
        
        ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ JSON ê°ì²´ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "evaluations": [
                {{"chunk_index": 0, "relevance_score": 8.5, "reason": "ê´€ë ¨ì„± ì´ìœ "}},
                {{"chunk_index": 1, "relevance_score": 6.0, "reason": "ê´€ë ¨ì„± ì´ìœ "}}
            ]
        }}
        """
        
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=2000,
                response_format={"type": "json_object"}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_content = response.choices[0].message.content
            evaluations = json.loads(response_content)
            
            # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
            if isinstance(evaluations, dict) and 'evaluations' in evaluations:
                eval_list = evaluations['evaluations']
            elif isinstance(evaluations, list):
                eval_list = evaluations
            else:
                # ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì¸ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                print(f"Unexpected GPT response format: {evaluations}")
                return []
            
            # ì›ë³¸ ì²­í¬ ì •ë³´ì™€ ë³‘í•©
            results = []
            for eval_item in eval_list:
                # íƒ€ì… ê²€ì¦
                if not isinstance(eval_item, dict):
                    continue
                    
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if 'chunk_index' not in eval_item:
                    continue
                    
                idx = eval_item.get('chunk_index', -1)
                if isinstance(idx, int) and 0 <= idx < len(chunks):
                    chunk = chunks[idx]
                    results.append({
                        'chunk': chunk,
                        'relevance_score': float(eval_item.get('relevance_score', 0)),
                        'reason': eval_item.get('reason', '')
                    })
            
            return results
            
        except Exception as e:
            print(f"Error in _evaluate_chunks_batch: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì ìˆ˜ë¡œ ëª¨ë“  ì²­í¬ ë°˜í™˜
            return [{
                'chunk': chunk,
                'relevance_score': 5.0,
                'reason': 'GPT í‰ê°€ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬'
            } for chunk in chunks]
    
    def _finalize_ranking(self, query: str, candidates: List[Dict], top_k: int) -> List[SearchResult]:
        """GPTê°€ ìµœì¢… ìˆœìœ„ë¥¼ ê²°ì •"""
        # í›„ë³´ ìš”ì•½
        candidates_summary = "\n".join([
            f"[í›„ë³´ {i}] (ì ìˆ˜: {c['relevance_score']:.1f}) {c['chunk']['source']}: {c['chunk']['content'][:100]}..."
            for i, c in enumerate(candidates[:10])
        ])
        
        prompt = f"""
        ì§ˆë¬¸: {query}
        
        ë‹¤ìŒ í›„ë³´ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ {top_k}ê°œë¥¼ ì„ íƒí•˜ê³  ìˆœìœ„ë¥¼ ë§¤ê¸°ì„¸ìš”.
        ë²•ì  ì •í™•ì„±ê³¼ ì‹¤ë¬´ì  ìœ ìš©ì„±ì„ ëª¨ë‘ ê³ ë ¤í•˜ì„¸ìš”.
        
        {candidates_summary}
        
        ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "selected_indices": [0, 3, 1, 2, 4],
            "explanation": "ì„ íƒ ì´ìœ  ì„¤ëª…"
        }}
        
        selected_indicesëŠ” ì„ íƒí•œ í›„ë³´ì˜ ì¸ë±ìŠ¤ë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•œ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                response_format={"type": "json_object"}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result = json.loads(response.choices[0].message.content)
            
            # ì¸ë±ìŠ¤ ì¶”ì¶œ
            if isinstance(result, dict) and 'selected_indices' in result:
                indices = result['selected_indices']
            else:
                # í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ ì‹œë„
                content = str(result)
                indices = re.findall(r'\d+', content)[:top_k]
            
            # SearchResult ê°ì²´ ìƒì„±
            results = []
            for idx in indices[:top_k]:
                try:
                    idx_int = int(idx)
                    if 0 <= idx_int < len(candidates):
                        candidate = candidates[idx_int]
                        chunk = candidate['chunk']
                        results.append(SearchResult(
                            chunk_id=chunk.get('chunk_id', str(idx_int)),
                            content=chunk['content'],
                            score=candidate['relevance_score'],
                            source=chunk['source'],
                            page=chunk['page'],
                            chunk_type=chunk.get('chunk_type', 'unknown'),
                            metadata=json.loads(chunk.get('metadata', '{}'))
                        ))
                except (ValueError, IndexError):
                    continue
            
            # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ìƒìœ„ í›„ë³´ë¡œ ì±„ìš°ê¸°
            if len(results) < top_k:
                for candidate in candidates:
                    if len(results) >= top_k:
                        break
                    # ì´ë¯¸ ì¶”ê°€ëœ ì²­í¬ì¸ì§€ í™•ì¸
                    chunk_id = candidate['chunk'].get('chunk_id', '')
                    if not any(r.chunk_id == chunk_id for r in results):
                        chunk = candidate['chunk']
                        results.append(SearchResult(
                            chunk_id=chunk_id or str(len(results)),
                            content=chunk['content'],
                            score=candidate['relevance_score'],
                            source=chunk['source'],
                            page=chunk['page'],
                            chunk_type=chunk.get('chunk_type', 'unknown'),
                            metadata=json.loads(chunk.get('metadata', '{}'))
                        ))
            
            return results
            
        except Exception as e:
            print(f"Error in _finalize_ranking: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì ìˆ˜ ìˆœìœ¼ë¡œ ìƒìœ„ kê°œ ë°˜í™˜
            results = []
            for i, candidate in enumerate(candidates[:top_k]):
                chunk = candidate['chunk']
                results.append(SearchResult(
                    chunk_id=chunk.get('chunk_id', str(i)),
                    content=chunk['content'],
                    score=candidate.get('relevance_score', 5.0),
                    source=chunk['source'],
                    page=chunk['page'],
                    chunk_type=chunk.get('chunk_type', 'unknown'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                ))
            return results
    
    def _estimate_cost(self, num_chunks: int) -> float:
        """ì˜ˆìƒ ë¹„ìš© ê³„ì‚° (ë‹¬ëŸ¬)"""
        # GPT-4o ê°€ê²© ê¸°ì¤€ (ëŒ€ëµì )
        tokens_per_chunk = 200
        total_tokens = num_chunks * tokens_per_chunk
        price_per_1k_tokens = 0.01
        return (total_tokens / 1000) * price_per_1k_tokens

# ===== 6. í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ (í•µì‹¬ í†µí•©) =====
class HybridRAGPipeline:
    """ë³µì¡ë„ì— ë”°ë¼ ì „í†µì  ë°©ì‹ê³¼ GPT í†µí•© ë°©ì‹ì„ ì„ íƒí•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, embedding_model, reranker_model, index, chunks):
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunks = chunks
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.classifier = QuestionClassifier()
        self.complexity_assessor = ComplexityAssessor()
        self.gpt_search = GPTIntegratedSearch(chunks)
        
        # ë²„ì „ ê´€ë¦¬ ë° ì¶©ëŒ í•´ê²° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.version_manager = DocumentVersionManager()
        self.conflict_resolver = ConflictResolver(self.version_manager)
        
        # ë§¤ë‰´ì–¼ë³„ ì²­í¬ ì¸ë±ìŠ¤ ë¯¸ë¦¬ êµ¬ì¶•
        self.manual_indices = self._build_manual_indices()
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
        self.search_cache = {}
        self.cache_max_size = 100
        
        # ê° ì²­í¬ì˜ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
        self._extract_chunk_dates()
        
    def _extract_chunk_dates(self):
        """ëª¨ë“  ì²­í¬ì˜ ë‚ ì§œ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ"""
        for chunk in self.chunks:
            doc_date = self.version_manager.extract_document_date(chunk)
            if doc_date:
                metadata = json.loads(chunk.get('metadata', '{}'))
                metadata['document_date'] = doc_date
                chunk['metadata'] = json.dumps(metadata)
    
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx, chunk in enumerate(self.chunks):
            source = chunk.get('source', '').lower()
            
            if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜'].append(idx)
            elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                indices['í˜„í™©ê³µì‹œ'].append(idx)
            elif 'ë¹„ìƒì¥' in source:
                indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­'].append(idx)
            else:
                indices['ê¸°íƒ€'].append(idx)
        
        return dict(indices)
    
    def process_query(self, query: str, top_k: int = 5) -> Tuple[List[SearchResult], Dict]:
        """ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ìµœì ì˜ ì²˜ë¦¬ ë°©ì‹ì„ ì„ íƒ"""
        # 1. ë³µì¡ë„ í‰ê°€
        complexity, confidence, complexity_analysis = self.complexity_assessor.assess(query)
        
        # 2. ë³µì¡ë„ì— ë”°ë¥¸ ì²˜ë¦¬
        if complexity == QueryComplexity.SIMPLE:
            # ë‹¨ìˆœ ì§ˆë¬¸: ë¹ ë¥¸ ì „í†µì  ê²€ìƒ‰
            results, stats = self._fast_traditional_search(query, top_k)
            stats['processing_mode'] = 'fast_traditional'
            
        elif complexity == QueryComplexity.COMPLEX:
            # ë³µì¡í•œ ì§ˆë¬¸: GPT í†µí•© ì²˜ë¦¬
            results, stats = self.gpt_search.search_and_analyze(query, top_k)
            stats['processing_mode'] = 'gpt_integrated'
            
        else:  # MEDIUM
            # ì¤‘ê°„ ë³µì¡ë„: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
            # ë¨¼ì € ë¹ ë¥¸ ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ë¥¼ ì°¾ê³ , GPTë¡œ ì •ì œ
            initial_results, initial_stats = self._fast_traditional_search(query, top_k * 3)
            results, stats = self._gpt_enhance_results(query, initial_results, top_k)
            stats['processing_mode'] = 'hybrid'
            stats['initial_search_time'] = initial_stats['search_time']
        
        # 3. ìµœì‹ ì„± ê²€ì¦ ë° ì¶©ëŒ í•´ê²°
        results = self.conflict_resolver.resolve_conflicts(results, query)
        
        # 4. êµ¬ë²„ì „ ì •ë³´ ê²½ê³  ìˆ˜ì§‘
        outdated_warnings = []
        for result in results:
            if result.metadata.get('has_outdated_info'):
                outdated_warnings.extend(result.metadata.get('warnings', []))
        
        # 5. ë³µì¡ë„ ì •ë³´ ì¶”ê°€
        stats['complexity'] = complexity.value
        stats['complexity_confidence'] = confidence
        stats['complexity_analysis'] = complexity_analysis
        stats['outdated_warnings'] = outdated_warnings
        stats['has_version_conflicts'] = len(outdated_warnings) > 0
        
        return results, stats
    
    def _fast_traditional_search(self, query: str, top_k: int) -> Tuple[List[SearchResult], Dict]:
        """ê¸°ì¡´ì˜ ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰ ë°©ì‹"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(f"{query}_{top_k}_traditional".encode()).hexdigest()
        if cache_key in self.search_cache:
            cached = self.search_cache[cache_key]
            stats = cached['stats'].copy()
            stats['cache_hit'] = True
            return cached['results'], stats
        
        # ì§ˆë¬¸ ë¶„ë¥˜
        category, cat_confidence = self.classifier.classify(query)
        
        # ê²€ìƒ‰ ì¸ë±ìŠ¤ ê²°ì •
        if category and cat_confidence > 0.3:
            primary_indices = self.manual_indices.get(category, [])
            secondary_indices = []
            for cat, indices in self.manual_indices.items():
                if cat != category and cat != 'ê¸°íƒ€':
                    secondary_indices.extend(indices)
        else:
            primary_indices = list(range(len(self.chunks)))
            secondary_indices = []
        
        # ë²¡í„° ê²€ìƒ‰
        query_vector = self.embedding_model.encode([query])
        query_vector = np.array(query_vector, dtype=np.float32)
        
        k_search = min(len(self.chunks), top_k * 10)
        scores, indices = self.index.search(query_vector, k_search)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        seen_chunks = set()
        
        # ìš°ì„ ìˆœìœ„ ì¸ë±ìŠ¤ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
        if primary_indices:
            primary_set = set(primary_indices)
            for idx, score in zip(indices[0], scores[0]):
                if idx in primary_set and idx not in seen_chunks:
                    seen_chunks.add(idx)
                    chunk = self.chunks[idx]
                    result = SearchResult(
                        chunk_id=chunk.get('chunk_id', str(idx)),
                        content=chunk['content'],
                        score=float(score),
                        source=chunk['source'],
                        page=chunk['page'],
                        chunk_type=chunk.get('chunk_type', 'unknown'),
                        metadata=json.loads(chunk.get('metadata', '{}'))
                    )
                    results.append(result)
                    if len(results) >= top_k:
                        break
        
        # í†µê³„
        stats = {
            'search_time': time.time() - start_time,
            'category': category,
            'category_confidence': cat_confidence,
            'cache_hit': False
        }
        
        # ìºì‹œ ì €ì¥
        if stats['search_time'] < 1.0:
            self.search_cache[cache_key] = {
                'results': results,
                'stats': stats
            }
        
        return results, stats
    
    def _gpt_enhance_results(self, query: str, initial_results: List[SearchResult], 
                           top_k: int) -> Tuple[List[SearchResult], Dict]:
        """GPTë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì •ì œí•˜ê³  ê°œì„ """
        start_time = time.time()
        
        # GPTì—ê²Œ ì¬ì •ë ¬ê³¼ ë¶„ì„ ìš”ì²­
        results_summary = "\n".join([
            f"[ê²°ê³¼ {i+1}] (ì ìˆ˜: {r.score:.2f}) {r.source} p.{r.page}:\n{r.content[:200]}..."
            for i, r in enumerate(initial_results[:10])
        ])
        
        prompt = f"""
        ì‚¬ìš©ì ì§ˆë¬¸: {query}
        
        ë‹¤ìŒì€ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ì¤‘ì—ì„œ ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ {top_k}ê°œë¥¼ ì„ íƒí•˜ê³ ,
        ê°ê°ì´ ì™œ ì¤‘ìš”í•œì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        
        {results_summary}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        1. ì„ íƒí•œ ê²°ê³¼ ë²ˆí˜¸ë“¤: [1, 3, 2, ...]
        2. ê° ê²°ê³¼ê°€ ì¤‘ìš”í•œ ì´ìœ 
        """
        
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1000
        )
        
        # ì‘ë‹µ íŒŒì‹±
        content = response.choices[0].message.content
        selected_indices = []
        
        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'\[([^\]]+)\]', content)
        if numbers:
            indices_str = numbers[0]
            selected_indices = [int(x.strip()) - 1 for x in indices_str.split(',') 
                              if x.strip().isdigit()]
        
        # ì„ íƒëœ ê²°ê³¼ ë°˜í™˜
        enhanced_results = []
        for idx in selected_indices[:top_k]:
            if 0 <= idx < len(initial_results):
                enhanced_results.append(initial_results[idx])
        
        # ë¶€ì¡±í•˜ë©´ ì›ë˜ ê²°ê³¼ë¡œ ì±„ìš°ê¸°
        if len(enhanced_results) < top_k:
            for result in initial_results:
                if result not in enhanced_results:
                    enhanced_results.append(result)
                    if len(enhanced_results) >= top_k:
                        break
        
        stats = {
            'enhancement_time': time.time() - start_time,
            'enhanced_count': len(selected_indices)
        }
        
        return enhanced_results, stats

# ===== 7. ë™ì  Temperature ë‹µë³€ ìƒì„± (ê¸°ì¡´ ì½”ë“œ ìœ ì§€ + ê°œì„ ) =====
def determine_temperature(query: str, complexity: QueryComplexity) -> float:
    """ì§ˆë¬¸ ìœ í˜•ê³¼ ë³µì¡ë„ì— ë”°ë¼ ìµœì ì˜ temperature ê²°ì •"""
    query_lower = query.lower()
    
    # ë³µì¡ë„ë³„ ê¸°ë³¸ê°’
    base_temps = {
        QueryComplexity.SIMPLE: 0.1,
        QueryComplexity.MEDIUM: 0.3,
        QueryComplexity.COMPLEX: 0.5
    }
    
    temp = base_temps[complexity]
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ì¡°ì •
    if any(keyword in query_lower for keyword in ['ì–¸ì œ', 'ë©°ì¹ ', 'ê¸°í•œ', 'ë‚ ì§œ', 'ê¸ˆì•¡', '%']):
        temp = min(temp, 0.1)
    elif any(keyword in query_lower for keyword in ['ì „ëµ', 'ëŒ€ì‘', 'ë¦¬ìŠ¤í¬', 'ì£¼ì˜', 'ê¶Œì¥']):
        temp = max(temp, 0.7)
    
    return temp

def generate_answer(query: str, results: List[SearchResult], stats: Dict) -> str:
    """GPT-4oë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„± (ìµœì‹  ì •ë³´ ìš°ì„ )"""
    
    # êµ¬ë²„ì „ ì •ë³´ ê²½ê³  í™•ì¸
    has_outdated = stats.get('has_version_conflicts', False)
    outdated_warnings = stats.get('outdated_warnings', [])
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ìµœì‹  ì •ë³´ ìš°ì„ )
    context_parts = []
    latest_info_parts = []
    outdated_info_parts = []
    
    for i, result in enumerate(results[:5]):
        context_str = f"""
[ì°¸ê³  {i+1}] {result.source} (í˜ì´ì§€ {result.page})
{result.content}
"""
        if result.metadata.get('has_outdated_info'):
            outdated_info_parts.append(context_str)
        else:
            latest_info_parts.append(context_str)
    
    # ìµœì‹  ì •ë³´ë¥¼ ë¨¼ì €, êµ¬ë²„ì „ ì •ë³´ëŠ” ë‚˜ì¤‘ì—
    context_parts = latest_info_parts + outdated_info_parts
    context = "\n---\n".join(context_parts)
    
    # ì¤‘ìš” ë²•ê·œ ë³€ê²½ì‚¬í•­ ëª…ì‹œ
    critical_updates = ""
    if has_outdated:
        critical_updates = "\n\n[ì¤‘ìš” ë²•ê·œ ë³€ê²½ì‚¬í•­]"
        for warning in outdated_warnings:
            if warning['severity'] == 'critical':
                critical_updates += f"\n- {warning['regulation']}: {warning['found']} â†’ {warning['current']} (ë³€ê²½ì¼: {warning['changed_date']})"
    
    # ë³µì¡ë„ ì •ë³´ í™œìš©
    complexity = QueryComplexity(stats.get('complexity', 'medium'))
    temperature = determine_temperature(query, complexity)
    
    # ì²˜ë¦¬ ëª¨ë“œë³„ íŠ¹ë³„ ì§€ì‹œ
    mode_instructions = {
        'gpt_integrated': "GPTê°€ ì‹¬ì¸µ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        'hybrid': "ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ GPTê°€ ì •ì œí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
        'fast_traditional': "ì œê³µëœ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    }
    
    mode = stats.get('processing_mode', 'fast_traditional')
    extra_instruction = mode_instructions.get(mode, "")
    
    # ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ì§€ì‹œì‚¬í•­
    category = stats.get('category')
    if category:
        category_instructions = {
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': "íŠ¹íˆ ì´ì‚¬íšŒ ì˜ê²° ìš”ê±´, ê³µì‹œ ê¸°í•œ, ë©´ì œ ì¡°ê±´ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”. ê¸ˆì•¡ ê¸°ì¤€ì€ ë°˜ë“œì‹œ ìµœì‹  ê¸°ì¤€(100ì–µì› ì´ìƒ ë˜ëŠ” ìë³¸ê¸ˆ ë° ìë³¸ì´ê³„ ì¤‘ í° ê¸ˆì•¡ì˜ 5% ì´ìƒ)ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
            'í˜„í™©ê³µì‹œ': "ê³µì‹œ ì£¼ì²´, ì‹œê¸°, ì œì¶œ ì„œë¥˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.",
            'ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­': "ê³µì‹œ ëŒ€ìƒ ê±°ë˜, ê¸°í•œ, ì œì¶œ ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”."
        }
        extra_instruction += f"\n{category_instructions.get(category, '')}"
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œë§Œì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì§ˆë¬¸ ë³µì¡ë„: {complexity.value}
ì²˜ë¦¬ ë°©ì‹: {mode}

ì¤‘ìš”: ë²•ê·œê°€ ë³€ê²½ëœ ê²½ìš° ë°˜ë“œì‹œ ìµœì‹  ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. 
íŠ¹íˆ ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ì€ 2023ë…„ë¶€í„° 100ì–µì› ì´ìƒìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥) - ìµœì‹  ë²•ê·œ ê¸°ì¤€
2. ìƒì„¸ ì„¤ëª… (ê·¼ê±° ì¡°í•­ í¬í•¨)
3. ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì˜ˆì™¸ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
4. ë²•ê·œ ë³€ê²½ì‚¬í•­ (ì¤‘ìš”í•œ ë³€ê²½ì´ ìˆì—ˆë˜ ê²½ìš°)

{extra_instruction}"""
    
    # GPT-4o í˜¸ì¶œ
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
{critical_updates}

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

{"ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ" if complexity == QueryComplexity.SIMPLE else "ìƒì„¸í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ"} ë‹µë³€í•´ì£¼ì„¸ìš”.
êµ¬ë²„ì „ ì •ë³´ì™€ ìµœì‹  ì •ë³´ê°€ ìƒì¶©í•˜ëŠ” ê²½ìš°, ë°˜ë“œì‹œ ìµœì‹  ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""}
    ]
    
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=temperature,
        max_tokens=1500
    )
    
    return response.choices[0].message.content

# ===== 8. ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© =====
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    try:
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["manuals_vector_db.index", "all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            st.info("ğŸ’¡ prepare_pdfs_ftc.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
            return None, None, None, None
        
        # ë°ì´í„° ë¡œë“œ
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)"):
            # ë²¡í„° ì¸ë±ìŠ¤ì™€ ì²­í¬ ë°ì´í„° ë¡œë“œ
            index = faiss.read_index("manuals_vector_db.index")
            with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
                chunks = json.load(f)
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            try:
                embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            except Exception as e:
                st.warning("í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            
            # ì¬ì •ë ¬ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            try:
                reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
            except:
                reranker_model = None
        
        return embedding_model, reranker_model, index, chunks
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None, None, None, None

# ===== 9. ë©”ì¸ UI (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ë°˜ì˜) =====
def main():
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ ì§€ëŠ¥í˜• í•˜ì´ë¸Œë¦¬ë“œ Q&A ì‹œìŠ¤í…œ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë¡œë“œ
    models = load_models_and_data()
    if not all(models):
        st.stop()
    
    embedding_model, reranker_model, index, chunks = models
    
    # í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = HybridRAGPipeline(embedding_model, reranker_model, index, chunks)
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ì»¨í…Œì´ë„ˆ
    chat_container = st.container()
    
    with chat_container:
        # ì´ì „ ëŒ€í™” í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "user":
                    st.write(message["content"])
                else:
                    # AI ì‘ë‹µ í‘œì‹œ
                    if isinstance(message["content"], dict):
                        st.write(message["content"]["answer"])
                        
                        # ë³µì¡ë„ í‘œì‹œ
                        complexity = message["content"].get("complexity", "unknown")
                        complexity_html = f'<span class="complexity-indicator complexity-{complexity}">{complexity.upper()}</span>'
                        st.markdown(f"ì²˜ë¦¬ ë³µì¡ë„: {complexity_html}", unsafe_allow_html=True)
                        
                        # ì‹œê°„ ì •ë³´ í‘œì‹œ
                        if "total_time" in message["content"]:
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ğŸ” ê²€ìƒ‰", f"{message['content']['search_time']:.1f}ì´ˆ")
                            with col2:
                                st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{message['content']['generation_time']:.1f}ì´ˆ")
                            with col3:
                                st.metric("â±ï¸ ì „ì²´", f"{message['content']['total_time']:.1f}ì´ˆ")
                    else:
                        st.write(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.write(prompt)
            
            # AI ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                # ì „ì²´ ì‹œê°„ ì¸¡ì • ì‹œì‘
                total_start_time = time.time()
                
                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
                search_start_time = time.time()
                with st.spinner("ğŸ” ìµœì ì˜ ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒí•˜ì—¬ ìë£Œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                    results, stats = rag.process_query(prompt, top_k=5)
                search_time = time.time() - search_start_time
                
                # ë‹µë³€ ìƒì„±
                generation_start_time = time.time()
                with st.spinner("ğŸ’­ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    answer = generate_answer(prompt, results, stats)
                generation_time = time.time() - generation_start_time
                
                # ì „ì²´ ì‹œê°„ ê³„ì‚°
                total_time = time.time() - total_start_time
                
                # ë‹µë³€ í‘œì‹œ
                st.write(answer)
                
                # ë³µì¡ë„ í‘œì‹œ
                complexity = stats.get('complexity', 'unknown')
                mode = stats.get('processing_mode', 'unknown')
                complexity_html = f'<span class="complexity-indicator complexity-{complexity}">{complexity.upper()}</span>'
                st.markdown(f"ì§ˆë¬¸ ë³µì¡ë„: {complexity_html} | ì²˜ë¦¬ ë°©ì‹: **{mode}**", unsafe_allow_html=True)
                
                # ì‹œê°„ ì •ë³´ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ” ê²€ìƒ‰", f"{search_time:.1f}ì´ˆ")
                with col2:
                    st.metric("âœï¸ ë‹µë³€ ìƒì„±", f"{generation_time:.1f}ì´ˆ")
                with col3:
                    st.metric("â±ï¸ ì „ì²´", f"{total_time:.1f}ì´ˆ")
                
                # ì„±ëŠ¥ ë¶„ì„ (ì ‘ì„ ìˆ˜ ìˆê²Œ)
                with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                    # êµ¬ë²„ì „ ì •ë³´ ê²½ê³  í‘œì‹œ
                    if stats.get('has_version_conflicts'):
                        st.error("âš ï¸ **ì¤‘ìš”: ë²•ê·œ ë³€ê²½ì‚¬í•­ ë°œê²¬**")
                        for warning in stats.get('outdated_warnings', []):
                            if warning['severity'] == 'critical':
                                st.warning(f"""
                                ğŸ“Œ **{warning['regulation']}** ë³€ê²½
                                - ì´ì „: {warning['found']}
                                - í˜„ì¬: **{warning['current']}** âœ…
                                - ë³€ê²½ì¼: {warning['changed_date']}
                                """)
                        st.info("ğŸ’¡ ë³¸ ì‹œìŠ¤í…œì€ ìµœì‹  ë²•ê·œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                    
                    # ì²˜ë¦¬ ë°©ì‹ ì„¤ëª…
                    mode_descriptions = {
                        'fast_traditional': "ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ì‹ ì†í•˜ê²Œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.",
                        'hybrid': "ì´ˆê¸° ê²€ìƒ‰ í›„ GPTë¡œ ê²°ê³¼ë¥¼ ì •ì œí•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤.",
                        'gpt_integrated': "GPTê°€ ì „ì²´ ê³¼ì •ì„ ë‹´ë‹¹í•˜ì—¬ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤."
                    }
                    st.info(f"ğŸ¯ **ì²˜ë¦¬ ë°©ì‹**: {mode_descriptions.get(mode, 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                    
                    # ë³µì¡ë„ ë¶„ì„
                    if 'complexity_analysis' in stats:
                        analysis = stats['complexity_analysis']
                        st.subheader("ğŸ“Š ë³µì¡ë„ ë¶„ì„")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ë‹¨ìˆœ ì ìˆ˜", f"{analysis['simple_score']:.1f}")
                        with col2:
                            st.metric("ì¤‘ê°„ ì ìˆ˜", f"{analysis['medium_score']:.1f}")
                        with col3:
                            st.metric("ë³µì¡ ì ìˆ˜", f"{analysis['complex_score']:.1f}")
                        
                        if mode == 'gpt_integrated':
                            st.warning(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ì¼ë°˜ ê²€ìƒ‰ì˜ ì•½ {analysis['estimated_cost_multiplier']:.1f}ë°°")
                    
                    # ê²€ìƒ‰ í†µê³„
                    if stats.get('category'):
                        st.info(f"ğŸ“‚ **{stats['category']}** ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ (ì‹ ë¢°ë„: {stats.get('category_confidence', 0):.0%})")
                    
                    # ì°¸ê³  ìë£Œ
                    st.subheader("ğŸ“š ì°¸ê³  ìë£Œ")
                    for i, result in enumerate(results[:3]):
                        # êµ¬ë²„ì „ ì •ë³´ í‘œì‹œ
                        version_indicator = ""
                        if result.metadata.get('has_outdated_info'):
                            version_indicator = " âš ï¸ **[êµ¬ë²„ì „ ì •ë³´ í¬í•¨]**"
                        
                        st.caption(f"**{result.source}** - í˜ì´ì§€ {result.page} (ê´€ë ¨ë„: {result.score:.2f}){version_indicator}")
                        
                        # ë¬¸ì„œ ë‚ ì§œ í‘œì‹œ
                        if result.document_date:
                            st.caption(f"ğŸ“… ë¬¸ì„œ ë‚ ì§œ: {result.document_date}")
                        
                        with st.container():
                            # ë‚´ìš© í‘œì‹œ (êµ¬ë²„ì „ ì •ë³´ëŠ” ì·¨ì†Œì„  ì²˜ë¦¬)
                            content = result.content[:300] + "..." if len(result.content) > 300 else result.content
                            
                            # 50ì–µì›ì´ë‚˜ 30ì–µì›ì´ í¬í•¨ëœ ê²½ìš° í•˜ì´ë¼ì´íŠ¸
                            if '50ì–µì›' in content or '30ì–µì›' in content:
                                content = re.sub(r'(50ì–µì›|30ì–µì›)', r'~~\1~~ â†’ **100ì–µì›**', content)
                            
                            st.text(content)
                    
                    # ì„±ëŠ¥ í‰ê°€
                    if total_time < 3:
                        st.success("âš¡ ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ ì†ë„!")
                    elif total_time < 5:
                        st.info("âœ… ì ì ˆí•œ ì‘ë‹µ ì†ë„")
                    else:
                        st.warning("â° ì‘ë‹µ ì‹œê°„ì´ ë‹¤ì†Œ ê¸¸ì—ˆìŠµë‹ˆë‹¤ (ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¸í•œ ì •ìƒì ì¸ ì²˜ë¦¬)")
                
                # ì„¸ì…˜ì— ì €ì¥
                response_data = {
                    "answer": answer,
                    "search_time": search_time,
                    "generation_time": generation_time,
                    "total_time": total_time,
                    "complexity": complexity,
                    "processing_mode": mode
                }
                st.session_state.messages.append({"role": "assistant", "content": response_data})
    
    # í•˜ë‹¨ ì•ˆë‚´
    st.divider()
    st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ìƒì„±í•œ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬í•­ì€ ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°” (ì˜ˆì‹œ ì§ˆë¬¸ - ë³µì¡ë„ë³„ë¡œ êµ¬ì„±)
    with st.sidebar:
        st.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        
        st.subheader("ğŸŸ¢ ë‹¨ìˆœ ì§ˆë¬¸ (ë¹ ë¥¸ ê²€ìƒ‰)")
        if st.button("ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œì€ ë©°ì¹ ì¸ê°€ìš”?"
            st.rerun()
        if st.button("ì´ì‚¬íšŒ ì˜ê²° ê¸ˆì•¡ ê¸°ì¤€ì€?"):
            st.session_state.new_question = "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ì—ì„œ ì´ì‚¬íšŒ ì˜ê²°ì´ í•„ìš”í•œ ê±°ë˜ ê¸ˆì•¡ì€?"
            st.rerun()
            
        st.subheader("ğŸŸ¡ ì¤‘ê°„ ë³µì¡ë„ (í•˜ì´ë¸Œë¦¬ë“œ)")
        if st.button("ê³„ì—´ì‚¬ ê±°ë˜ ì‹œ ì£¼ì˜ì‚¬í•­ì€?"):
            st.session_state.new_question = "ê³„ì—´ì‚¬ì™€ ìê¸ˆê±°ë˜ë¥¼ í•  ë•Œ ì–´ë–¤ ì ˆì°¨ë¥¼ ê±°ì³ì•¼ í•˜ê³  ì£¼ì˜í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            st.rerun()
        if st.button("ë¹„ìƒì¥ì‚¬ ì£¼ì‹ ì–‘ë„ ì ˆì°¨ëŠ”?"):
            st.session_state.new_question = "ë¹„ìƒì¥íšŒì‚¬ê°€ ì£¼ì‹ì„ ì–‘ë„í•  ë•Œ í•„ìš”í•œ ì ˆì°¨ì™€ ê³µì‹œ ì˜ë¬´ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
            st.rerun()
            
        st.subheader("ğŸ”´ ë³µì¡í•œ ì§ˆë¬¸ (GPT í†µí•©)")
        if st.button("ë³µí•© ê±°ë˜ ë¶„ì„"):
            st.session_state.new_question = "AíšŒì‚¬ê°€ Bê³„ì—´ì‚¬ì— ìê¸ˆì„ ëŒ€ì—¬í•˜ë©´ì„œ ë™ì‹œì— Cê³„ì—´ì‚¬ì˜ ì£¼ì‹ì„ ì·¨ë“í•˜ëŠ” ê²½ìš°, ê°ê° ì–´ë–¤ ê·œì œê°€ ì ìš©ë˜ê³  ê³µì‹œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
            st.rerun()
        if st.button("ì¢…í•©ì  ë¦¬ìŠ¤í¬ ê²€í† "):
            st.session_state.new_question = "ìš°ë¦¬ íšŒì‚¬ê°€ ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜ë¥¼ ì§„í–‰í•  ë•Œ ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê·œì œì™€ ê´€ë ¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•´ì•¼ í•  ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ì „ëµì€?"
            st.rerun()
        
        st.divider()
        st.caption("ğŸ’¡ ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ë§Œ, ì²˜ë¦¬ ì‹œê°„ê³¼ ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤.")
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
