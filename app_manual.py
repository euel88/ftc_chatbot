# íŒŒì¼ ì´ë¦„: app_manual.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì›)

# ===== í•„ìˆ˜ import ë¬¸ë“¤ì„ ë§¨ ìœ„ë¡œ ì´ë™ =====
import streamlit as st
import numpy as np  # SimpleVectorSearch í´ë˜ìŠ¤ë³´ë‹¤ ë¨¼ì € import í•„ìš”
from typing import List, Dict, Tuple, Optional, Set, TypedDict, Protocol, Iterator, Generator, OrderedDict, Any, Union
import json
import openai
import re
from collections import defaultdict, Counter, OrderedDict
import time
from dataclasses import dataclass, field
import os
import hashlib
from enum import Enum
import asyncio
import logging
from contextlib import contextmanager
import traceback
import gc
import concurrent.futures
from functools import lru_cache, wraps
import pickle
from pathlib import Path

# ì„ íƒì  import - ì—†ì–´ë„ ì•±ì´ ì‹¤í–‰ë˜ë„ë¡ ì²˜ë¦¬
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logging.warning("FAISS module not available - using alternative search method")

try:
    from cryptography.fernet import Fernet
    CRYPTOGRAPHY_AVAILABLE = True
except ImportError:
    CRYPTOGRAPHY_AVAILABLE = False
    logging.warning("cryptography module not available - encryption features disabled")

try:
    import ijson
    IJSON_AVAILABLE = True
except ImportError:
    IJSON_AVAILABLE = False
    logging.warning("ijson module not available - using standard JSON loading")

try:
    import nest_asyncio
    nest_asyncio.apply()
    NEST_ASYNCIO_AVAILABLE = True
except ImportError:
    NEST_ASYNCIO_AVAILABLE = False
    logging.warning("nest_asyncio not available - async features may be limited")

try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("sentence-transformers not available - will use OpenAI embeddings")

# ===== ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ (FAISS ëŒ€ì²´) =====
class SimpleVectorSearch:
    """FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰
    
    ì´ í´ë˜ìŠ¤ëŠ” NumPyë§Œì„ ì‚¬ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ì˜
    ë²¡í„° ê²€ìƒ‰ì„ êµ¬í˜„í•©ë‹ˆë‹¤. FAISSë³´ë‹¤ëŠ” ëŠë¦¬ì§€ë§Œ,
    ì–´ë–¤ í™˜ê²½ì—ì„œë„ ì‘ë™í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, embeddings: np.ndarray):
        """
        Args:
            embeddings: ë¬¸ì„œ ì„ë² ë”© ë°°ì—´ (n_docs, embedding_dim)
        """
        self.embeddings = embeddings
        # ì •ê·œí™”ëœ ì„ë² ë”© ì €ì¥ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”)
        self.normalized_embeddings = self._normalize_vectors(embeddings)
        logging.info(f"SimpleVectorSearch initialized with {len(embeddings)} documents")
    
    def _normalize_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """ë²¡í„° ì •ê·œí™” (L2 norm = 1)"""
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        norms = np.where(norms == 0, 1, norms)
        return vectors / norms
    
    def search(self, query_vector: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        
        Args:
            query_vector: ì¿¼ë¦¬ ë²¡í„° (1, embedding_dim)
            k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
        
        Returns:
            scores: ìœ ì‚¬ë„ ì ìˆ˜ ë°°ì—´
            indices: ë¬¸ì„œ ì¸ë±ìŠ¤ ë°°ì—´
        """
        # ì¿¼ë¦¬ ë²¡í„° ì •ê·œí™”
        query_norm = self._normalize_vectors(query_vector.reshape(1, -1))
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ ë²¡í„°ì˜ ë‚´ì )
        similarities = np.dot(self.normalized_embeddings, query_norm.T).squeeze()
        
        # ìƒìœ„ kê°œ ì„ íƒ
        top_k_indices = np.argpartition(similarities, -k)[-k:]
        top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]
        
        # FAISSì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        scores = similarities[top_k_indices].reshape(1, -1)
        indices = top_k_indices.reshape(1, -1)
        
        return scores, indices

# ===== ë¡œê¹… ì„¤ì • =====
def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    logger = logging.getLogger('ftc_chatbot')
    logger.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler('ftc_chatbot_debug.log')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# ===== ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ í´ë˜ìŠ¤ =====
class RAGPipelineError(Exception):
    """RAG íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

class APIKeyError(RAGPipelineError):
    """API í‚¤ ê´€ë ¨ ì˜¤ë¥˜"""
    pass

# ===== íƒ€ì… ì •ì˜ =====
class ModelSelection(Enum):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤"""
    GPT4O = "gpt-4o"
    GPT4O_MINI = "gpt-4o-mini"
    O4_MINI = "o4-mini"  # ì¶”ë¡  íŠ¹í™” ëª¨ë¸

class ProcessStep(Enum):
    """ì²˜ë¦¬ ë‹¨ê³„"""
    INTENT_ANALYSIS = "intent_analysis"
    DOCUMENT_SEARCH = "document_search"
    ANSWER_GENERATION = "answer_generation"

# ===== ë°ì´í„° êµ¬ì¡° ì •ì˜ =====
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    metadata: Dict
    
    @property
    def document_date(self) -> Optional[str]:
        """ë¬¸ì„œì˜ ì‘ì„±/ê°œì • ë‚ ì§œ ë°˜í™˜"""
        return self.metadata.get('document_date') or self.metadata.get('revision_date')

@dataclass
class IntentAnalysis:
    """ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ê²°ê³¼"""
    core_intent: str  # í•µì‹¬ ì˜ë„
    query_type: str  # simple_lookup, complex_analysis, procedural
    target_documents: List[str]  # ê²€ìƒ‰í•´ì•¼ í•  ë¬¸ì„œë“¤
    key_entities: List[str]  # í•µì‹¬ ê°œì²´ë“¤ (íšŒì‚¬, ê±°ë˜ ìœ í˜• ë“±)
    search_keywords: List[str]  # ê²€ìƒ‰ í‚¤ì›Œë“œ
    requires_timeline: bool  # ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”í•œì§€
    requires_calculation: bool  # ê³„ì‚°ì´ í•„ìš”í•œì§€
    confidence: float  # ë¶„ì„ ì‹ ë¢°ë„

# ===== API ê´€ë¦¬ì (ê°„ì†Œí™”) =====
class APIManager:
    """OpenAI API í‚¤ ê´€ë¦¬"""
    
    def __init__(self):
        self._api_key = None
        
    def load_api_key(self) -> str:
        """API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
        # Streamlit secrets í™•ì¸ (ìµœìš°ì„ )
        try:
            if 'OPENAI_API_KEY' in st.secrets:
                self._api_key = st.secrets["OPENAI_API_KEY"]
                logger.info("API key loaded from Streamlit secrets")
                return self._api_key
        except Exception as e:
            logger.debug(f"Streamlit secrets not available: {e}")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        if os.environ.get('OPENAI_API_KEY'):
            self._api_key = os.environ.get('OPENAI_API_KEY')
            logger.info("API key loaded from environment variable")
            return self._api_key
        
        raise APIKeyError("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    def get_embedding(self, text: str) -> np.ndarray:
        """OpenAI embeddings APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            response = openai.embeddings.create(
                input=text,
                model="text-embedding-ada-002"
            )
            return np.array(response.data[0].embedding)
        except Exception as e:
            logger.error(f"Failed to get OpenAI embedding: {e}")
            # ì‹¤íŒ¨ ì‹œ ëœë¤ ë²¡í„° ë°˜í™˜ (ì„ì‹œ)
            return np.random.randn(1536)  # OpenAI ì„ë² ë”© ì°¨ì›

# ===== ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë¡œë” =====
class ChunkLoader:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²­í¬ ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, filepath: str):
        self.filepath = filepath
        self._chunks = None
        self._chunk_cache = OrderedDict()
        self._cache_size = 1000
        self._load_all_chunks()
    
    def _load_all_chunks(self):
        """ì „ì²´ ì²­í¬ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ"""
        logger.info(f"Loading chunks from {self.filepath}")
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                self._chunks = json.load(f)
            logger.info(f"Loaded {len(self._chunks)} chunks into memory")
        except Exception as e:
            logger.error(f"Failed to load chunks: {e}")
            self._chunks = []
    
    def get_chunk(self, idx: int) -> Dict:
        """íŠ¹ì • ì¸ë±ìŠ¤ì˜ ì²­í¬ë¥¼ ê°€ì ¸ì˜´"""
        if self._chunks and 0 <= idx < len(self._chunks):
            return self._chunks[idx]
        else:
            raise IndexError(f"Chunk index {idx} out of range")
    
    def get_total_chunks(self) -> int:
        """ì „ì²´ ì²­í¬ ìˆ˜ ë°˜í™˜"""
        return len(self._chunks) if self._chunks else 0

# ===== 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ êµ¬í˜„ =====

class Step1_IntentAnalyzer:
    """Step 1: ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ íŒŒì•…
    
    ì´ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬:
    - ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…
    - ê²€ìƒ‰í•´ì•¼ í•  ë¬¸ì„œë¥¼ ê²°ì •
    - ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
    """
    
    def __init__(self, api_manager: APIManager):
        self.api_manager = api_manager
        
    async def analyze_intent(self, query: str) -> IntentAnalysis:
        """ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„"""
        
        prompt = f"""
ë‹¹ì‹ ì€ ê³µì •ê±°ë˜ìœ„ì›íšŒ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³ , ì–´ë–¤ ë§¤ë‰´ì–¼ì„ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë§¤ë‰´ì–¼:
1. ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼: ê³„ì—´ì‚¬ ê°„ ìê¸ˆê±°ë˜, ìì‚°ê±°ë˜, ìƒí’ˆìš©ì—­ê±°ë˜ ê´€ë ¨ ê·œì •
2. í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼: ê¸°ì—…ì§‘ë‹¨ í˜„í™©ê³µì‹œ, ê³µì‹œ ì˜ë¬´ì‚¬í•­
3. ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼: ë¹„ìƒì¥íšŒì‚¬ì˜ ì£¼ì‹ ì–‘ë„, í•©ë³‘, ë¶„í•  ë“±

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”:

{{
    "core_intent": "í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„",
    "query_type": "simple_lookup/complex_analysis/procedural ì¤‘ ì„ íƒ",
    "target_documents": ["ê²€ìƒ‰í•  ë§¤ë‰´ì–¼ ì´ë¦„ë“¤"],
    "key_entities": ["ì§ˆë¬¸ì— í¬í•¨ëœ í•µì‹¬ ê°œì²´ë“¤ (ì˜ˆ: ê³„ì—´ì‚¬, ìê¸ˆ, ì´ì‚¬íšŒ ë“±)"],
    "search_keywords": ["ë§¤ë‰´ì–¼ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œ 5-10ê°œ"],
    "requires_timeline": true/false,
    "requires_calculation": true/false,
    "confidence": 0.0-1.0
}}

ë¶„ì„ ì‹œ ê³ ë ¤ì‚¬í•­:
- query_type íŒë‹¨ ê¸°ì¤€:
  - simple_lookup: ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ (ê¸°í•œ, ê¸ˆì•¡ ë“±)
  - complex_analysis: ì—¬ëŸ¬ ì¡°ê±´ì´ ê²°í•©ëœ ë³µì¡í•œ ìƒí™©
  - procedural: ì ˆì°¨ë‚˜ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì§ˆë¬¸
- ì§ˆë¬¸ì— ì—¬ëŸ¬ ë§¤ë‰´ì–¼ì´ ê´€ë ¨ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì‹ ì¤‘íˆ íŒë‹¨
- search_keywordsëŠ” ì‹¤ì œ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ìš©ì–´ë¡œ ì„ ì •
"""
        
        try:
            response = await openai.chat.completions.create(
                model="gpt-4o-mini",  # ì˜ë„ ë¶„ì„ì€ ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            
            analysis_data = json.loads(response.choices[0].message.content)
            
            return IntentAnalysis(
                core_intent=analysis_data.get("core_intent", ""),
                query_type=analysis_data.get("query_type", "simple_lookup"),
                target_documents=analysis_data.get("target_documents", []),
                key_entities=analysis_data.get("key_entities", []),
                search_keywords=analysis_data.get("search_keywords", []),
                requires_timeline=analysis_data.get("requires_timeline", False),
                requires_calculation=analysis_data.get("requires_calculation", False),
                confidence=analysis_data.get("confidence", 0.8)
            )
            
        except Exception as e:
            logger.error(f"Intent analysis failed: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return IntentAnalysis(
                core_intent=query,
                query_type="simple_lookup",
                target_documents=["ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼", "í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼", "ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼"],
                key_entities=[],
                search_keywords=query.split()[:5],
                requires_timeline=False,
                requires_calculation=False,
                confidence=0.5
            )

class Step2_DocumentSearcher:
    """Step 2: ì˜ë„ì— ë§ëŠ” ë§¤ë‰´ì–¼ ê²€ìƒ‰
    
    ì´ ë‹¨ê³„ì—ì„œëŠ” Step 1ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
    - ê´€ë ¨ ë¬¸ì„œë¥¼ ë²¡í„° ê²€ìƒ‰
    - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬ ë° í•„í„°ë§
    - ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ ì„ ë³„
    """
    
    def __init__(self, index, chunk_loader: ChunkLoader, embedding_model, api_manager: APIManager):
        self.index = index
        self.chunk_loader = chunk_loader
        self.embedding_model = embedding_model
        self.api_manager = api_manager
        self.use_faiss = index is not None
        
        # ë§¤ë‰´ì–¼ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.manual_indices = self._build_manual_indices()
        
    def _build_manual_indices(self) -> Dict[str, List[int]]:
        """ê° ë§¤ë‰´ì–¼ë³„ë¡œ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•"""
        indices = defaultdict(list)
        
        for idx in range(self.chunk_loader.get_total_chunks()):
            try:
                chunk = self.chunk_loader.get_chunk(idx)
                source = chunk.get('source', '').lower()
                
                if 'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜' in source:
                    indices['ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ë§¤ë‰´ì–¼'].append(idx)
                elif 'í˜„í™©ê³µì‹œ' in source or 'ê¸°ì—…ì§‘ë‹¨' in source:
                    indices['í˜„í™©ê³µì‹œ ë§¤ë‰´ì–¼'].append(idx)
                elif 'ë¹„ìƒì¥' in source:
                    indices['ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ë§¤ë‰´ì–¼'].append(idx)
                    
            except Exception as e:
                logger.warning(f"Failed to process chunk {idx}: {e}")
                continue
        
        return dict(indices)
    
    async def search_documents(self, intent: IntentAnalysis, top_k: int = 10) -> List[SearchResult]:
        """ì˜ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰"""
        
        # ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ê²°ì •
        search_indices = []
        for doc_name in intent.target_documents:
            if doc_name in self.manual_indices:
                search_indices.extend(self.manual_indices[doc_name])
        
        if not search_indices:
            # ëª¨ë“  ë¬¸ì„œì—ì„œ ê²€ìƒ‰
            search_indices = list(range(self.chunk_loader.get_total_chunks()))
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (í‚¤ì›Œë“œ ê²°í•©)
        search_query = f"{intent.core_intent} {' '.join(intent.search_keywords)}"
        
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        if self.use_faiss:
            results = await self._vector_search(search_query, search_indices, top_k * 2)
        else:
            results = self._keyword_search(search_query, search_indices, top_k * 2)
        
        # ì˜ë„ì— ë§ê²Œ ê²°ê³¼ ì¬ì •ë ¬
        results = self._rerank_by_intent(results, intent)
        
        return results[:top_k]
    
    async def _vector_search(self, query: str, indices: List[int], k: int) -> List[SearchResult]:
        """ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        if self.embedding_model is not None:
            query_vector = self.embedding_model.encode([query])[0]
        else:
            query_vector = self.api_manager.get_embedding(query)
        
        query_vector = query_vector.reshape(1, -1).astype(np.float32)
        
        # FAISS ê²€ìƒ‰
        scores, search_indices = self.index.search(query_vector, k)
        
        # ê²°ê³¼ ë³€í™˜
        results = []
        for idx, score in zip(search_indices[0], scores[0]):
            if idx in indices and 0 <= idx < self.chunk_loader.get_total_chunks():
                chunk = self.chunk_loader.get_chunk(idx)
                result = SearchResult(
                    chunk_id=str(idx),
                    content=chunk['content'],
                    score=float(score),
                    source=chunk.get('source', 'Unknown'),
                    page=chunk.get('page', 0),
                    chunk_type=chunk.get('chunk_type', 'text'),
                    metadata=json.loads(chunk.get('metadata', '{}'))
                )
                results.append(result)
        
        return results
    
    def _keyword_search(self, query: str, indices: List[int], k: int) -> List[SearchResult]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (í´ë°±)"""
        query_words = set(query.lower().split())
        scored_chunks = []
        
        for idx in indices[:1000]:  # ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ
            try:
                chunk = self.chunk_loader.get_chunk(idx)
                content_lower = chunk['content'].lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                score = sum(1 for word in query_words if word in content_lower)
                
                if score > 0:
                    scored_chunks.append((idx, score, chunk))
            except Exception as e:
                continue
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_chunks.sort(key=lambda x: x[1], reverse=True)
        
        # SearchResult ìƒì„±
        results = []
        for idx, score, chunk in scored_chunks[:k]:
            result = SearchResult(
                chunk_id=str(idx),
                content=chunk['content'],
                score=float(score),
                source=chunk.get('source', 'Unknown'),
                page=chunk.get('page', 0),
                chunk_type=chunk.get('chunk_type', 'text'),
                metadata=json.loads(chunk.get('metadata', '{}'))
            )
            results.append(result)
        
        return results
    
    def _rerank_by_intent(self, results: List[SearchResult], intent: IntentAnalysis) -> List[SearchResult]:
        """ì˜ë„ì— ë§ê²Œ ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬"""
        
        for result in results:
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            keyword_matches = sum(1 for kw in intent.search_keywords 
                                if kw.lower() in result.content.lower())
            result.score += keyword_matches * 0.1
            
            # ê°œì²´ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            entity_matches = sum(1 for entity in intent.key_entities 
                               if entity.lower() in result.content.lower())
            result.score += entity_matches * 0.15
            
            # íŠ¹ì • ì¡°ê±´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
            if intent.requires_calculation and re.search(r'\d+ì–µ|\d+%', result.content):
                result.score *= 1.2
            
            if intent.requires_timeline and re.search(r'\d+ì¼|ê¸°í•œ|ë‚ ì§œ', result.content):
                result.score *= 1.2
        
        # ì¬ì •ë ¬
        results.sort(key=lambda x: x.score, reverse=True)
        return results

class Step3_AnswerGenerator:
    """Step 3: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    
    ì´ ë‹¨ê³„ì—ì„œëŠ”:
    - ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ ì˜ë„ë¥¼ ê²°í•©
    - ì ì ˆí•œ ëª¨ë¸ ì„ íƒ (o4-mini, gpt-4o ë“±)
    - ì²´ê³„ì ì´ê³  ì •í™•í•œ ë‹µë³€ ìƒì„±
    """
    
    def __init__(self, api_manager: APIManager):
        self.api_manager = api_manager
        
    async def generate_answer(self, 
                            query: str, 
                            intent: IntentAnalysis,
                            search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        # ëª¨ë¸ ì„ íƒ
        model = self._select_model(intent)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(search_results)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if model == "o4-mini":
            prompt = self._build_reasoning_prompt(query, intent, context)
        else:
            prompt = self._build_standard_prompt(query, intent, context)
        
        # ë‹µë³€ ìƒì„±
        try:
            response = await openai.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=2000
            )
            
            answer = response.choices[0].message.content
            
            # ë‹µë³€ í›„ì²˜ë¦¬
            answer = self._postprocess_answer(answer, intent)
            
            return answer
            
        except Exception as e:
            logger.error(f"Answer generation failed: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _select_model(self, intent: IntentAnalysis) -> str:
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ ì„ íƒ"""
        
        if intent.query_type == "complex_analysis":
            # ë³µì¡í•œ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ì¶”ë¡  ëª¨ë¸ ì‚¬ìš©
            return "o4-mini"
        elif intent.query_type == "simple_lookup":
            # ë‹¨ìˆœ ì¡°íšŒëŠ” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            return "gpt-4o-mini"
        else:
            # ì ˆì°¨ì  ì§ˆë¬¸ì€ ë²”ìš© ëª¨ë¸ ì‚¬ìš©
            return "gpt-4o"
    
    def _build_context(self, search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        for i, result in enumerate(search_results[:5]):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            context_parts.append(f"""
[ì°¸ê³ ìë£Œ {i+1}]
ì¶œì²˜: {result.source} (í˜ì´ì§€ {result.page})
ë‚´ìš©: {result.content}
""")
        
        return "\n---\n".join(context_parts)
    
    def _build_reasoning_prompt(self, query: str, intent: IntentAnalysis, context: str) -> str:
        """o4-miniìš© ì¶”ë¡  ì¤‘ì‹¬ í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì§ˆë¬¸ ì˜ë„ ë¶„ì„]
- í•µì‹¬ ì˜ë„: {intent.core_intent}
- ê´€ë ¨ ê°œì²´: {', '.join(intent.key_entities)}
- ì‹œê°„ìˆœì„œ ì¤‘ìš”: {'ì˜ˆ' if intent.requires_timeline else 'ì•„ë‹ˆì˜¤'}
- ê³„ì‚° í•„ìš”: {'ì˜ˆ' if intent.requires_calculation else 'ì•„ë‹ˆì˜¤'}

[ì°¸ê³ ìë£Œ]
{context}

[ë‹µë³€ ì‘ì„± ì§€ì¹¨]
1. ë¨¼ì € ì§ˆë¬¸ì˜ ê° ìš”ì†Œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
2. ê´€ë ¨ ê·œì •ê³¼ ì¡°ê±´ì„ ë‹¨ê³„ë³„ë¡œ í™•ì¸í•˜ì„¸ìš”
3. í•„ìš”í•œ ê²½ìš° ê³„ì‚° ê³¼ì •ì„ ëª…ì‹œí•˜ì„¸ìš”
4. ìµœì¢… ê²°ë¡ ì„ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”

ë‹µë³€ í˜•ì‹:
## í•µì‹¬ ë‹µë³€
(1-2ë¬¸ì¥ìœ¼ë¡œ ì§ì ‘ì ì¸ ë‹µë³€)

## ìƒì„¸ ë¶„ì„
### 1. ì ìš© ê·œì •
- ê´€ë ¨ ì¡°í•­ê³¼ ê¸°ì¤€

### 2. êµ¬ì²´ì  ê²€í† 
- ë‹¨ê³„ë³„ ë¶„ì„ ë‚´ìš©

### 3. ì£¼ì˜ì‚¬í•­
- ì˜ˆì™¸ì‚¬í•­ì´ë‚˜ íŠ¹ë³„íˆ ìœ ì˜í•  ì 

## ê²°ë¡ 
(ìµœì¢… ì •ë¦¬ ë° ì‹¤ë¬´ ì§€ì¹¨)
"""
    
    def _build_standard_prompt(self, query: str, intent: IntentAnalysis, context: str) -> str:
        """í‘œì¤€ ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì°¸ê³ ìë£Œ]
{context}

[ë‹µë³€ ì§€ì¹¨]
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì œì‹œí•˜ê³  ìƒì„¸ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”
- ê·¼ê±° ì¡°í•­ì„ ëª…í™•íˆ ì¸ìš©í•˜ì„¸ìš”
- ì‹¤ë¬´ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
- ê´€ë ¨ ì£¼ì˜ì‚¬í•­ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”

ë‹µë³€:
"""
    
    def _postprocess_answer(self, answer: str, intent: IntentAnalysis) -> str:
        """ë‹µë³€ í›„ì²˜ë¦¬ - ìµœì‹  ì •ë³´ í™•ì¸ ë“±"""
        
        # ìµœì‹  ê·œì • ì •ë³´ ì¶”ê°€
        if "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜" in ' '.join(intent.target_documents):
            if "50ì–µ" in answer or "30ì–µ" in answer:
                answer += "\n\nâš ï¸ **ì¤‘ìš”**: ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê¸°ì¤€ ê¸ˆì•¡ì€ 2023ë…„ 1ì›” 1ì¼ë¶€í„° 100ì–µì›ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        if "ê³µì‹œ" in answer and "7ì¼" in answer:
            if "ì˜ì—…ì¼" not in answer:
                answer += "\n\nğŸ“Œ **ì°¸ê³ **: ê³µì‹œ ê¸°í•œì€ ì˜ì—…ì¼ ê¸°ì¤€ 7ì¼ì…ë‹ˆë‹¤."
        
        return answer

# ===== í†µí•© RAG íŒŒì´í”„ë¼ì¸ =====
class IntegratedRAGPipeline:
    """3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•©í•œ RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, embedding_model, index, chunk_loader: ChunkLoader, api_manager: APIManager):
        self.step1_analyzer = Step1_IntentAnalyzer(api_manager)
        self.step2_searcher = Step2_DocumentSearcher(index, chunk_loader, embedding_model, api_manager)
        self.step3_generator = Step3_AnswerGenerator(api_manager)
        
    async def process_query(self, query: str) -> Tuple[str, Dict]:
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        stats = {
            "process_times": {},
            "intent_analysis": None,
            "search_results_count": 0
        }
        
        try:
            # Step 1: ì˜ë„ ë¶„ì„
            start_time = time.time()
            intent = await self.step1_analyzer.analyze_intent(query)
            stats["process_times"]["intent_analysis"] = time.time() - start_time
            stats["intent_analysis"] = intent
            
            # Step 2: ë¬¸ì„œ ê²€ìƒ‰
            start_time = time.time()
            search_results = await self.step2_searcher.search_documents(intent)
            stats["process_times"]["document_search"] = time.time() - start_time
            stats["search_results_count"] = len(search_results)
            
            # Step 3: ë‹µë³€ ìƒì„±
            start_time = time.time()
            answer = await self.step3_generator.generate_answer(query, intent, search_results)
            stats["process_times"]["answer_generation"] = time.time() - start_time
            
            stats["total_time"] = sum(stats["process_times"].values())
            
            return answer, stats
            
        except Exception as e:
            logger.error(f"Pipeline error: {e}")
            return "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", stats

# ===== ë¹„ë™ê¸° ì‹¤í–‰ í—¬í¼ =====
def run_async_in_streamlit(coro):
    """Streamlit í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰"""
    try:
        # ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸ ë° ì‹¤í–‰
        try:
            loop = asyncio.get_running_loop()
            if NEST_ASYNCIO_AVAILABLE:
                import nest_asyncio
                nest_asyncio.apply()
            return asyncio.run(coro)
        except RuntimeError:
            return asyncio.run(coro)
    except Exception as e:
        logger.error(f"Async execution failed: {str(e)}")
        raise

# ===== Streamlit UI =====
st.set_page_config(
    page_title="ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    /* ë©”ì¸ í—¤ë” */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #1f4788 0%, #2a5298 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .main-header h1 {
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .main-header p {
        margin: 0.5rem 0 0 0;
        opacity: 0.9;
        font-size: 1.1rem;
    }
    
    /* í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„ í‘œì‹œ */
    .process-step {
        display: inline-block;
        padding: 0.5rem 1rem;
        margin: 0.25rem;
        border-radius: 20px;
        font-size: 0.9rem;
        font-weight: 500;
    }
    
    .step-active {
        background-color: #2a5298;
        color: white;
    }
    
    .step-completed {
        background-color: #28a745;
        color: white;
    }
    
    .step-pending {
        background-color: #e0e0e0;
        color: #666;
    }
    
    /* ì˜ë„ ë¶„ì„ ê²°ê³¼ */
    .intent-box {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    
    /* ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
</style>
""", unsafe_allow_html=True)

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë”©
@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ"""
    try:
        # API ê´€ë¦¬ì ì´ˆê¸°í™”
        api_manager = APIManager()
        api_manager.load_api_key()
        openai.api_key = api_manager._api_key
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ["all_manual_chunks.json"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            st.error(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
            return None
        
        with st.spinner("ğŸ¤– AI ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            index = None
            if os.path.exists("manuals_vector_db.index") and FAISS_AVAILABLE:
                try:
                    import faiss
                    index = faiss.read_index("manuals_vector_db.index")
                    logger.info("FAISS index loaded successfully")
                except Exception as e:
                    logger.warning(f"Failed to load FAISS index: {e}")
            
            # ì²­í¬ ë¡œë” ì´ˆê¸°í™”
            chunk_loader = ChunkLoader("all_manual_chunks.json")
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œë„
            embedding_model = None
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                try:
                    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                    logger.info("Sentence transformer model loaded")
                except Exception as e:
                    logger.warning(f"Failed to load sentence transformer: {e}")
            
            return embedding_model, index, chunk_loader, api_manager
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>âš–ï¸ ê³µì •ê±°ë˜ìœ„ì›íšŒ AI ë²•ë¥  ë³´ì¡°ì›</h1>
        <p>ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜, í˜„í™©ê³µì‹œ, ë¹„ìƒì¥ì‚¬ ì¤‘ìš”ì‚¬í•­ ê´€ë ¨ ì „ë¬¸ ìƒë‹´</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ëª¨ë¸ ë¡œë“œ
    models = load_models_and_data()
    if not models or len(models) != 4:
        st.stop()
    
    embedding_model, index, chunk_loader, api_manager = models
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = IntegratedRAGPipeline(embedding_model, index, chunk_loader, api_manager)
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "user":
                st.write(message["content"])
            else:
                st.write(message["answer"])
                
                # í”„ë¡œì„¸ìŠ¤ ì •ë³´ í‘œì‹œ
                if "stats" in message:
                    stats = message["stats"]
                    if stats.get("intent_analysis"):
                        with st.expander("ğŸ” ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"):
                            intent = stats["intent_analysis"]
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write(f"**í•µì‹¬ ì˜ë„**: {intent.core_intent}")
                                st.write(f"**ì§ˆë¬¸ ìœ í˜•**: {intent.query_type}")
                                st.write(f"**ê²€ìƒ‰ ëŒ€ìƒ**: {', '.join(intent.target_documents)}")
                            with col2:
                                st.write(f"**í•µì‹¬ í‚¤ì›Œë“œ**: {', '.join(intent.search_keywords[:5])}")
                                st.write(f"**ê´€ë ¨ ê°œì²´**: {', '.join(intent.key_entities)}")
                                st.write(f"**ì‹ ë¢°ë„**: {intent.confidence:.1%}")
                    
                    # ì²˜ë¦¬ ì‹œê°„ í‘œì‹œ
                    if "process_times" in stats:
                        times = stats["process_times"]
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ì˜ë„ ë¶„ì„", f"{times.get('intent_analysis', 0):.1f}ì´ˆ")
                        with col2:
                            st.metric("ë¬¸ì„œ ê²€ìƒ‰", f"{times.get('document_search', 0):.1f}ì´ˆ")
                        with col3:
                            st.metric("ë‹µë³€ ìƒì„±", f"{times.get('answer_generation', 0):.1f}ì´ˆ")
    
    # ìƒˆ ì§ˆë¬¸ ì…ë ¥
    if prompt := st.chat_input("ê³µì •ê±°ë˜ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.write(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            # í”„ë¡œì„¸ìŠ¤ ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_container = st.container()
            with progress_container:
                col1, col2, col3 = st.columns(3)
                with col1:
                    step1_placeholder = st.empty()
                    step1_placeholder.markdown('<span class="process-step step-active">1ï¸âƒ£ ì§ˆë¬¸ ë¶„ì„ ì¤‘...</span>', unsafe_allow_html=True)
                with col2:
                    step2_placeholder = st.empty()
                    step2_placeholder.markdown('<span class="process-step step-pending">2ï¸âƒ£ ë¬¸ì„œ ê²€ìƒ‰</span>', unsafe_allow_html=True)
                with col3:
                    step3_placeholder = st.empty()
                    step3_placeholder.markdown('<span class="process-step step-pending">3ï¸âƒ£ ë‹µë³€ ìƒì„±</span>', unsafe_allow_html=True)
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            answer, stats = run_async_in_streamlit(pipeline.process_query(prompt))
            
            # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ í‘œì‹œ
            step1_placeholder.markdown('<span class="process-step step-completed">1ï¸âƒ£ ì§ˆë¬¸ ë¶„ì„ âœ“</span>', unsafe_allow_html=True)
            step2_placeholder.markdown('<span class="process-step step-completed">2ï¸âƒ£ ë¬¸ì„œ ê²€ìƒ‰ âœ“</span>', unsafe_allow_html=True)
            step3_placeholder.markdown('<span class="process-step step-completed">3ï¸âƒ£ ë‹µë³€ ìƒì„± âœ“</span>', unsafe_allow_html=True)
            
            # ë‹µë³€ í‘œì‹œ
            st.write(answer)
            
            # ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append({
                "role": "assistant",
                "answer": answer,
                "stats": stats
            })
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“š ì‚¬ìš© ê°€ì´ë“œ")
        
        st.subheader("ì§ˆë¬¸ ì˜ˆì‹œ")
        
        example_questions = [
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ì´ì‚¬íšŒ ì˜ê²° í›„ ê³µì‹œ ê¸°í•œì€?",
            "ê³„ì—´ì‚¬ ê°„ ìê¸ˆ ëŒ€ì—¬ ì‹œ ì´ì‚¬íšŒ ì˜ê²°ì´ í•„ìš”í•œ ê¸ˆì•¡ì€?",
            "ë¹„ìƒì¥íšŒì‚¬ê°€ ì£¼ì‹ì„ ì–‘ë„í•  ë•Œ í•„ìš”í•œ ì ˆì°¨ëŠ”?",
            "ê¸°ì—…ì§‘ë‹¨ í˜„í™©ê³µì‹œëŠ” ì–¸ì œ í•´ì•¼ í•˜ë‚˜ìš”?",
            "ì—¬ëŸ¬ ê³„ì—´ì‚¬ì™€ ë™ì‹œì— ê±°ë˜í•  ë•Œ ì£¼ì˜ì‚¬í•­ì€?"
        ]
        
        for q in example_questions:
            if st.button(q, key=q):
                st.session_state.new_question = q
                st.rerun()
        
        st.divider()
        
        st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´")
        st.caption("ì´ ì‹œìŠ¤í…œì€ 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‘ë™í•©ë‹ˆë‹¤:")
        st.caption("1ï¸âƒ£ **ì§ˆë¬¸ ì˜ë„ ë¶„ì„**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½")
        st.caption("2ï¸âƒ£ **ë¬¸ì„œ ê²€ìƒ‰**: ê´€ë ¨ ë§¤ë‰´ì–¼ì—ì„œ í•„ìš”í•œ ì •ë³´ ê²€ìƒ‰")
        st.caption("3ï¸âƒ£ **ë‹µë³€ ìƒì„±**: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ìƒì„±")
        
        st.divider()
        
        st.caption("ğŸ’¡ ê° ë‹¨ê³„ì—ì„œ ìµœì í™”ëœ AI ëª¨ë¸ì´ ì‚¬ìš©ë©ë‹ˆë‹¤:")
        st.caption("â€¢ **ì˜ë„ ë¶„ì„**: GPT-4o-mini (ë¹ ë¥¸ ë¶„ì„)")
        st.caption("â€¢ **ë‹µë³€ ìƒì„±**: o4-mini, GPT-4o (ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼)")
    
    # ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
    if "new_question" in st.session_state:
        st.session_state.messages.append({"role": "user", "content": st.session_state.new_question})
        del st.session_state.new_question
        st.rerun()

if __name__ == "__main__":
    main()
