# íŒŒì¼ ì´ë¦„: app_ftc.py (ê³µì •ê±°ë˜ìœ„ì›íšŒ ë¬¸ì„œ íŠ¹í™” ChatGPT ìˆ˜ì¤€ ì •í™•ë„ ë²„ì „)

import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
import json
import openai
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
import re
from collections import defaultdict, Counter
import time
from dataclasses import dataclass
import pandas as pd

# --- 1. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ ---
st.set_page_config(
    page_title="ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›", 
    page_icon="âš–ï¸", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1f4788;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        margin-bottom: 2rem;
    }
    .metric-container {
        background: #f0f5ff;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #1f4788;
    }
    .source-tag {
        background: #e8f0fe;
        padding: 0.2rem 0.5rem;
        border-radius: 4px;
        font-size: 0.9rem;
        color: #1967d2;
    }
    .importance-high {
        color: #d93025;
        font-weight: bold;
    }
    .importance-medium {
        color: #f9ab00;
        font-weight: 600;
    }
</style>
""", unsafe_allow_html=True)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except:
    st.error("âš ï¸ OpenAI API í‚¤ë¥¼ Streamlit Secretsì— ë“±ë¡í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- 2. ë°ì´í„° êµ¬ì¡° ì •ì˜ ---

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    source: str
    page: int
    chunk_type: str
    importance: float
    keywords: List[str]
    metadata: Dict

class FTCQueryAnalyzer:
    """ê³µì •ê±°ë˜ ê´€ë ¨ ì¿¼ë¦¬ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.query_patterns = {
            'definition': re.compile(r'(ì •ì˜|ì˜ë¯¸|ëœ»|ê°œë…|ë¬´ì—‡|ë­|ë€)'),
            'requirement': re.compile(r'(ìš”ê±´|ì¡°ê±´|ê¸°ì¤€|ìê²©)'),
            'deadline': re.compile(r'(ê¸°í•œ|ê¸°ê°„|ì–¸ì œ|ê¹Œì§€|ì´ë‚´|ë§ˆê°)'),
            'procedure': re.compile(r'(ì ˆì°¨|ë°©ë²•|ì–´ë–»ê²Œ|ê³¼ì •|ë‹¨ê³„)'),
            'penalty': re.compile(r'(ë²Œì¹™|ì²˜ë²Œ|ê³¼íƒœë£Œ|ì œì¬|ë²Œê¸ˆ)'),
            'obligation': re.compile(r'(ì˜ë¬´|í•„ìˆ˜|ë°˜ë“œì‹œ|í•´ì•¼|í•˜ì—¬ì•¼)'),
            'exception': re.compile(r'(ì˜ˆì™¸|ì œì™¸|ë©´ì œ|íŠ¹ë¡€)'),
            'calculation': re.compile(r'(ê³„ì‚°|ì‚°ì •|ì‚°ì¶œ|ë¹„ìœ¨|í¼ì„¼íŠ¸)'),
            'reference': re.compile(r'(ì¡°í•­|ì¡°|í•­|í˜¸|ë³„í‘œ|ì„œì‹)')
        }
        
        self.entity_patterns = {
            'article': re.compile(r'ì œ?\s*(\d+)ì¡°(?:ì˜(\d+))?'),
            'percentage': re.compile(r'(\d+(?:\.\d+)?)\s*(%|í¼ì„¼íŠ¸|í”„ë¡œ)'),
            'amount': re.compile(r'(\d+(?:,\d{3})*)\s*(ì›|ë§Œì›|ì–µì›)'),
            'days': re.compile(r'(\d+)\s*ì¼'),
            'company_type': re.compile(r'(ìƒì¥|ë¹„ìƒì¥|ê³„ì—´ì‚¬|ìíšŒì‚¬|ì†ìíšŒì‚¬|íŠ¹ìˆ˜ê´€ê³„ì¸)')
        }
    
    def analyze(self, query: str) -> Dict:
        """ì¿¼ë¦¬ ë¶„ì„ ë° ì˜ë„ íŒŒì•…"""
        analysis = {
            'query_type': self._identify_query_type(query),
            'entities': self._extract_entities(query),
            'key_terms': self._extract_key_terms(query),
            'priority_chunks': self._determine_priority_chunks(query)
        }
        return analysis
    
    def _identify_query_type(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ íƒ€ì… ì‹ë³„"""
        types = []
        for q_type, pattern in self.query_patterns.items():
            if pattern.search(query):
                types.append(q_type)
        return types if types else ['general']
    
    def _extract_entities(self, query: str) -> Dict:
        """ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {}
        for entity_type, pattern in self.entity_patterns.items():
            matches = pattern.findall(query)
            if matches:
                entities[entity_type] = matches
        return entities
    
    def _extract_key_terms(self, query: str) -> List[str]:
        """í•µì‹¬ ìš©ì–´ ì¶”ì¶œ"""
        # ê³µì •ê±°ë˜ ë„ë©”ì¸ íŠ¹í™” ìš©ì–´
        ftc_terms = [
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜', 'ê³µì‹œ', 'ì´ì‚¬íšŒ', 'ì˜ê²°', 'íŠ¹ìˆ˜ê´€ê³„ì¸',
            'ì§€ë¶„ìœ¨', 'ì˜ê²°ê¶Œ', 'ìƒí˜¸ì¶œì', 'ìˆœí™˜ì¶œì', 'ê¸°ì—…ì§‘ë‹¨',
            'ë™ì¼ì¸', 'ì¹œì¡±', 'ì„ì›', 'ì£¼ì£¼', 'ë…ë¦½ê²½ì˜', 'ë¶€ë‹¹ì§€ì›'
        ]
        
        key_terms = []
        query_lower = query.lower()
        
        for term in ftc_terms:
            if term in query_lower:
                key_terms.append(term)
        
        return key_terms
    
    def _determine_priority_chunks(self, query: str) -> List[str]:
        """ìš°ì„ ìˆœìœ„ ì²­í¬ íƒ€ì… ê²°ì •"""
        query_types = self._identify_query_type(query)
        
        priority_map = {
            'definition': ['definition', 'article'],
            'requirement': ['article', 'section'],
            'deadline': ['article', 'penalty', 'section'],
            'procedure': ['section', 'article'],
            'penalty': ['penalty', 'article'],
            'obligation': ['article', 'penalty'],
            'exception': ['article', 'section'],
            'calculation': ['article', 'section'],
            'reference': ['article']
        }
        
        priority_chunks = []
        for q_type in query_types:
            if q_type in priority_map:
                priority_chunks.extend(priority_map[q_type])
        
        return list(set(priority_chunks)) if priority_chunks else ['article', 'section']

# --- 3. ìºì‹±ëœ ë¦¬ì†ŒìŠ¤ ë¡œë”© ---

@st.cache_resource(show_spinner=False)
def load_models_and_data():
    """ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œ"""
    with st.spinner("ğŸ”§ AI ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘..."):
        try:
            # í™˜ê²½ í™•ì¸ (ë¡œì»¬ì¸ì§€ í´ë¼ìš°ë“œì¸ì§€)
            is_cloud = os.environ.get('STREAMLIT_CLOUD', 'false').lower() == 'true'
            
            # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë¨¼ì € í™•ì¸
            if not os.path.exists("manuals_vector_db.index"):
                st.error("âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.info("ğŸ’¡ ë¨¼ì € prepare_pdfs_ftc.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
                return None, None, None, None, None
                
            if not os.path.exists("all_manual_chunks.json"):
                st.error("âŒ ì²­í¬ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None, None, None
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            embedding_model = None
            
            # Streamlit Cloud í™˜ê²½ì—ì„œëŠ” í•­ìƒ ì˜¨ë¼ì¸ ëª¨ë¸ ì‚¬ìš©
            if is_cloud:
                st.info("â˜ï¸ í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì˜¨ë¼ì¸ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
                try:
                    # ì£¼ì˜: ì´ ëª¨ë¸ì€ prepare_pdfs_ftc.pyì—ì„œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•´ì•¼ í•¨
                    embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
                    st.success("âœ… í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                except Exception as e:
                    st.warning(f"âš ï¸ í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    st.info("ğŸ”„ ëŒ€ì²´ ë‹¤êµ­ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
                    try:
                        # ëŒ€ì²´ ëª¨ë¸ (prepare_pdfsì—ì„œë„ ê°™ì€ ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•¨)
                        embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                        st.warning("âš ï¸ ì£¼ì˜: ëŒ€ì²´ ëª¨ë¸ì„ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ê²€ìƒ‰ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"âŒ ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                        return None, None, None, None, None
            
            # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ë¡œì»¬ ëª¨ë¸ ìš°ì„  ì‹œë„
            else:
                # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
                local_model_path = r"C:\Users\OK\Desktop\íŒŒì´ì¬ ì½”ë“œ ëª¨ìŒ\ì±—ë´‡_ê³µì •ìœ„ ê¸°ì—…ì§‘ë‹¨ ê´€ë ¨\models\ko-sroberta-multitask"
                
                # ìƒëŒ€ ê²½ë¡œë¡œë„ ì‹œë„
                if not os.path.exists(local_model_path):
                    script_dir = os.path.dirname(os.path.abspath(__file__))
                    local_model_path = os.path.join(script_dir, "models", "ko-sroberta-multitask")
                
                # ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹œë„
                if os.path.exists(local_model_path):
                    try:
                        st.info("ğŸ’» ë¡œì»¬ í™˜ê²½: ì €ì¥ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
                        embedding_model = SentenceTransformer(local_model_path)
                        st.success("âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                    except Exception as e:
                        st.warning(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                
                # ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì˜¨ë¼ì¸ ëª¨ë¸ ì‹œë„
                if embedding_model is None:
                    try:
                        st.info("ğŸŒ ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
                        # SSL ì˜¤ë¥˜ ë°©ì§€ (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
                        import ssl
                        ssl._create_default_https_context = ssl._create_unverified_context
                        os.environ['HF_HUB_DISABLE_SSL_VERIFY'] = '1'
                        
                        embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
                        st.success("âœ… ì˜¨ë¼ì¸ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                    except Exception as e:
                        st.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                        return None, None, None, None, None
            
            # CrossEncoder ëª¨ë¸ ë¡œë“œ (ì¬ì •ë ¬ìš©)
            try:
                reranker_model = CrossEncoder('Dongjin-kr/ko-reranker')
            except:
                st.warning("âš ï¸ í•œêµ­ì–´ ì¬ì •ë ¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ ì¬ì •ë ¬ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                try:
                    # ëŒ€ì²´ ì¬ì •ë ¬ ëª¨ë¸
                    reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
                except:
                    st.warning("âš ï¸ ì¬ì •ë ¬ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                    reranker_model = None
            
            # ì¸ë±ìŠ¤ì™€ ë°ì´í„° ë¡œë“œ
            index = faiss.read_index("manuals_vector_db.index")
            
            with open("all_manual_chunks.json", "r", encoding="utf-8") as f:
                chunks_data = json.load(f)
            
            # ì²­í¬ íƒ€ì…ë³„ ì¸ë±ìŠ¤ ìƒì„± (ë¹ ë¥¸ í•„í„°ë§ìš©)
            chunk_type_index = defaultdict(list)
            for idx, chunk in enumerate(chunks_data):
                chunk_type_index[chunk.get('chunk_type', 'unknown')].append(idx)
            
            # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
            st.success(f"âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì •ë³´ ë‹¨ìœ„", f"{len(chunks_data):,}ê°œ")
            with col2:
                st.metric("ë²¡í„° ì°¨ì›", f"{index.d}")
            with col3:
                env_type = "â˜ï¸ í´ë¼ìš°ë“œ" if is_cloud else "ğŸ’» ë¡œì»¬"
                st.metric("ì‹¤í–‰ í™˜ê²½", env_type)
            
            return embedding_model, reranker_model, index, chunks_data, chunk_type_index
            
        except Exception as e:
            st.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            st.info("ğŸ’¡ ë¬¸ì œ í•´ê²° ë°©ë²•:")
            st.info("1. prepare_pdfs_ftc.pyë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            st.info("2. ìƒì„±ëœ íŒŒì¼ë“¤ì´ GitHubì— ì œëŒ€ë¡œ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            st.info("3. requirements.txtì— í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            
            # ë””ë²„ê¹… ì •ë³´
            with st.expander("ğŸ” ë””ë²„ê¹… ì •ë³´"):
                st.write("í˜„ì¬ ë””ë ‰í† ë¦¬:", os.getcwd())
                st.write("íŒŒì¼ ëª©ë¡:", os.listdir('.'))
                st.write("í™˜ê²½ ë³€ìˆ˜ STREAMLIT_CLOUD:", os.environ.get('STREAMLIT_CLOUD', 'Not set'))
            
            return None, None, None, None, None

# --- 4. ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ ---

class FTCAdvancedRAG:
    """ê³µì •ê±°ë˜ ë¬¸ì„œ íŠ¹í™” ê³ ê¸‰ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, embedding_model, reranker_model, index, chunks, chunk_type_index):
        self.embedding_model = embedding_model
        self.reranker_model = reranker_model
        self.index = index
        self.chunks = chunks
        self.chunk_type_index = chunk_type_index
        self.query_analyzer = FTCQueryAnalyzer()
        
    def search(self, query: str, top_k: int = 7) -> Tuple[List[SearchResult], Dict]:
        """í†µí•© ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸"""
        start_time = time.time()
        
        # 1. ì¿¼ë¦¬ ë¶„ì„
        query_analysis = self.query_analyzer.analyze(query)
        
        # 2. ì¿¼ë¦¬ í™•ì¥
        expanded_queries = self._expand_query(query, query_analysis)
        
        # 3. ë²¡í„° ê²€ìƒ‰ + í•„í„°ë§
        candidates = self._vector_search_with_filtering(
            expanded_queries, 
            query_analysis['priority_chunks'],
            k=50
        )
        
        # 4. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€
        candidates = self._add_keyword_scores(candidates, query, query_analysis)
        
        # 5. CrossEncoder ì¬ì •ë ¬
        reranked = self._rerank_results(query, candidates, top_k=top_k*2)
        
        # 6. ì»¨í…ìŠ¤íŠ¸ í™•ì¥ (ì¸ì ‘ ì²­í¬ í¬í•¨)
        final_results = self._expand_context(reranked[:top_k])
        
        # í†µê³„ ìƒì„±
        stats = {
            'query_analysis': query_analysis,
            'expanded_queries': expanded_queries,
            'initial_candidates': len(candidates),
            'after_rerank': len(reranked),
            'final_results': len(final_results),
            'search_time': time.time() - start_time
        }
        
        return final_results, stats
    
    def _expand_query(self, original_query: str, analysis: Dict) -> List[str]:
        """ì¿¼ë¦¬ í™•ì¥ (ê³µì •ê±°ë˜ ë„ë©”ì¸ íŠ¹í™”)"""
        queries = [original_query]
        
        # 1. ë™ì˜ì–´/ìœ ì‚¬ì–´ í™•ì¥
        synonym_map = {
            'ê³µì‹œ': ['ê³µì‹œì˜ë¬´', 'ê³µì‹œì‚¬í•­', 'ì‹ ê³ ', 'ë³´ê³ '],
            'ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜': ['ë‚´ë¶€ê±°ë˜', 'ê³„ì—´ì‚¬ê±°ë˜', 'íŠ¹ìˆ˜ê´€ê³„ì¸ê±°ë˜'],
            'ì´ì‚¬íšŒ': ['ì´ì‚¬íšŒ ì˜ê²°', 'ì´ì‚¬íšŒ ê²°ì˜', 'ì´ì‚¬íšŒ ìŠ¹ì¸'],
            'ê³¼íƒœë£Œ': ['ë²Œê¸ˆ', 'ì œì¬ê¸ˆ', 'ë²Œì¹™', 'ì²˜ë²Œ'],
            'íŠ¹ìˆ˜ê´€ê³„ì¸': ['íŠ¹ê´€ì', 'íŠ¹ìˆ˜ê´€ê³„ì', 'ê´€ê³„íšŒì‚¬'],
            'ê¸°í•œ': ['ê¸°ê°„', 'ë§ˆê°ì¼', 'ì œì¶œì¼', 'ì‹ ê³ ì¼']
        }
        
        query_lower = original_query.lower()
        for key, synonyms in synonym_map.items():
            if key in query_lower:
                for syn in synonyms:
                    queries.append(query_lower.replace(key, syn))
        
        # 2. êµ¬ì¡°í™”ëœ ì¿¼ë¦¬ ìƒì„±
        if analysis['entities'].get('article'):
            for article in analysis['entities']['article']:
                queries.append(f"ì œ{article[0]}ì¡° {' '.join(analysis['key_terms'])}")
        
        # 3. ì§ˆë¬¸ ìœ í˜•ë³„ í™•ì¥
        if 'definition' in analysis['query_type']:
            queries.append(f"{' '.join(analysis['key_terms'])} ì •ì˜")
            queries.append(f"{' '.join(analysis['key_terms'])}ë€")
        
        if 'deadline' in analysis['query_type']:
            queries.append(f"{' '.join(analysis['key_terms'])} ê¸°í•œ ì¼ìˆ˜")
            queries.append(f"{' '.join(analysis['key_terms'])} ì œì¶œ ê¸°ê°„")
        
        # ì¤‘ë³µ ì œê±° ë° ë°˜í™˜
        return list(dict.fromkeys(queries))[:6]
    
    def _vector_search_with_filtering(self, queries: List[str], 
                                    priority_chunks: List[str], 
                                    k: int = 50) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰ ë° ì²­í¬ íƒ€ì… í•„í„°ë§"""
        all_results = []
        seen_ids = set()
        
        for query in queries:
            # ë²¡í„° ê²€ìƒ‰
            query_vector = self.embedding_model.encode([query])
            scores, indices = self.index.search(
                np.array(query_vector, dtype=np.float32), 
                min(k, len(self.chunks))
            )
            
            for idx, score in zip(indices[0], scores[0]):
                chunk = self.chunks[idx]
                chunk_id = chunk.get('chunk_id', str(idx))
                
                if chunk_id not in seen_ids:
                    seen_ids.add(chunk_id)
                    
                    # ì²­í¬ íƒ€ì…ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
                    type_weight = 1.2 if chunk.get('chunk_type') in priority_chunks else 1.0
                    
                    # ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
                    importance_weight = chunk.get('importance', 0.5) + 0.5
                    
                    # ìµœì¢… ì ìˆ˜
                    adjusted_score = score * type_weight * importance_weight
                    
                    result = SearchResult(
                        chunk_id=chunk_id,
                        content=chunk['content'],
                        score=adjusted_score,
                        source=chunk['source'],
                        page=chunk['page'],
                        chunk_type=chunk.get('chunk_type', 'unknown'),
                        importance=chunk.get('importance', 0.5),
                        keywords=chunk.get('keywords', []),
                        metadata=json.loads(chunk.get('metadata', '{}'))
                    )
                    all_results.append(result)
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        all_results.sort(key=lambda x: x.score, reverse=True)
        return all_results[:k]
    
    def _add_keyword_scores(self, results: List[SearchResult], 
                          query: str, analysis: Dict) -> List[SearchResult]:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€"""
        query_terms = set(query.lower().split()) | set(analysis['key_terms'])
        
        for result in results:
            content_lower = result.content.lower()
            
            # ì •í™•í•œ ë§¤ì¹­ ì ìˆ˜
            exact_matches = sum(1 for term in query_terms if term in content_lower)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            keyword_matches = sum(1 for kw in result.keywords if kw.lower() in query_terms)
            
            # ì—”í‹°í‹° ë§¤ì¹­ ì ìˆ˜
            entity_score = 0
            for entity_type, entities in analysis['entities'].items():
                for entity in entities:
                    if str(entity) in content_lower:
                        entity_score += 1
            
            # ì ìˆ˜ ì¡°ì •
            keyword_boost = (exact_matches * 0.1 + 
                           keyword_matches * 0.05 + 
                           entity_score * 0.15)
            
            result.score = result.score * (1 + keyword_boost)
        
        return results
    
    def _rerank_results(self, query: str, candidates: List[SearchResult], 
                       top_k: int = 20) -> List[SearchResult]:
        """CrossEncoderë¥¼ ì‚¬ìš©í•œ ì •ë°€ ì¬ì •ë ¬"""
        if not candidates:
            return []
        
        # ì¬ì •ë ¬ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ìŒ ìƒì„±
        pairs = []
        for candidate in candidates[:50]:  # ìƒìœ„ 50ê°œë§Œ ì¬ì •ë ¬
            # ë©”íƒ€ë°ì´í„° ì •ë³´ í¬í•¨
            enhanced_text = f"{candidate.content}"
            if candidate.chunk_type == 'article':
                if 'article_number' in candidate.metadata:
                    enhanced_text = f"{candidate.metadata['article_number']} {enhanced_text}"
            pairs.append([query, enhanced_text])
        
        # CrossEncoder ì ìˆ˜ ê³„ì‚°
        ce_scores = self.reranker_model.predict(pairs)
        
        # ê¸°ì¡´ ì ìˆ˜ì™€ ê²°í•©
        for i, (candidate, ce_score) in enumerate(zip(candidates[:len(pairs)], ce_scores)):
            # ë²¡í„° ì ìˆ˜ì™€ CrossEncoder ì ìˆ˜ ê²°í•©
            combined_score = candidate.score * 0.3 + float(ce_score) * 0.7
            candidate.score = combined_score
        
        # ì¬ì •ë ¬
        candidates.sort(key=lambda x: x.score, reverse=True)
        return candidates[:top_k]
    
    def _expand_context(self, results: List[SearchResult]) -> List[SearchResult]:
        """ì„ íƒëœ ì²­í¬ì˜ ì»¨í…ìŠ¤íŠ¸ í™•ì¥"""
        expanded_results = []
        added_ids = set()
        
        for result in results:
            # í˜„ì¬ ì²­í¬ ì¶”ê°€
            if result.chunk_id not in added_ids:
                expanded_results.append(result)
                added_ids.add(result.chunk_id)
            
            # ê°™ì€ ë¬¸ì„œì˜ ì¸ì ‘ ì²­í¬ ì°¾ê¸° (ì¡°í•­ êµ¬ì¡° ê³ ë ¤)
            if result.chunk_type == 'article_paragraph':
                # ê°™ì€ ì¡°í•­ì˜ ë‹¤ë¥¸ í•­ ì°¾ê¸°
                article_num = result.metadata.get('article_number')
                if article_num:
                    for chunk in self.chunks:
                        if (chunk.get('metadata') and 
                            json.loads(chunk['metadata']).get('article_number') == article_num and
                            chunk['chunk_id'] != result.chunk_id and
                            chunk['chunk_id'] not in added_ids):
                            
                            context_result = SearchResult(
                                chunk_id=chunk['chunk_id'],
                                content=chunk['content'],
                                score=result.score * 0.8,  # ì»¨í…ìŠ¤íŠ¸ëŠ” ì•½ê°„ ë‚®ì€ ì ìˆ˜
                                source=chunk['source'],
                                page=chunk['page'],
                                chunk_type=chunk.get('chunk_type', 'unknown'),
                                importance=chunk.get('importance', 0.5),
                                keywords=chunk.get('keywords', []),
                                metadata=json.loads(chunk.get('metadata', '{}'))
                            )
                            context_result.is_context = True
                            expanded_results.append(context_result)
                            added_ids.add(chunk['chunk_id'])
                            break
        
        return expanded_results

# --- 5. ë‹µë³€ ìƒì„± ì—”ì§„ ---

class FTCAnswerGenerator:
    """ê³µì •ê±°ë˜ ë¬¸ì„œ íŠ¹í™” ë‹µë³€ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ê³µì •ê±°ë˜ìœ„ì›íšŒ ì „ëµê¸°íšë¶€ì˜ ì „ë¬¸ AI ë²•ë¥  ë³´ì¡°ì›ì…ë‹ˆë‹¤.

ì—­í• :
- ê³µì •ê±°ë˜ë²•, ê´€ë ¨ ê³ ì‹œ, ê·œì •, ë§¤ë‰´ì–¼ì„ ì •í™•íˆ í•´ì„í•˜ì—¬ ì‹¤ë¬´ì— ì ìš© ê°€ëŠ¥í•œ ë‹µë³€ ì œê³µ
- ë²•ë¥  ìš©ì–´ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ë©´ì„œë„ ì‹¤ë¬´ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì „ë‹¬
- êµ¬ì²´ì ì¸ ì¡°í•­, ê¸°í•œ, ìš”ê±´ ë“±ì„ ì •í™•íˆ ì¸ìš©

ë‹µë³€ ì›ì¹™:
1. **ì •í™•ì„± ìµœìš°ì„ **: ì œê³µëœ ìë£Œì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€
2. **êµ¬ì¡°í™”ëœ ì„¤ëª…**: ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…
3. **ì‹¤ë¬´ ì ìš©ì„±**: ì´ë¡ ì  ì„¤ëª…ê³¼ í•¨ê»˜ ì‹¤ì œ ì ìš© ë°©ë²• ì œì‹œ
4. **ëª…í™•í•œ ê·¼ê±°**: ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ ì¡°í•­ì´ë‚˜ í˜ì´ì§€ ì¸ìš©
5. **ì˜ˆì™¸ì‚¬í•­ ëª…ì‹œ**: ì¼ë°˜ ì›ì¹™ê³¼ ì˜ˆì™¸ë¥¼ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…

ë‹µë³€ êµ¬ì¡°:
- í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ)
- ìƒì„¸ ì„¤ëª… (ê·¼ê±° ì¡°í•­ê³¼ í•¨ê»˜)
- ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì˜ˆì™¸ì‚¬í•­
- ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ (í•„ìš”ì‹œ)"""
    
    def generate(self, query: str, search_results: List[SearchResult], 
                query_analysis: Dict) -> Tuple[str, str, Dict]:
        """ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±"""
        
        # 1. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(search_results, query_analysis)
        
        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        messages = self._build_prompt(query, context, query_analysis)
        
        # 3. ë‹µë³€ ìƒì„±
        answer = self._generate_answer(messages)
        
        # 4. ë‹µë³€ ê²€ì¦
        verification = self._verify_answer(answer, context)
        
        # 5. ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            'sources': list(set(r.source for r in search_results if not hasattr(r, 'is_context'))),
            'primary_chunks': sum(1 for r in search_results if not hasattr(r, 'is_context')),
            'context_chunks': sum(1 for r in search_results if hasattr(r, 'is_context')),
            'query_type': query_analysis['query_type'],
            'confidence': self._calculate_confidence(search_results, query_analysis)
        }
        
        return answer, verification, metadata
    
    def _build_context(self, results: List[SearchResult], analysis: Dict) -> str:
        """êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context_parts = []
        
        # ì²­í¬ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        grouped_results = defaultdict(list)
        for result in results:
            grouped_results[result.chunk_type].append(result)
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        priority_order = ['definition', 'article', 'penalty', 'section', 'paragraph']
        
        for chunk_type in priority_order:
            if chunk_type in grouped_results:
                context_parts.append(f"\n[{self._get_type_label(chunk_type)}]")
                
                for i, result in enumerate(grouped_results[chunk_type]):
                    source_info = f"{result.source} (p.{result.page})"
                    
                    # ì¤‘ìš”ë„ í‘œì‹œ
                    importance_marker = ""
                    if result.importance > 0.8:
                        importance_marker = "â­ [í•µì‹¬] "
                    elif result.importance > 0.6:
                        importance_marker = "â— [ì¤‘ìš”] "
                    
                    # ì¡°í•­ ì •ë³´ í¬í•¨
                    article_info = ""
                    if 'article_number' in result.metadata:
                        article_info = f"{result.metadata['article_number']} "
                        if 'article_title' in result.metadata:
                            article_info += f"({result.metadata['article_title']}) "
                    
                    context_parts.append(
                        f"\n{importance_marker}{article_info}- ì¶œì²˜: {source_info}\n"
                        f"{result.content}\n"
                    )
        
        return "\n".join(context_parts)
    
    def _get_type_label(self, chunk_type: str) -> str:
        """ì²­í¬ íƒ€ì…ì˜ í•œê¸€ ë ˆì´ë¸”"""
        labels = {
            'definition': 'ìš©ì–´ ì •ì˜',
            'article': 'ê´€ë ¨ ì¡°í•­',
            'penalty': 'ë²Œì¹™/ì œì¬',
            'section': 'ì„¸ë¶€ ë‚´ìš©',
            'paragraph': 'ì°¸ê³  ë‚´ìš©',
            'article_paragraph': 'ì¡°í•­ ì„¸ë¶€ì‚¬í•­'
        }
        return labels.get(chunk_type, chunk_type)
    
    def _build_prompt(self, query: str, context: str, analysis: Dict) -> List[Dict]:
        """ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ ì¶”ê°€ ì§€ì‹œì‚¬í•­
        type_instructions = {
            'definition': "ìš©ì–´ì˜ ë²•ì  ì •ì˜ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ê³ , ê´€ë ¨ ì¡°í•­ì„ ì¸ìš©í•˜ì„¸ìš”.",
            'deadline': "êµ¬ì²´ì ì¸ ê¸°í•œ(ì¼ìˆ˜)ì„ ëª…ì‹œí•˜ê³ , ê¸°ì‚°ì¼ê³¼ ë§ˆê°ì¼ ê³„ì‚° ë°©ë²•ì„ ì„¤ëª…í•˜ì„¸ìš”.",
            'penalty': "ìœ„ë°˜ ì‹œ ì œì¬ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ê³¼íƒœë£Œ ê¸ˆì•¡ì´ë‚˜ ì²˜ë²Œ ìˆ˜ì¤€ì„ ëª…ì‹œí•˜ì„¸ìš”.",
            'procedure': "ì ˆì°¨ë¥¼ ë‹¨ê³„ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ê³ , ê° ë‹¨ê³„ì˜ ì£¼ì²´ì™€ ê¸°í•œì„ ëª…í™•íˆ í•˜ì„¸ìš”.",
            'requirement': "í•„ìš”í•œ ìš”ê±´ì„ ëª©ë¡ í˜•íƒœë¡œ ì •ë¦¬í•˜ê³ , ê° ìš”ê±´ì˜ ì¶©ì¡± ê¸°ì¤€ì„ ì„¤ëª…í•˜ì„¸ìš”."
        }
        
        additional_instruction = ""
        for q_type in analysis['query_type']:
            if q_type in type_instructions:
                additional_instruction += f"\n- {type_instructions[q_type]}"
        
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"""ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ê²€ìƒ‰ëœ ìë£Œ]
{context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€ ì‹œ ìœ ì˜ì‚¬í•­]
1. ë°˜ë“œì‹œ ì œê³µëœ ìë£Œì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ì¡°í•­ ë²ˆí˜¸, í˜ì´ì§€ ë“± ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”.
3. ë²•ë¥  ìš©ì–´ëŠ” ì •í™•íˆ ì‚¬ìš©í•˜ë˜, í•„ìš”ì‹œ ì‰¬ìš´ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.{additional_instruction}

ë‹µë³€ í˜•ì‹:
ğŸ“Œ **í•µì‹¬ ë‹µë³€**
(ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ)

ğŸ“‹ **ìƒì„¸ ì„¤ëª…**
(ê·¼ê±° ì¡°í•­ê³¼ êµ¬ì²´ì ì¸ ë‚´ìš©)

âš ï¸ **ì£¼ì˜ì‚¬í•­**
(ìˆëŠ” ê²½ìš°ì—ë§Œ, ì˜ˆì™¸ì‚¬í•­ì´ë‚˜ íŠ¹ë³„íˆ ìœ ì˜í•  ì )"""}
        ]
        
        return messages
    
    def _generate_answer(self, messages: List[Dict]) -> str:
        """GPT-4ë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.1,
            max_tokens=2000
        )
        return response.choices[0].message.content
    
    def _verify_answer(self, answer: str, context: str) -> str:
        """ë‹µë³€ ì •í™•ì„± ê²€ì¦"""
        verification_prompt = f"""ë‹¤ìŒ AI ë‹µë³€ì´ ì œê³µëœ ì°¸ê³  ìë£Œì—ë§Œ ê·¼ê±°í•˜ì—¬ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ì—„ê²©íˆ ê²€ì¦í•˜ì„¸ìš”.

[AI ë‹µë³€]
{answer}

[ì°¸ê³  ìë£Œ]
{context}

ê²€ì¦ í•­ëª©:
1. ë‹µë³€ì˜ ëª¨ë“  ì‚¬ì‹¤ì´ ì°¸ê³  ìë£Œì— ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€?
2. ì¡°í•­ ë²ˆí˜¸ë‚˜ ì¸ìš©ì´ ì •í™•í•œê°€?
3. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜í™”ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
4. ìˆ˜ì¹˜ë‚˜ ê¸°í•œì´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ê°€?

í˜•ì‹: "âœ… ê²€ì¦ í†µê³¼" ë˜ëŠ” "âš ï¸ ê²€ì¦ ì£¼ì˜" ë˜ëŠ” "âŒ ê²€ì¦ ì‹¤íŒ¨"
ì´ìœ ë¥¼ í•œ ì¤„ë¡œ ì„¤ëª…í•˜ì„¸ìš”."""

        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": verification_prompt}],
            temperature=0
        )
        
        return response.choices[0].message.content
    
    def _calculate_confidence(self, results: List[SearchResult], analysis: Dict) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not results:
            return 0.0
        
        # ìš”ì¸ë³„ ì ìˆ˜
        factors = {
            'top_score': min(results[0].score / 1.0, 1.0) * 0.3,  # ìµœê³  ì ìˆ˜
            'avg_score': min(np.mean([r.score for r in results[:3]]) / 0.8, 1.0) * 0.2,  # ìƒìœ„ í‰ê· 
            'type_match': 0.2 if any(r.chunk_type in analysis['priority_chunks'] for r in results[:3]) else 0.0,
            'keyword_coverage': min(len([kw for kw in analysis['key_terms'] if any(kw in r.content.lower() for r in results[:3])]) / max(len(analysis['key_terms']), 1), 1.0) * 0.2,
            'source_diversity': min(len(set(r.source for r in results[:5])) / 3, 1.0) * 0.1
        }
        
        return sum(factors.values())

# --- 6. ë©”ì¸ UI êµ¬í˜„ ---

def main():
    # í—¤ë”
    st.markdown('<h1 class="main-header">âš–ï¸ ì „ëµê¸°íšë¶€ AI ë²•ë¥  ë³´ì¡°ì›</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">ê³µì •ê±°ë˜ìœ„ì›íšŒ ê·œì • ë° ë§¤ë‰´ì–¼ ê¸°ë°˜ ì „ë¬¸ Q&A ì‹œìŠ¤í…œ</p>', unsafe_allow_html=True)
    
    # ëª¨ë¸ ë¡œë“œ
    model, reranker, index, chunks, chunk_type_index = load_models_and_data()
    
    if model is None:
        st.error("í•„ìˆ˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = FTCAdvancedRAG(model, reranker, index, chunks, chunk_type_index)
    answer_generator = FTCAnswerGenerator()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "search_history" not in st.session_state:
        st.session_state.search_history = []
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ” ê²€ìƒ‰ ì˜µì…˜")
        
        # ê²€ìƒ‰ ì„¤ì •
        top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", min_value=3, max_value=10, value=7, 
                         help="ë” ë§ì€ ìë£Œë¥¼ ê²€í† í•˜ë©´ ì •í™•ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ ì‹œê°„ì´ ë” ê±¸ë¦½ë‹ˆë‹¤.")
        
        use_context = st.checkbox("ë¬¸ë§¥ í™•ì¥ ì‚¬ìš©", value=True,
                                help="ê´€ë ¨ ì¡°í•­ì˜ ë‹¤ë¥¸ í•­ëª©ë„ í•¨ê»˜ ê²€í† í•©ë‹ˆë‹¤.")
        
        st.divider()
        
        # í†µê³„ ì •ë³´
        if chunks:
            st.header("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("í•™ìŠµ ë¬¸ì„œ", f"{len(set(c['source'] for c in chunks))}ê°œ")
                st.metric("ì´ ì •ë³´ ë‹¨ìœ„", f"{len(chunks):,}ê°œ")
            
            with col2:
                # ì²­í¬ íƒ€ì…ë³„ í†µê³„
                type_counts = Counter(c.get('chunk_type', 'unknown') for c in chunks)
                st.metric("ì¡°í•­ ì •ë³´", f"{type_counts.get('article', 0):,}ê°œ")
                st.metric("ì •ì˜ ì •ë³´", f"{type_counts.get('definition', 0):,}ê°œ")
        
        st.divider()
        
        # ìì£¼ ë¬»ëŠ” ì§ˆë¬¸
        st.header("ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
        example_questions = [
            "ëŒ€ê·œëª¨ë‚´ë¶€ê±°ë˜ ê³µì‹œ ê¸°í•œì€?",
            "íŠ¹ìˆ˜ê´€ê³„ì¸ì˜ ì •ì˜ëŠ”?",
            "ê³µì‹œì˜ë¬´ ìœ„ë°˜ ì‹œ ê³¼íƒœë£ŒëŠ”?",
            "ì´ì‚¬íšŒ ì˜ê²° ì˜ˆì™¸ì‚¬í•­ì€?",
            "ë…ë¦½ê²½ì˜ ì¸ì • ìš”ê±´ì€?"
        ]
        
        for q in example_questions:
            if st.button(q, key=f"example_{q}"):
                st.session_state.pending_question = q
    
    # ë©”ì¸ ëŒ€í™” ì˜ì—­
    # ì´ì „ ëŒ€í™” í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "user":
                st.markdown(message["content"])
            else:
                st.markdown(message["content"]["answer"])
                
                # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                if "metadata" in message["content"]:
                    with st.expander("ğŸ“ ë‹µë³€ ìƒì„¸ ì •ë³´", expanded=False):
                        meta = message["content"]["metadata"]
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ì°¸ì¡° ë¬¸ì„œ", f"{len(meta['sources'])}ê°œ")
                        with col2:
                            confidence_pct = meta['confidence'] * 100
                            st.metric("ì‹ ë¢°ë„", f"{confidence_pct:.0f}%")
                        with col3:
                            st.metric("ê²€í†  ìë£Œ", f"{meta['primary_chunks']}ê°œ")
                        
                        # ì¶œì²˜ í‘œì‹œ
                        st.subheader("ğŸ“š ì°¸ì¡° ë¬¸ì„œ")
                        for source in meta['sources']:
                            st.markdown(f'<span class="source-tag">{source}</span>', 
                                      unsafe_allow_html=True)
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    # ì˜ˆì‹œ ì§ˆë¬¸ í´ë¦­ ì²˜ë¦¬
    if hasattr(st.session_state, 'pending_question'):
        question = st.session_state.pending_question
        del st.session_state.pending_question
    else:
        question = st.chat_input("ê³µì •ê±°ë˜ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    
    if question:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": question})
        
        with st.chat_message("user"):
            st.markdown(question)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_placeholder = st.empty()
            
            # 1. ê²€ìƒ‰ ìˆ˜í–‰
            with st.spinner("ğŸ” ê´€ë ¨ ìë£Œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                search_results, search_stats = rag_system.search(question, top_k=top_k)
                progress_placeholder.info(f"âœ… {len(search_results)}ê°œì˜ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # 2. ë‹µë³€ ìƒì„±
            with st.spinner("ğŸ’­ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                answer, verification, metadata = answer_generator.generate(
                    question, 
                    search_results,
                    search_stats['query_analysis']
                )
                progress_placeholder.empty()
            
            # ë‹µë³€ í‘œì‹œ
            st.markdown(answer)
            
            # ê²€ì¦ ê²°ê³¼ í‘œì‹œ
            if "âœ…" in verification:
                st.success(verification)
            elif "âš ï¸" in verification:
                st.warning(verification)
            else:
                st.error(verification)
            
            # ì‹ ë¢°ë„ ì‹œê°í™”
            confidence = metadata['confidence']
            confidence_color = "#4CAF50" if confidence > 0.7 else "#FF9800" if confidence > 0.4 else "#F44336"
            st.markdown(
                f"""
                <div style="background: linear-gradient(to right, {confidence_color} {confidence*100}%, #e0e0e0 {confidence*100}%); 
                     padding: 5px 15px; border-radius: 20px; text-align: center; margin: 20px 0;">
                    <strong>ë‹µë³€ ì‹ ë¢°ë„: {confidence*100:.0f}%</strong>
                </div>
                """,
                unsafe_allow_html=True
            )
            
            # ìƒì„¸ ì •ë³´ (ì ‘íŒ ìƒíƒœ)
            with st.expander("ğŸ” ê²€ìƒ‰ ë° ë¶„ì„ ìƒì„¸ ì •ë³´"):
                tab1, tab2, tab3 = st.tabs(["ê²€ìƒ‰ í†µê³„", "ì¿¼ë¦¬ ë¶„ì„", "ì°¸ì¡° ìë£Œ"])
                
                with tab1:
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ê²€ìƒ‰ ì‹œê°„", f"{search_stats['search_time']:.2f}ì´ˆ")
                    with col2:
                        st.metric("ì´ˆê¸° í›„ë³´", f"{search_stats['initial_candidates']}ê°œ")
                    with col3:
                        st.metric("ì¬ì •ë ¬ í›„", f"{search_stats['after_rerank']}ê°œ")
                    with col4:
                        st.metric("ìµœì¢… ì‚¬ìš©", f"{search_stats['final_results']}ê°œ")
                
                with tab2:
                    st.json(search_stats['query_analysis'])
                
                with tab3:
                    for i, result in enumerate(search_results[:5]):
                        with st.container():
                            col1, col2 = st.columns([3, 1])
                            with col1:
                                st.markdown(f"**{result.source}** - í˜ì´ì§€ {result.page}")
                            with col2:
                                importance_class = "importance-high" if result.importance > 0.7 else "importance-medium"
                                st.markdown(f'<span class="{importance_class}">ì¤‘ìš”ë„: {result.importance:.2f}</span>', 
                                          unsafe_allow_html=True)
                            
                            st.text(result.content[:300] + "..." if len(result.content) > 300 else result.content)
                            st.caption(f"ì ìˆ˜: {result.score:.3f} | íƒ€ì…: {result.chunk_type}")
                            st.divider()
            
            # ê²½ê³  ë©”ì‹œì§€
            st.divider()
            st.caption("âš ï¸ ë³¸ ë‹µë³€ì€ AIê°€ ì œê³µí•˜ëŠ” ì°¸ê³ ìš© ì •ë³´ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì •ì—ëŠ” ë°˜ë“œì‹œ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
            
            # ì„¸ì…˜ì— ì €ì¥
            response_data = {
                "answer": answer,
                "verification": verification,
                "metadata": metadata,
                "search_stats": search_stats
            }
            st.session_state.messages.append({"role": "assistant", "content": response_data})
            st.session_state.search_history.append({
                "query": question,
                "timestamp": time.time(),
                "confidence": confidence
            })
    
    # í•˜ë‹¨ ì •ë³´
    with st.container():
        st.divider()
        col1, col2, col3 = st.columns(3)
        with col1:
            st.caption("ğŸ¢ ì „ëµê¸°íšë¶€")
        with col2:
            st.caption("ğŸ“… 2025ë…„ ìµœì‹  ìë£Œ ê¸°ë°˜")
        with col3:
            st.caption("ğŸ¤– Powered by GPT-4")

if __name__ == "__main__":
    main()
